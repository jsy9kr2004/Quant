"""
Robust Backtester

ê°œì„ ëœ ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ
- Cross-validation
- Walk-forward validation
- Performance monitoring
- Feature selection
"""

import pandas as pd
import numpy as np
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path

from validation.time_series_cv import TimeSeriesCV
from validation.walk_forward import WalkForwardValidator
from monitoring.performance_monitor import PerformanceMonitor
from feature_engineering.feature_selector import FeatureSelector


class RobustBacktester:
    """
    ê°œì„ ëœ ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ

    íŠ¹ì§•:
    1. Time Series Cross-Validation
    2. Walk-Forward Validation (ëª¨ë¸ ìž¬í•™ìŠµ)
    3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ì €í•˜ ê°ì§€
    4. í”¼ì²˜ ì„ íƒ (ê³¼ì í•© ë°©ì§€)
    """

    def __init__(self,
                 train_months: int = 24,
                 test_months: int = 3,
                 retrain_frequency: int = 3,
                 cv_splits: int = 5,
                 top_k_features: int = 50,
                 feature_selection_method: str = 'mutual_info',
                 performance_window: int = 10,
                 alert_threshold: float = 0.1):
        """
        Args:
            train_months: í•™ìŠµ ë°ì´í„° ê¸°ê°„ (ê°œì›”)
            test_months: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ê°„ (ê°œì›”)
            retrain_frequency: ìž¬í•™ìŠµ ë¹ˆë„ (ê°œì›”)
            cv_splits: Cross-validation fold ìˆ˜
            top_k_features: ì„ íƒí•  í”¼ì²˜ ê°œìˆ˜
            feature_selection_method: í”¼ì²˜ ì„ íƒ ë°©ë²•
            performance_window: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìœˆë„ìš° í¬ê¸°
            alert_threshold: ì„±ëŠ¥ ì €í•˜ ì•Œë¦¼ ìž„ê³„ê°’
        """
        # 1. êµì°¨ ê²€ì¦
        self.cv = TimeSeriesCV(n_splits=cv_splits)

        # 2. Walk-forward validation
        self.wfv = WalkForwardValidator(
            train_months=train_months,
            test_months=test_months,
            retrain_frequency=retrain_frequency,
            anchored=False  # Rolling window
        )

        # 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.monitor = PerformanceMonitor(
            window_size=performance_window,
            alert_threshold=alert_threshold
        )

        # 4. í”¼ì²˜ ì„ íƒ
        self.feature_selector = FeatureSelector(
            method=feature_selection_method,
            top_k=top_k_features
        )

        self.selected_features = None
        self.results = []

    def run_backtest(self,
                    model_class: Any,
                    df: pd.DataFrame,
                    date_col: str,
                    feature_cols: List[str],
                    target_col: str,
                    start_date: datetime,
                    end_date: datetime,
                    model_params: Optional[Dict] = None,
                    use_feature_selection: bool = True,
                    use_cv: bool = True,
                    use_monitoring: bool = True,
                    save_results: bool = True,
                    output_dir: str = './results') -> pd.DataFrame:
        """
        ê°•ê±´í•œ ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰

        Args:
            model_class: ëª¨ë¸ í´ëž˜ìŠ¤
            df: ì „ì²´ ë°ì´í„°í”„ë ˆìž„
            date_col: ë‚ ì§œ ì»¬ëŸ¼ëª…
            feature_cols: í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            start_date: ì‹œìž‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            model_params: ëª¨ë¸ íŒŒë¼ë¯¸í„°
            use_feature_selection: í”¼ì²˜ ì„ íƒ ì‚¬ìš© ì—¬ë¶€
            use_cv: êµì°¨ê²€ì¦ ì‚¬ìš© ì—¬ë¶€
            use_monitoring: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‚¬ìš© ì—¬ë¶€
            save_results: ê²°ê³¼ ì €ìž¥ ì—¬ë¶€
            output_dir: ê²°ê³¼ ì €ìž¥ ë””ë ‰í† ë¦¬

        Returns:
            ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ ë°ì´í„°í”„ë ˆìž„
        """
        if model_params is None:
            model_params = {}

        logging.info(f"\n{'='*60}")
        logging.info("ðŸš€ Robust Backtesting ì‹œìž‘")
        logging.info(f"{'='*60}")
        logging.info(f"ê¸°ê°„: {start_date} ~ {end_date}")
        logging.info(f"í”¼ì²˜ ìˆ˜: {len(feature_cols)}")
        logging.info(f"í”¼ì²˜ ì„ íƒ: {use_feature_selection}")
        logging.info(f"êµì°¨ê²€ì¦: {use_cv}")
        logging.info(f"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§: {use_monitoring}")
        logging.info(f"{'='*60}\n")

        # 1ë‹¨ê³„: í”¼ì²˜ ì„ íƒ (ì„ íƒ ì‚¬í•­)
        if use_feature_selection:
            logging.info(f"\n{'='*60}")
            logging.info("1ï¸âƒ£ í”¼ì²˜ ì„ íƒ")
            logging.info(f"{'='*60}")

            # ì „ì²´ ë°ì´í„°ì—ì„œ ìƒ˜í”Œë§í•˜ì—¬ í”¼ì²˜ ì„ íƒ
            sample_mask = (df[date_col] >= start_date) & \
                         (df[date_col] < start_date + pd.DateOffset(months=12))
            sample_df = df[sample_mask]

            if len(sample_df) > 0:
                X_sample = sample_df[feature_cols]
                y_sample = sample_df[target_col]

                # NaN ì œê±°
                valid_mask = ~(X_sample.isnull().any(axis=1) | y_sample.isnull())
                X_sample = X_sample[valid_mask]
                y_sample = y_sample[valid_mask]

                self.selected_features = self.feature_selector.select_features(
                    X_sample, y_sample
                )
            else:
                logging.warning("ìƒ˜í”Œ ë°ì´í„°ê°€ ì—†ì–´ í”¼ì²˜ ì„ íƒì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                self.selected_features = feature_cols
        else:
            self.selected_features = feature_cols

        # 2ë‹¨ê³„: Walk-Forward Validation
        logging.info(f"\n{'='*60}")
        logging.info("2ï¸âƒ£ Walk-Forward Validation")
        logging.info(f"{'='*60}")

        windows = self.wfv.generate_windows(start_date, end_date)
        results = []

        for i, window in enumerate(windows, 1):
            logging.info(f"\n{'='*60}")
            logging.info(f"Window {i}/{len(windows)}")
            logging.info(f"  Train: {window['train_start'].strftime('%Y-%m-%d')} ~ {window['train_end'].strftime('%Y-%m-%d')}")
            logging.info(f"  Test:  {window['test_start'].strftime('%Y-%m-%d')} ~ {window['test_end'].strftime('%Y-%m-%d')}")
            logging.info(f"{'='*60}")

            # ë°ì´í„° ë¶„í• 
            train_mask = (df[date_col] >= window['train_start']) & \
                        (df[date_col] < window['train_end'])
            test_mask = (df[date_col] >= window['test_start']) & \
                       (df[date_col] < window['test_end'])

            train_df = df[train_mask].copy()
            test_df = df[test_mask].copy()

            if len(train_df) == 0 or len(test_df) == 0:
                logging.warning(f"  âš ï¸ Empty data in window {i}, skipping...")
                continue

            try:
                # í”¼ì²˜ ë° íƒ€ê²Ÿ ì¶”ì¶œ
                X_train = train_df[self.selected_features]
                y_train = train_df[target_col]
                X_test = test_df[self.selected_features]
                y_test = test_df[target_col]

                # NaN ì œê±°
                train_valid_mask = ~(X_train.isnull().any(axis=1) | y_train.isnull())
                test_valid_mask = ~(X_test.isnull().any(axis=1) | y_test.isnull())

                X_train = X_train[train_valid_mask]
                y_train = y_train[train_valid_mask]
                X_test = X_test[test_valid_mask]
                y_test = y_test[test_valid_mask]

                if len(X_train) == 0 or len(X_test) == 0:
                    logging.warning(f"  âš ï¸ No valid data after NaN removal, skipping...")
                    continue

                logging.info(f"  Train samples: {len(X_train):,}")
                logging.info(f"  Test samples:  {len(X_test):,}")

                # ëª¨ë¸ ìž¬í•™ìŠµ â­
                model = model_class(**model_params)

                if use_cv:
                    # êµì°¨ ê²€ì¦
                    logging.info(f"  ðŸ”„ êµì°¨ ê²€ì¦ ì¤‘...")
                    cv_scores, _ = model.cross_validate(
                        X_train, y_train, cv_splits=3, verbose=False
                    )
                    logging.info(f"  ðŸ“Š CV ê²°ê³¼: {cv_scores}")

                # í•™ìŠµ
                logging.info(f"  ðŸ”„ ëª¨ë¸ í•™ìŠµ ì¤‘...")
                model.fit(X_train, y_train, verbose=0)

                # í‰ê°€
                metrics = model.evaluate(X_test, y_test)

                result = {
                    'window': i,
                    'train_start': window['train_start'],
                    'train_end': window['train_end'],
                    'test_start': window['test_start'],
                    'test_end': window['test_end'],
                    'train_samples': len(X_train),
                    'test_samples': len(X_test),
                    **metrics
                }

                if use_cv:
                    for k, v in cv_scores.items():
                        result[f'cv_{k}'] = v

                results.append(result)

                logging.info(f"  ðŸ“Š Test Performance:")
                for metric, value in metrics.items():
                    logging.info(f"     {metric}: {value:.4f}")

                # 3ë‹¨ê³„: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (ì„ íƒ ì‚¬í•­)
                if use_monitoring:
                    period_label = window['test_start'].strftime('%Y-%m')
                    self.monitor.update_performance(metrics, period_label)

                    # í”¼ì²˜ ë“œë¦¬í”„íŠ¸ ì²´í¬
                    drift_features = self.monitor.check_feature_drift(
                        X_test.values,
                        feature_names=self.selected_features
                    )

                    if drift_features:
                        logging.warning(f"  âš ï¸ {len(drift_features)}ê°œ í”¼ì²˜ì—ì„œ ë“œë¦¬í”„íŠ¸ ê°ì§€")

                    # ìž¬í•™ìŠµ í•„ìš” ì—¬ë¶€
                    if self.monitor.should_retrain():
                        logging.warning(f"  ðŸ”„ ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•œ ìž¬í•™ìŠµ ê¶Œìž¥!")

            except Exception as e:
                logging.error(f"  âŒ Error in window {i}: {str(e)}")
                import traceback
                traceback.print_exc()
                continue

        # ê²°ê³¼ ìš”ì•½
        if not results:
            logging.error("No valid results from backtesting!")
            return pd.DataFrame()

        results_df = pd.DataFrame(results)

        logging.info(f"\n{'='*60}")
        logging.info("3ï¸âƒ£ ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ ìš”ì•½")
        logging.info(f"{'='*60}")

        # í‰ê·  ì„±ëŠ¥
        metric_cols = [col for col in results_df.columns if col not in [
            'window', 'train_start', 'train_end', 'test_start', 'test_end',
            'train_samples', 'test_samples'
        ]]

        logging.info("\nðŸ“Š í‰ê·  ì„±ëŠ¥:")
        for metric in metric_cols:
            mean_val = results_df[metric].mean()
            std_val = results_df[metric].std()
            logging.info(f"  {metric}: {mean_val:.4f} Â± {std_val:.4f}")

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìš”ì•½
        if use_monitoring:
            logging.info(f"\n{'='*60}")
            logging.info("4ï¸âƒ£ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìš”ì•½")
            logging.info(f"{'='*60}")
            self.monitor.print_summary()

        # ê²°ê³¼ ì €ìž¥
        if save_results:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)

            # ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ ì €ìž¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = output_path / f'backtest_results_{timestamp}.csv'
            results_df.to_csv(results_file, index=False)
            logging.info(f"\nðŸ’¾ ê²°ê³¼ ì €ìž¥: {results_file}")

            # ì„ íƒëœ í”¼ì²˜ ì €ìž¥
            if use_feature_selection:
                features_file = output_path / f'selected_features_{timestamp}.csv'
                self.feature_selector.save_selected_features(str(features_file))

            # ì„±ëŠ¥ ì•Œë¦¼ ì €ìž¥
            if use_monitoring:
                alerts_df = self.monitor.get_alerts_dataframe()
                if not alerts_df.empty:
                    alerts_file = output_path / f'performance_alerts_{timestamp}.csv'
                    alerts_df.to_csv(alerts_file, index=False)
                    logging.info(f"ðŸ’¾ ì•Œë¦¼ ì €ìž¥: {alerts_file}")

        self.results = results_df
        return results_df

    def get_results(self) -> pd.DataFrame:
        """ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ ë°˜í™˜"""
        return self.results

    def get_selected_features(self) -> List[str]:
        """ì„ íƒëœ í”¼ì²˜ ë°˜í™˜"""
        return self.selected_features

    def get_performance_summary(self) -> Dict:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        return self.monitor.get_performance_summary()
