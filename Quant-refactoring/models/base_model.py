"""
Base model class for all ML models
"""

from abc import ABC, abstractmethod
import joblib
import logging
from pathlib import Path
from typing import Optional, Dict, Any
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, mean_squared_error, classification_report


class BaseModel(ABC):
    """
    ëª¨ë“  ML ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤

    Provides:
    - Consistent interface for all models
    - Model saving/loading
    - Evaluation metrics
    - Feature importance
    """

    def __init__(self, model_type: str, task: str = 'classification'):
        """
        Initialize base model

        Args:
            model_type: ëª¨ë¸ íƒ€ì… ('xgboost', 'lightgbm', 'catboost')
            task: 'classification' or 'regression'
        """
        self.model_type = model_type
        self.task = task
        self.model = None
        self.feature_names = None
        self.is_trained = False

    @abstractmethod
    def build_model(self, params: Dict[str, Any]):
        """ëª¨ë¸ ìƒì„± (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass

    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """
        ëª¨ë¸ í•™ìŠµ

        Args:
            X_train: í•™ìŠµ ë°ì´í„°
            y_train: í•™ìŠµ ë ˆì´ë¸”
            X_val: ê²€ì¦ ë°ì´í„° (ì„ íƒ)
            y_val: ê²€ì¦ ë ˆì´ë¸” (ì„ íƒ)
        """
        if self.model is None:
            raise ValueError("Model not built. Call build_model() first.")

        self.feature_names = X_train.columns.tolist() if hasattr(X_train, 'columns') else None

        logging.info(f"Training {self.model_type} {self.task} model...")
        logging.info(f"  Training samples: {len(X_train):,}")
        if X_val is not None:
            logging.info(f"  Validation samples: {len(X_val):,}")

        # í•™ìŠµ
        if X_val is not None and y_val is not None:
            eval_set = [(X_val, y_val)]
            self.model.fit(X_train, y_train, eval_set=eval_set, **kwargs)
        else:
            self.model.fit(X_train, y_train, **kwargs)

        self.is_trained = True
        logging.info(f"âœ… Training completed")

        return self

    def predict(self, X):
        """ì˜ˆì¸¡"""
        if not self.is_trained:
            raise ValueError("Model not trained. Call fit() first.")

        return self.model.predict(X)

    def predict_proba(self, X):
        """í™•ë¥  ì˜ˆì¸¡ (ë¶„ë¥˜ ëª¨ë¸ë§Œ)"""
        if self.task != 'classification':
            raise ValueError("predict_proba only available for classification")

        if not self.is_trained:
            raise ValueError("Model not trained. Call fit() first.")

        return self.model.predict_proba(X)

    def evaluate(self, X, y, threshold: Optional[float] = None) -> Dict[str, float]:
        """
        ëª¨ë¸ í‰ê°€

        Args:
            X: í‰ê°€ ë°ì´í„°
            y: í‰ê°€ ë ˆì´ë¸”
            threshold: ë¶„ë¥˜ ì„ê³„ê°’ (ì„ íƒ, ë¶„ë¥˜ ëª¨ë¸ë§Œ)

        Returns:
            í‰ê°€ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_trained:
            raise ValueError("Model not trained. Call fit() first.")

        metrics = {}

        if self.task == 'classification':
            y_pred_proba = self.predict_proba(X)[:, 1]

            if threshold is not None:
                y_pred = (y_pred_proba >= threshold).astype(int)
            else:
                y_pred = self.predict(X)

            metrics['accuracy'] = accuracy_score(y, y_pred)

            # Precision, Recall, F1
            report = classification_report(y, y_pred, output_dict=True)
            metrics['precision'] = report['1']['precision']
            metrics['recall'] = report['1']['recall']
            metrics['f1'] = report['1']['f1-score']

            logging.info(f"Evaluation - Accuracy: {metrics['accuracy']:.4f}, "
                        f"Precision: {metrics['precision']:.4f}, "
                        f"Recall: {metrics['recall']:.4f}, "
                        f"F1: {metrics['f1']:.4f}")

        else:  # regression
            y_pred = self.predict(X)
            metrics['rmse'] = np.sqrt(mean_squared_error(y, y_pred))
            metrics['mae'] = np.mean(np.abs(y - y_pred))

            logging.info(f"Evaluation - RMSE: {metrics['rmse']:.4f}, "
                        f"MAE: {metrics['mae']:.4f}")

        return metrics

    def get_feature_importance(self, top_n: int = 20) -> pd.DataFrame:
        """
        íŠ¹ì§• ì¤‘ìš”ë„ ë°˜í™˜

        Args:
            top_n: ìƒìœ„ Nê°œ íŠ¹ì§•

        Returns:
            íŠ¹ì§• ì¤‘ìš”ë„ DataFrame
        """
        if not self.is_trained:
            raise ValueError("Model not trained. Call fit() first.")

        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
        else:
            logging.warning("Model does not support feature importance")
            return pd.DataFrame()

        if self.feature_names is None:
            feature_names = [f"feature_{i}" for i in range(len(importances))]
        else:
            feature_names = self.feature_names

        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        })

        importance_df = importance_df.sort_values('importance', ascending=False)

        if top_n is not None:
            importance_df = importance_df.head(top_n)

        return importance_df

    def save(self, path: str):
        """
        ëª¨ë¸ ì €ì¥

        Args:
            path: ì €ì¥ ê²½ë¡œ
        """
        if not self.is_trained:
            raise ValueError("Model not trained. Cannot save untrained model.")

        path_obj = Path(path)
        path_obj.parent.mkdir(parents=True, exist_ok=True)

        model_data = {
            'model': self.model,
            'model_type': self.model_type,
            'task': self.task,
            'feature_names': self.feature_names
        }

        joblib.dump(model_data, path)
        logging.info(f"ğŸ’¾ Model saved to: {path}")

    def load(self, path: str):
        """
        ëª¨ë¸ ë¡œë“œ

        Args:
            path: ëª¨ë¸ ê²½ë¡œ
        """
        if not Path(path).exists():
            raise FileNotFoundError(f"Model file not found: {path}")

        model_data = joblib.load(path)

        self.model = model_data['model']
        self.model_type = model_data['model_type']
        self.task = model_data['task']
        self.feature_names = model_data.get('feature_names')
        self.is_trained = True

        logging.info(f"ğŸ“‚ Model loaded from: {path}")

    def get_params(self) -> Dict:
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        if self.model is None:
            return {}
        return self.model.get_params()

    def set_params(self, **params):
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •"""
        if self.model is None:
            raise ValueError("Model not built. Call build_model() first.")
        self.model.set_params(**params)

    def cross_validate(self, X, y, dates=None, cv_splits=5, verbose=True):
        """
        êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•œ ëª¨ë¸ í‰ê°€

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            dates: ë‚ ì§œ ì •ë³´ (ì‹œê³„ì—´ êµì°¨ê²€ì¦ì— ì‚¬ìš©, ì„ íƒ)
            cv_splits: Cross-validation fold ìˆ˜
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

        Returns:
            (í‰ê·  ì ìˆ˜, ê° fold ì ìˆ˜ ë¦¬ìŠ¤íŠ¸)
        """
        from validation.time_series_cv import TimeSeriesCV

        cv = TimeSeriesCV(n_splits=cv_splits)
        avg_scores, all_scores = cv.cross_validate_model(
            self, X, y, dates=dates, verbose=verbose
        )

        return avg_scores, all_scores

    def fit_with_cv(self, X, y, dates=None, cv_splits=5):
        """
        êµì°¨ ê²€ì¦ í›„ ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            dates: ë‚ ì§œ ì •ë³´ (ì„ íƒ)
            cv_splits: Cross-validation fold ìˆ˜

        Returns:
            (í‰ê·  ì ìˆ˜, ê° fold ì ìˆ˜ ë¦¬ìŠ¤íŠ¸)
        """
        # êµì°¨ ê²€ì¦
        avg_scores, all_scores = self.cross_validate(X, y, dates, cv_splits)

        # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ
        logging.info(f"\n{'='*60}")
        logging.info("ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...")
        logging.info(f"{'='*60}")
        self.fit(X, y)

        return avg_scores, all_scores

    def __repr__(self):
        status = "trained" if self.is_trained else "not trained"
        return f"{self.model_type.upper()} {self.task} model ({status})"
