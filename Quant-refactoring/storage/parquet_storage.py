"""
Parquet storage module with automatic validation and sample generation
"""

import logging
import pandas as pd
from pathlib import Path
from typing import Optional, List, Union
from .data_validator import DataValidator


class ParquetStorage:
    """
    ê°œì„ ëœ Parquet ì €ìž¥ì†Œ (ê²€ì¦ ê¸°ëŠ¥ í¬í•¨)

    Features:
    - Automatic validation after saving
    - Sample CSV generation for quick inspection
    - Efficient compression with configurable algorithms
    - Metadata tracking
    """

    def __init__(self, root_path: str, auto_validate: bool = True):
        """
        Initialize ParquetStorage

        Args:
            root_path: ë£¨íŠ¸ ë°ì´í„° ê²½ë¡œ
            auto_validate: ì €ìž¥ í›„ ìžë™ ê²€ì¦ ì—¬ë¶€
        """
        self.root_path = Path(root_path)
        self.parquet_path = self.root_path / "parquet"
        self.view_path = self.root_path / "VIEW"
        self.sample_path = self.root_path / "samples"

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.parquet_path.mkdir(parents=True, exist_ok=True)
        self.view_path.mkdir(parents=True, exist_ok=True)
        self.sample_path.mkdir(parents=True, exist_ok=True)

        self.auto_validate = auto_validate
        self.validator = DataValidator()

        logging.info(f"ParquetStorage initialized at: {self.root_path}")

    def save_parquet(self,
                    df: pd.DataFrame,
                    name: str,
                    compression: str = 'snappy',
                    save_sample: bool = True,
                    sample_size: int = 100,
                    partition_cols: Optional[List[str]] = None) -> bool:
        """
        Parquet ì €ìž¥ + ê²€ì¦ + ìƒ˜í”Œ CSV ìƒì„±

        Args:
            df: ì €ìž¥í•  DataFrame
            name: í…Œì´ë¸” ì´ë¦„
            compression: ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ('snappy', 'gzip', 'zstd', 'brotli')
            save_sample: ìƒ˜í”Œ CSV ìƒì„± ì—¬ë¶€
            sample_size: ìƒ˜í”Œ í–‰ ìˆ˜
            partition_cols: íŒŒí‹°ì…˜ ì»¬ëŸ¼ (ì˜ˆ: ['year'] - ì—°ë„ë³„ ë¶„í• )

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        file_path = self.parquet_path / f"{name}.parquet"

        try:
            # 1. Parquet ì €ìž¥
            logging.info(f"Saving {name}.parquet...")

            if partition_cols:
                # íŒŒí‹°ì…”ë‹ ì €ìž¥ (ë””ë ‰í† ë¦¬ êµ¬ì¡°)
                df.to_parquet(
                    self.parquet_path / name,
                    engine='pyarrow',
                    compression=compression,
                    partition_cols=partition_cols,
                    index=False
                )
                file_path = self.parquet_path / name
            else:
                # ë‹¨ì¼ íŒŒì¼ ì €ìž¥
                df.to_parquet(
                    file_path,
                    engine='pyarrow',
                    compression=compression,
                    index=False
                )

            # 2. ì €ìž¥ ì •ë³´ ë¡œê¹…
            if file_path.is_file():
                file_size = file_path.stat().st_size / 1024**2
            else:
                # íŒŒí‹°ì…˜ëœ ê²½ìš° ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°
                file_size = sum(f.stat().st_size for f in file_path.rglob('*.parquet')) / 1024**2

            memory_size = df.memory_usage(deep=True).sum() / 1024**2
            compression_ratio = memory_size / file_size if file_size > 0 else 0

            logging.info(f"âœ… Saved: {name}.parquet")
            logging.info(f"   ðŸ“Š Rows: {len(df):,} | Columns: {len(df.columns)}")
            logging.info(f"   ðŸ“ File size: {file_size:.2f} MB")
            logging.info(f"   ðŸ’¾ Memory size: {memory_size:.2f} MB")
            logging.info(f"   ðŸ—œï¸  Compression ratio: {compression_ratio:.1f}x")

            # 3. ìƒ˜í”Œ CSV ìƒì„± (ë¹ ë¥¸ í™•ì¸ìš©)
            if save_sample:
                sample_file = self.sample_path / f"{name}_sample.csv"
                df.head(sample_size).to_csv(sample_file, index=False)
                sample_size_kb = sample_file.stat().st_size / 1024
                logging.info(f"   ðŸ“„ Sample saved: {sample_file.name} ({sample_size_kb:.1f} KB)")

            # 4. ìžë™ ê²€ì¦
            if self.auto_validate and not partition_cols:  # íŒŒí‹°ì…˜ì€ ê²€ì¦ ìŠ¤í‚µ
                logging.info(f"   ðŸ” Validating {name}...")
                result = self.validator.validate_file(str(file_path), name)

                if not result['passed']:
                    logging.error(f"   âŒ Validation failed for {name}")
                    for error in result['errors']:
                        logging.error(f"      {error}")
                    return False
                else:
                    logging.info(f"   âœ… Validation passed")

                    # ê²½ê³ ê°€ ìžˆìœ¼ë©´ ì¶œë ¥
                    for warning in result['warnings']:
                        logging.warning(f"      âš ï¸  {warning}")

            return True

        except Exception as e:
            logging.error(f"âŒ Failed to save {name}: {e}")
            return False

    def load_parquet(self,
                    name: str,
                    columns: Optional[List[str]] = None,
                    filters: Optional[List] = None) -> pd.DataFrame:
        """
        Parquet íŒŒì¼ ë¡œë“œ

        Args:
            name: í…Œì´ë¸” ì´ë¦„
            columns: ë¡œë“œí•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
            filters: PyArrow í•„í„° (ì˜ˆ: [('year', '=', 2020)])

        Returns:
            DataFrame
        """
        file_path = self.parquet_path / f"{name}.parquet"

        if not file_path.exists():
            # íŒŒí‹°ì…˜ ë””ë ‰í† ë¦¬ì¸ì§€ í™•ì¸
            partition_path = self.parquet_path / name
            if partition_path.exists() and partition_path.is_dir():
                file_path = partition_path
            else:
                raise FileNotFoundError(f"Parquet file not found: {file_path}")

        logging.info(f"Loading {name}.parquet...")

        df = pd.read_parquet(
            file_path,
            columns=columns,
            filters=filters,
            engine='pyarrow'
        )

        logging.info(f"âœ… Loaded: {len(df):,} rows, {len(df.columns)} columns")
        return df

    def get_info(self, name: str) -> dict:
        """
        Parquet íŒŒì¼ ì •ë³´ ì¡°íšŒ (ë©”íƒ€ë°ì´í„°ë§Œ, ë°ì´í„° ë¡œë“œ ì—†ìŒ)

        Args:
            name: í…Œì´ë¸” ì´ë¦„

        Returns:
            íŒŒì¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        file_path = self.parquet_path / f"{name}.parquet"

        if not file_path.exists():
            return {'error': 'File not found'}

        # PyArrowë¡œ ë©”íƒ€ë°ì´í„°ë§Œ ì½ê¸° (ë¹ ë¦„)
        import pyarrow.parquet as pq

        parquet_file = pq.ParquetFile(file_path)
        metadata = parquet_file.metadata
        schema = parquet_file.schema_arrow

        return {
            'name': name,
            'rows': metadata.num_rows,
            'row_groups': metadata.num_row_groups,
            'columns': len(schema),
            'column_names': schema.names,
            'dtypes': {field.name: str(field.type) for field in schema},
            'size_mb': file_path.stat().st_size / 1024**2,
            'created': metadata.created_by if hasattr(metadata, 'created_by') else None
        }

    def list_tables(self) -> List[str]:
        """ì €ìž¥ëœ ëª¨ë“  í…Œì´ë¸” ì´ë¦„ ë°˜í™˜"""
        tables = []

        # íŒŒì¼
        for file_path in self.parquet_path.glob("*.parquet"):
            tables.append(file_path.stem)

        # íŒŒí‹°ì…˜ ë””ë ‰í† ë¦¬
        for dir_path in self.parquet_path.iterdir():
            if dir_path.is_dir() and list(dir_path.glob("*.parquet")):
                tables.append(dir_path.name)

        return sorted(tables)

    def delete_table(self, name: str) -> bool:
        """í…Œì´ë¸” ì‚­ì œ"""
        file_path = self.parquet_path / f"{name}.parquet"
        partition_path = self.parquet_path / name

        try:
            if file_path.exists():
                file_path.unlink()
                logging.info(f"ðŸ—‘ï¸  Deleted: {name}.parquet")
            elif partition_path.exists() and partition_path.is_dir():
                import shutil
                shutil.rmtree(partition_path)
                logging.info(f"ðŸ—‘ï¸  Deleted: {name}/ (partitioned)")
            else:
                logging.warning(f"Table not found: {name}")
                return False

            # ìƒ˜í”Œ íŒŒì¼ë„ ì‚­ì œ
            sample_file = self.sample_path / f"{name}_sample.csv"
            if sample_file.exists():
                sample_file.unlink()

            return True

        except Exception as e:
            logging.error(f"Failed to delete {name}: {e}")
            return False

    def validate_all_tables(self) -> dict:
        """ëª¨ë“  í…Œì´ë¸” ê²€ì¦"""
        return self.validator.validate_all(str(self.parquet_path))

    def generate_validation_report(self, output_file: str = "validation_report.txt") -> str:
        """ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±"""
        results = self.validate_all_tables()
        return self.validator.generate_report(results, output_file)

    def get_statistics(self) -> dict:
        """ì €ìž¥ì†Œ ì „ì²´ í†µê³„"""
        tables = self.list_tables()
        total_size = 0
        total_rows = 0

        stats = {
            'total_tables': len(tables),
            'tables': []
        }

        for table in tables:
            try:
                info = self.get_info(table)
                total_size += info.get('size_mb', 0)
                total_rows += info.get('rows', 0)
                stats['tables'].append({
                    'name': table,
                    'rows': info.get('rows', 0),
                    'size_mb': info.get('size_mb', 0)
                })
            except Exception as e:
                logging.warning(f"Failed to get info for {table}: {e}")

        stats['total_size_mb'] = total_size
        stats['total_rows'] = total_rows

        return stats
