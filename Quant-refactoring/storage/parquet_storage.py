"""ìë™ ê²€ì¦ ë° ìƒ˜í”Œ ìƒì„± ê¸°ëŠ¥ì´ ìˆëŠ” Parquet ì €ì¥ì†Œ ëª¨ë“ˆì…ë‹ˆë‹¤.

ì´ ëª¨ë“ˆì€ ë‚´ì¥ ê²€ì¦, ìƒ˜í”Œ ìƒì„±, ë©”íƒ€ë°ì´í„° ì¶”ì  ê¸°ëŠ¥ì´ ìˆëŠ” Parquet íŒŒì¼ ê´€ë¦¬ë¥¼ ìœ„í•œ
í¬ê´„ì ì¸ ParquetStorage í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìë™ ì••ì¶•, íŒŒí‹°ì…”ë‹, ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ë¥¼
ì§€ì›í•©ë‹ˆë‹¤.

ParquetStorage í´ë˜ìŠ¤ëŠ” PyArrow Parquet ì‘ì—…ì„ ë˜í•‘í•˜ê³  ë‹¤ìŒì„ ì¶”ê°€í•©ë‹ˆë‹¤:
    - ì €ì¥ í›„ ìë™ ë°ì´í„° ê²€ì¦
    - ë¹ ë¥¸ ê²€ì‚¬ë¥¼ ìœ„í•œ ìƒ˜í”Œ CSV ìƒì„±
    - ì••ì¶• í†µê³„ ë° ë¦¬í¬íŒ…
    - ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ ì—†ì´ ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬
    - ì €ì¥ëœ ëª¨ë“  í…Œì´ë¸”ì˜ ë°°ì¹˜ ê²€ì¦

ì‚¬ìš© ì˜ˆì‹œ:
    ìë™ ê²€ì¦ì„ ì‚¬ìš©í•œ ê¸°ë³¸ ì‚¬ìš©ë²•::

        from storage import ParquetStorage
        import pandas as pd

        # ìë™ ê²€ì¦ì´ í™œì„±í™”ëœ ì €ì¥ì†Œ ì´ˆê¸°í™”
        storage = ParquetStorage(
            root_path="/data/stocks",
            auto_validate=True
        )

        # ì••ì¶•ê³¼ í•¨ê»˜ DataFrame ì €ì¥
        df = pd.DataFrame({
            'symbol': ['AAPL', 'GOOGL', 'MSFT'],
            'price': [150.0, 2800.0, 300.0]
        })

        success = storage.save_parquet(
            df=df,
            name='stock_prices',
            compression='snappy',
            save_sample=True
        )

        # ì»¬ëŸ¼ í•„í„°ë§ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ
        df_loaded = storage.load_parquet(
            name='stock_prices',
            columns=['symbol', 'price']
        )

        # ë°ì´í„° ë¡œë“œ ì—†ì´ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        info = storage.get_info('stock_prices')
        print(f"Rows: {info['rows']}, Size: {info['size_mb']:.2f} MB")

Attributes:
    Module-level exports: ParquetStorage, DataValidator
"""

import logging
import pandas as pd
from pathlib import Path
from typing import Optional, List, Union, Dict, Any
from .data_validator import DataValidator


class ParquetStorage:
    """ê²€ì¦ ë° ìƒ˜í”Œ ìƒì„± ê¸°ëŠ¥ì´ ê°•í™”ëœ Parquet ì €ì¥ì†Œ ê´€ë¦¬ìì…ë‹ˆë‹¤.

    ì´ í´ë˜ìŠ¤ëŠ” ìë™ ë°ì´í„° ê²€ì¦, ìƒ˜í”Œ CSV ìƒì„±, í¬ê´„ì ì¸ ë©”íƒ€ë°ì´í„° ì¶”ì ì´ í¬í•¨ëœ
    Parquet íŒŒì¼ ì €ì¥ ë° ê²€ìƒ‰ì„ ìœ„í•œ ê³ ìˆ˜ì¤€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë””ë ‰í† ë¦¬ êµ¬ì¡°
    ìƒì„±, ì••ì¶•, íŒŒí‹°ì…”ë‹, í’ˆì§ˆ ê²€ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    ì €ì¥ì†ŒëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:
        - parquet/: Parquet íŒŒì¼ì„ ìœ„í•œ ë©”ì¸ ì €ì¥ì†Œ
        - VIEW/: ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ìœ„í•œ ë·° í…Œì´ë¸”
        - samples/: ë¹ ë¥¸ ê²€ì‚¬ë¥¼ ìœ„í•œ CSV ìƒ˜í”Œ

    Attributes:
        root_path (Path): ëª¨ë“  ì €ì¥ì†Œ ì‘ì—…ì„ ìœ„í•œ ë£¨íŠ¸ ë””ë ‰í† ë¦¬.
        parquet_path (Path): Parquet íŒŒì¼ì´ ì €ì¥ë˜ëŠ” ë””ë ‰í† ë¦¬.
        view_path (Path): ë·° í…Œì´ë¸”ì„ ìœ„í•œ ë””ë ‰í† ë¦¬.
        sample_path (Path): ìƒ˜í”Œ CSV íŒŒì¼ì„ ìœ„í•œ ë””ë ‰í† ë¦¬.
        auto_validate (bool): ì €ì¥ í›„ ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê²€ì¦í• ì§€ ì—¬ë¶€.
        validator (DataValidator): ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ë¥¼ ìœ„í•œ ê²€ì¦ê¸° ì¸ìŠ¤í„´ìŠ¤.

    ì‚¬ìš© ì˜ˆì‹œ:
        íŒŒí‹°ì…”ë‹ê³¼ í•¨ê»˜ ì €ì¥ì†Œ ì´ˆê¸°í™” ë° ë°ì´í„° ì €ì¥::

            storage = ParquetStorage(
                root_path="/data/financial",
                auto_validate=True
            )

            # ì—°ë„ íŒŒí‹°ì…”ë‹ê³¼ í•¨ê»˜ ì €ì¥
            df = pd.DataFrame({
                'date': pd.date_range('2020-01-01', periods=1000),
                'symbol': ['AAPL'] * 1000,
                'price': range(1000),
                'year': [2020] * 1000
            })

            storage.save_parquet(
                df=df,
                name='prices',
                partition_cols=['year'],
                compression='zstd'
            )

            # ì €ì¥ëœ ëª¨ë“  í…Œì´ë¸” ë‚˜ì—´
            tables = storage.list_tables()
            print(f"Stored tables: {tables}")

            # ëª¨ë“  í…Œì´ë¸” ê²€ì¦
            results = storage.validate_all_tables()
    """

    def __init__(self, root_path: str, auto_validate: bool = True) -> None:
        """ë””ë ‰í† ë¦¬ êµ¬ì¡°ì™€ í•¨ê»˜ ParquetStorageë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Parquet ì €ì¥ì†Œ, ë·°, ìƒ˜í”Œì„ ìœ„í•œ í•„ìš”í•œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        í’ˆì§ˆ ê²€ì‚¬ë¥¼ ìœ„í•œ ë°ì´í„° ê²€ì¦ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            root_path (str): ëª¨ë“  ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
                ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±ë©ë‹ˆë‹¤.
            auto_validate (bool, optional): Trueì¸ ê²½ìš°, ì €ì¥ í›„ ìë™ìœ¼ë¡œ
                ë°ì´í„°ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ True.

        ì‚¬ìš© ì˜ˆì‹œ:
            ì»¤ìŠ¤í…€ ê²€ì¦ ì„¤ì •ìœ¼ë¡œ ì €ì¥ì†Œ ì´ˆê¸°í™”::

                # ìë™ ê²€ì¦ ì‚¬ìš©
                storage = ParquetStorage(
                    root_path="/data/stocks",
                    auto_validate=True
                )

                # ìë™ ê²€ì¦ ì—†ì´ (ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì— ëŒ€í•´ ë” ë¹ ë¦„)
                storage_fast = ParquetStorage(
                    root_path="/data/stocks",
                    auto_validate=False
                )
        """
        self.root_path = Path(root_path)
        self.parquet_path = self.root_path / "parquet"
        self.view_path = self.root_path / "VIEW"
        self.sample_path = self.root_path / "samples"

        # Create directory structure
        self.parquet_path.mkdir(parents=True, exist_ok=True)
        self.view_path.mkdir(parents=True, exist_ok=True)
        self.sample_path.mkdir(parents=True, exist_ok=True)

        self.auto_validate = auto_validate
        self.validator = DataValidator()

        logging.info(f"ParquetStorage initialized at: {self.root_path}")

    def save_parquet(self,
                    df: pd.DataFrame,
                    name: str,
                    compression: str = 'snappy',
                    save_sample: bool = True,
                    sample_size: int = 100,
                    partition_cols: Optional[List[str]] = None) -> bool:
        """ê²€ì¦ ë° ìƒ˜í”Œ ìƒì„±ê³¼ í•¨ê»˜ DataFrameì„ Parquetìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        ì„ íƒì  ì••ì¶•, íŒŒí‹°ì…”ë‹, ìë™ ê²€ì¦ê³¼ í•¨ê»˜ DataFrameì„ Parquet í˜•ì‹ìœ¼ë¡œ
        ì €ì¥í•©ë‹ˆë‹¤. ë˜í•œ ë¹ ë¥¸ ê²€ì‚¬ë¥¼ ìœ„í•œ ìƒ˜í”Œ CSVë¥¼ ìƒì„±í•©ë‹ˆë‹¤. íŒŒì¼ í¬ê¸°,
        ì••ì¶• ë¹„ìœ¨, ê²€ì¦ ê²°ê³¼ë¥¼ í¬í•¨í•œ ìƒì„¸ í†µê³„ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.

        Args:
            df (pd.DataFrame): ì €ì¥í•  DataFrame. ë¹„ì–´ìˆìœ¼ë©´ ì•ˆ ë©ë‹ˆë‹¤.
            name (str): í…Œì´ë¸” ì´ë¦„ (.parquet í™•ì¥ì ì œì™¸). íŒŒì¼ëª… ë˜ëŠ”
                íŒŒí‹°ì…˜ëœ ê²½ìš° ë””ë ‰í† ë¦¬ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
            compression (str, optional): ì••ì¶• ì•Œê³ ë¦¬ì¦˜. ì§€ì›ë˜ëŠ” ê°’:
                'snappy' (ë¹ ë¦„, ê¸°ë³¸ê°’), 'gzip' (ë” ì‘ìŒ), 'zstd' (ê· í˜•),
                'brotli' (ê°€ì¥ ì‘ìŒ). ê¸°ë³¸ê°’ì€ 'snappy'.
            save_sample (bool, optional): Trueì¸ ê²½ìš°, ë¹ ë¥¸ ê²€ì‚¬ë¥¼ ìœ„í•œ
                ìƒ˜í”Œ CSVë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ True.
            sample_size (int, optional): ìƒ˜í”Œ CSVì˜ í–‰ ìˆ˜. ê¸°ë³¸ê°’ì€ 100.
            partition_cols (Optional[List[str]], optional): íŒŒí‹°ì…”ë‹ì„ ìœ„í•œ
                ì»¬ëŸ¼ ì´ë¦„. íŒŒí‹°ì…˜ ê°’ìœ¼ë¡œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                ì˜ˆ: ['year']ëŠ” year=2020/, year=2021/ í•˜ìœ„ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                ê¸°ë³¸ê°’ì€ None.

        Returns:
            bool: ì €ì¥ ë° ê²€ì¦ì´ ì„±ê³µí•˜ë©´ True, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False.

        Raises:
            Exception: ì €ì¥ ì‘ì—…ì´ ì‹¤íŒ¨í•˜ë©´ ì˜ˆì™¸ë¥¼ ë¡œê¹…í•˜ê³  Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        ì‚¬ìš© ì˜ˆì‹œ:
            ë‹¤ì–‘í•œ ì••ì¶• ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì €ì¥::

                # ë¹ ë¥¸ ì••ì¶• (ê¸°ë³¸ê°’)
                storage.save_parquet(df, 'fast_data', compression='snappy')

                # ìµœëŒ€ ì••ì¶•
                storage.save_parquet(df, 'archive_data', compression='brotli')

                # ì—°ë„ë³„ íŒŒí‹°ì…”ë‹
                storage.save_parquet(
                    df=df,
                    name='historical_prices',
                    partition_cols=['year'],
                    compression='zstd'
                )

                # ìƒ˜í”Œ ìƒì„± ì—†ì´ ì €ì¥
                storage.save_parquet(
                    df=large_df,
                    name='huge_dataset',
                    save_sample=False
                )

        Note:
            - íŒŒí‹°ì…˜ëœ ë°ì´í„°ì…‹ì€ ìë™ ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤
            - ì••ì¶• ë¹„ìœ¨ì€ memory_size / file_sizeë¡œ ê³„ì‚°ë©ë‹ˆë‹¤
            - ê²€ì¦ ì˜¤ë¥˜ëŠ” ë¡œê¹…ë˜ì§€ë§Œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤
        """
        file_path = self.parquet_path / f"{name}.parquet"

        try:
            # 1. Save to Parquet format
            logging.info(f"Saving {name}.parquet...")

            if partition_cols:
                # Partitioned save (directory structure)
                df.to_parquet(
                    self.parquet_path / name,
                    engine='pyarrow',
                    compression=compression,
                    partition_cols=partition_cols,
                    index=False
                )
                file_path = self.parquet_path / name
            else:
                # Single file save
                df.to_parquet(
                    file_path,
                    engine='pyarrow',
                    compression=compression,
                    index=False
                )

            # 2. Calculate and log storage statistics
            if file_path.is_file():
                file_size = file_path.stat().st_size / 1024**2
            else:
                # For partitioned datasets, calculate total directory size
                file_size = sum(f.stat().st_size for f in file_path.rglob('*.parquet')) / 1024**2

            memory_size = df.memory_usage(deep=True).sum() / 1024**2
            compression_ratio = memory_size / file_size if file_size > 0 else 0

            logging.info(f"âœ… Saved: {name}.parquet")
            logging.info(f"   ğŸ“Š Rows: {len(df):,} | Columns: {len(df.columns)}")
            logging.info(f"   ğŸ“ File size: {file_size:.2f} MB")
            logging.info(f"   ğŸ’¾ Memory size: {memory_size:.2f} MB")
            logging.info(f"   ğŸ—œï¸  Compression ratio: {compression_ratio:.1f}x")

            # 3. Generate sample CSV for quick inspection
            if save_sample:
                sample_file = self.sample_path / f"{name}_sample.csv"
                df.head(sample_size).to_csv(sample_file, index=False)
                sample_size_kb = sample_file.stat().st_size / 1024
                logging.info(f"   ğŸ“„ Sample saved: {sample_file.name} ({sample_size_kb:.1f} KB)")

            # 4. Automatic validation (skip for partitioned datasets)
            if self.auto_validate and not partition_cols:
                logging.info(f"   ğŸ” Validating {name}...")
                result = self.validator.validate_file(str(file_path), name)

                if not result['passed']:
                    logging.error(f"   âŒ Validation failed for {name}")
                    for error in result['errors']:
                        logging.error(f"      {error}")
                    return False
                else:
                    logging.info(f"   âœ… Validation passed")

                    # Log warnings if present
                    for warning in result['warnings']:
                        logging.warning(f"      âš ï¸  {warning}")

            return True

        except Exception as e:
            logging.error(f"âŒ Failed to save {name}: {e}")
            return False

    def load_parquet(self,
                    name: str,
                    columns: Optional[List[str]] = None,
                    filters: Optional[List] = None) -> pd.DataFrame:
        """Parquet íŒŒì¼ ë˜ëŠ” íŒŒí‹°ì…˜ëœ ë°ì´í„°ì…‹ì„ DataFrameìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.

        ì„ íƒì  ì»¬ëŸ¼ ì„ íƒ ë° í–‰ í•„í„°ë§ê³¼ í•¨ê»˜ Parquet ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
        ë‹¨ì¼ íŒŒì¼ ë° íŒŒí‹°ì…˜ëœ ë°ì´í„°ì…‹ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤. ë¹ ë¥¸ I/O ì‘ì—…ì„ ìœ„í•´
        PyArrowë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            name (str): í…Œì´ë¸” ì´ë¦„ (.parquet í™•ì¥ì ì œì™¸).
            columns (Optional[List[str]], optional): ë¡œë“œí•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸.
                Noneì¸ ê²½ìš°, ëª¨ë“  ì»¬ëŸ¼ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì»¬ëŸ¼ í•„í„°ë§ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„
                ì¤„ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ None.
            filters (Optional[List], optional): í–‰ í•„í„°ë§ì„ ìœ„í•œ PyArrow í•„í„° í‘œí˜„ì‹.
                í˜•ì‹: [('column', 'operator', value)].
                ì˜ˆ: [('year', '=', 2020), ('price', '>', 100)].
                ê¸°ë³¸ê°’ì€ None.

        Returns:
            pd.DataFrame: ì§€ì •ëœ ì»¬ëŸ¼ê³¼ í•„í„°ê°€ ì ìš©ëœ ë¡œë“œëœ DataFrame.

        Raises:
            FileNotFoundError: ì§€ì •ëœ Parquet íŒŒì¼ ë˜ëŠ” íŒŒí‹°ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°.

        ì‚¬ìš© ì˜ˆì‹œ:
            ë‹¤ì–‘í•œ í•„í„°ë§ ì˜µì…˜ìœ¼ë¡œ ë¡œë“œ::

                # ëª¨ë“  ë°ì´í„° ë¡œë“œ
                df = storage.load_parquet('stock_prices')

                # íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¡œë“œ
                df = storage.load_parquet(
                    'stock_prices',
                    columns=['symbol', 'close', 'volume']
                )

                # í–‰ í•„í„°ë§ê³¼ í•¨ê»˜ ë¡œë“œ
                df = storage.load_parquet(
                    'stock_prices',
                    filters=[('date', '>=', '2020-01-01')]
                )

                # ì»¬ëŸ¼ ì„ íƒê³¼ í•„í„°ë§ ê²°í•©
                df = storage.load_parquet(
                    'stock_prices',
                    columns=['symbol', 'close'],
                    filters=[('symbol', 'in', ['AAPL', 'GOOGL'])]
                )

        Note:
            - ì»¬ëŸ¼ í•„í„°ë§ì€ ì½ê¸° ì‹œì ì— ë°œìƒí•˜ì—¬ I/Oë¥¼ ì¤„ì…ë‹ˆë‹¤
            - PyArrow í•„í„°ëŠ” ë‹¤ìŒì„ ì§€ì›í•©ë‹ˆë‹¤: =, !=, <, <=, >, >=, in, not in
            - íŒŒí‹°ì…˜ëœ ë°ì´í„°ì…‹ì˜ ê²½ìš°, í•„í„°ëŠ” íŒŒí‹°ì…˜ ì»¬ëŸ¼ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        """
        file_path = self.parquet_path / f"{name}.parquet"

        if not file_path.exists():
            # Check if it's a partitioned directory
            partition_path = self.parquet_path / name
            if partition_path.exists() and partition_path.is_dir():
                file_path = partition_path
            else:
                raise FileNotFoundError(f"Parquet file not found: {file_path}")

        logging.info(f"Loading {name}.parquet...")

        df = pd.read_parquet(
            file_path,
            columns=columns,
            filters=filters,
            engine='pyarrow'
        )

        logging.info(f"âœ… Loaded: {len(df):,} rows, {len(df.columns)} columns")
        return df

    def get_info(self, name: str) -> Dict[str, Any]:
        """ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ì•Šê³  Parquet íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

        ì‹¤ì œ ë°ì´í„°ë¥¼ ì½ì§€ ì•Šê³  ìŠ¤í‚¤ë§ˆ, í–‰ ìˆ˜, íŒŒì¼ í¬ê¸°ë¥¼ í¬í•¨í•œ íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼
        ê²€ìƒ‰í•©ë‹ˆë‹¤. ì´ê²ƒì€ ì „ì²´ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤.
        PyArrowì˜ ë©”íƒ€ë°ì´í„° ì½ê¸° ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            name (str): í…Œì´ë¸” ì´ë¦„ (.parquet í™•ì¥ì ì œì™¸).

        Returns:
            Dict[str, Any]: ë‹¤ìŒ í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬:
                - name (str): í…Œì´ë¸” ì´ë¦„
                - rows (int): í–‰ ìˆ˜
                - row_groups (int): í–‰ ê·¸ë£¹ ìˆ˜
                - columns (int): ì»¬ëŸ¼ ìˆ˜
                - column_names (List[str]): ì»¬ëŸ¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
                - dtypes (Dict[str, str]): ì»¬ëŸ¼ ì´ë¦„ì„ ë°ì´í„° íƒ€ì…ì— ë§¤í•‘
                - size_mb (float): íŒŒì¼ í¬ê¸° (MB)
                - created (Optional[str]): ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ìƒì„± ë©”íƒ€ë°ì´í„°

                íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ {'error': 'File not found'}ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        ì‚¬ìš© ì˜ˆì‹œ:
            ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ì•Šê³  ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬::

                # ê¸°ë³¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                info = storage.get_info('stock_prices')
                print(f"Rows: {info['rows']:,}")
                print(f"Columns: {info['column_names']}")
                print(f"Size: {info['size_mb']:.2f} MB")

                # ë¡œë“œí•˜ê¸° ì „ì— ìŠ¤í‚¤ë§ˆ í™•ì¸
                info = storage.get_info('financial_data')
                if 'revenue' in info['column_names']:
                    df = storage.load_parquet('financial_data')

                # ëª¨ë“  í…Œì´ë¸” í¬ê¸° ê°€ì ¸ì˜¤ê¸°
                for table in storage.list_tables():
                    info = storage.get_info(table)
                    print(f"{table}: {info['size_mb']:.2f} MB")

        Note:
            - íŒŒì¼ ë©”íƒ€ë°ì´í„°ë§Œ ì½ê³ , ì‹¤ì œ ë°ì´í„°ëŠ” ì½ì§€ ì•ŠìŠµë‹ˆë‹¤
            - ì „ì²´ DataFrameì„ ë¡œë“œí•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤
            - íŒŒí‹°ì…˜ëœ ë°ì´í„°ì…‹(ë””ë ‰í† ë¦¬ êµ¬ì¡°)ì—ì„œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        """
        file_path = self.parquet_path / f"{name}.parquet"

        if not file_path.exists():
            return {'error': 'File not found'}

        # Read metadata only using PyArrow (fast operation)
        import pyarrow.parquet as pq

        parquet_file = pq.ParquetFile(file_path)
        metadata = parquet_file.metadata
        schema = parquet_file.schema_arrow

        return {
            'name': name,
            'rows': metadata.num_rows,
            'row_groups': metadata.num_row_groups,
            'columns': len(schema),
            'column_names': schema.names,
            'dtypes': {field.name: str(field.type) for field in schema},
            'size_mb': file_path.stat().st_size / 1024**2,
            'created': metadata.created_by if hasattr(metadata, 'created_by') else None
        }

    def list_tables(self) -> List[str]:
        """ì €ì¥ëœ ëª¨ë“  í…Œì´ë¸” ì´ë¦„ì„ ë‚˜ì—´í•©ë‹ˆë‹¤.

        ë‹¨ì¼ íŒŒì¼ í…Œì´ë¸”ê³¼ íŒŒí‹°ì…˜ëœ ë°ì´í„°ì…‹(Parquet íŒŒì¼ì„ í¬í•¨í•˜ëŠ” ë””ë ‰í† ë¦¬)ì„
        ëª¨ë‘ ìœ„í•´ Parquet ë””ë ‰í† ë¦¬ë¥¼ ìŠ¤ìº”í•©ë‹ˆë‹¤. íŒŒì¼ í™•ì¥ì ì—†ì´ ì •ë ¬ëœ
        í…Œì´ë¸” ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Returns:
            List[str]: í…Œì´ë¸” ì´ë¦„ì˜ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ (.parquet í™•ì¥ì ì œì™¸).

        ì‚¬ìš© ì˜ˆì‹œ:
            ëª¨ë“  í…Œì´ë¸” ë‚˜ì—´ ë° ë°˜ë³µ::

                # ëª¨ë“  í…Œì´ë¸” ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                tables = storage.list_tables()
                print(f"Found {len(tables)} tables: {tables}")

                # ëª¨ë“  í…Œì´ë¸” ì²˜ë¦¬
                for table_name in storage.list_tables():
                    df = storage.load_parquet(table_name)
                    print(f"{table_name}: {len(df)} rows")

                # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
                if 'stock_prices' in storage.list_tables():
                    df = storage.load_parquet('stock_prices')
        """
        tables = []

        # Find single Parquet files
        for file_path in self.parquet_path.glob("*.parquet"):
            tables.append(file_path.stem)

        # Find partitioned directories
        for dir_path in self.parquet_path.iterdir():
            if dir_path.is_dir() and list(dir_path.glob("*.parquet")):
                tables.append(dir_path.name)

        return sorted(tables)

    def delete_table(self, name: str) -> bool:
        """í…Œì´ë¸”ê³¼ ê´€ë ¨ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.

        Parquet íŒŒì¼ ë˜ëŠ” íŒŒí‹°ì…˜ëœ ë””ë ‰í† ë¦¬ì™€ ê´€ë ¨ ìƒ˜í”Œ CSV íŒŒì¼ì„ ì œê±°í•©ë‹ˆë‹¤.
        ë‹¨ì¼ íŒŒì¼ ë° íŒŒí‹°ì…˜ëœ ë°ì´í„°ì…‹ì„ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            name (str): ì‚­ì œí•  í…Œì´ë¸” ì´ë¦„ (.parquet í™•ì¥ì ì œì™¸).

        Returns:
            bool: ì‚­ì œê°€ ì„±ê³µí•˜ë©´ True, í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í•˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ False.

        ì‚¬ìš© ì˜ˆì‹œ:
            í™•ì¸ê³¼ í•¨ê»˜ í…Œì´ë¸” ì‚­ì œ::

                # ë‹¨ì¼ í…Œì´ë¸” ì‚­ì œ
                if storage.delete_table('old_data'):
                    print("Table deleted successfully")

                # ì—¬ëŸ¬ í…Œì´ë¸” ì‚­ì œ
                tables_to_delete = ['temp1', 'temp2', 'test_data']
                for table in tables_to_delete:
                    storage.delete_table(table)

                # ì¡´ì¬ í™•ì¸ê³¼ í•¨ê»˜ ì‚­ì œ
                if 'deprecated_table' in storage.list_tables():
                    storage.delete_table('deprecated_table')

        Note:
            - ê´€ë ¨ ìƒ˜í”Œ CSV íŒŒì¼ë„ ì‚­ì œí•©ë‹ˆë‹¤
            - íŒŒí‹°ì…˜ëœ ë°ì´í„°ì…‹ì˜ ê²½ìš°, ì „ì²´ ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤
            - í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê²½ê³ ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤
        """
        file_path = self.parquet_path / f"{name}.parquet"
        partition_path = self.parquet_path / name

        try:
            if file_path.exists():
                file_path.unlink()
                logging.info(f"ğŸ—‘ï¸  Deleted: {name}.parquet")
            elif partition_path.exists() and partition_path.is_dir():
                import shutil
                shutil.rmtree(partition_path)
                logging.info(f"ğŸ—‘ï¸  Deleted: {name}/ (partitioned)")
            else:
                logging.warning(f"Table not found: {name}")
                return False

            # Delete associated sample file
            sample_file = self.sample_path / f"{name}_sample.csv"
            if sample_file.exists():
                sample_file.unlink()

            return True

        except Exception as e:
            logging.error(f"Failed to delete {name}: {e}")
            return False

    def validate_all_tables(self) -> Dict[str, Dict[str, Any]]:
        """ì €ì¥ëœ ëª¨ë“  Parquet í…Œì´ë¸”ì„ ê²€ì¦í•©ë‹ˆë‹¤.

        ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  í…Œì´ë¸”ì— ëŒ€í•´ í¬ê´„ì ì¸ ê²€ì¦ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        ë°ì´í„° í’ˆì§ˆ, ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„±, í•„ìˆ˜ ì»¬ëŸ¼, ë°ì´í„° íƒ€ì…ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
        ìƒì„¸ ê²€ì¦ ê·œì¹™ì„ ìœ„í•´ DataValidatorë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Returns:
            Dict[str, Dict[str, Any]]: í…Œì´ë¸” ì´ë¦„ì„ ê²€ì¦ ê²°ê³¼ì— ë§¤í•‘í•˜ëŠ”
                ì¤‘ì²© ë”•ì…”ë„ˆë¦¬. ê° ê²°ê³¼ëŠ” ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
                - passed (bool): ì „ì²´ ê²€ì¦ ìƒíƒœ
                - errors (List[str]): ê²€ì¦ ì˜¤ë¥˜ ë¦¬ìŠ¤íŠ¸
                - warnings (List[str]): ê²½ê³  ë¦¬ìŠ¤íŠ¸
                - info (Dict): ë©”íƒ€ë°ì´í„° ë° í†µê³„

        ì‚¬ìš© ì˜ˆì‹œ:
            ëª¨ë“  í…Œì´ë¸” ê²€ì¦ ë° ê²°ê³¼ í™•ì¸::

                # ëª¨ë“  í…Œì´ë¸”ì— ëŒ€í•´ ê²€ì¦ ì‹¤í–‰
                results = storage.validate_all_tables()

                # ì–´ë–¤ í…Œì´ë¸”ì´ í†µê³¼í–ˆëŠ”ì§€ í™•ì¸
                for table_name, result in results.items():
                    if result['passed']:
                        print(f"âœ… {table_name}: OK")
                    else:
                        print(f"âŒ {table_name}: FAILED")
                        for error in result['errors']:
                            print(f"   - {error}")

                # ê²€ì¦ ë¬¸ì œ ìˆ˜ ê³„ì‚°
                failed = sum(1 for r in results.values() if not r['passed'])
                print(f"{failed} tables failed validation")

        See Also:
            generate_validation_report: ê²€ì¦ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±.
        """
        return self.validator.validate_all(str(self.parquet_path))

    def generate_validation_report(self, output_file: str = "validation_report.txt") -> str:
        """ê²€ì¦ ê²°ê³¼ì˜ í…ìŠ¤íŠ¸ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ëª¨ë“  í…Œì´ë¸”ì„ ê²€ì¦í•˜ê³  ìƒì„¸í•œ ë³´ê³ ì„œë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì‘ì„±í•©ë‹ˆë‹¤.
        ë³´ê³ ì„œëŠ” ê° í…Œì´ë¸”ì— ëŒ€í•œ ê²€ì¦ ìƒíƒœ, ì˜¤ë¥˜, ê²½ê³ , í†µê³„ë¥¼ ì‚¬ëŒì´ ì½ê¸°
        ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ í¬í•¨í•©ë‹ˆë‹¤.

        Args:
            output_file (str, optional): ì¶œë ¥ ë³´ê³ ì„œ íŒŒì¼ ê²½ë¡œ. ì ˆëŒ€ ë˜ëŠ”
                ìƒëŒ€ ê²½ë¡œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ "validation_report.txt".

        Returns:
            str: ìƒì„±ëœ ë³´ê³ ì„œ íŒŒì¼ ê²½ë¡œ.

        ì‚¬ìš© ì˜ˆì‹œ:
            ê²€ì¦ ë³´ê³ ì„œ ìƒì„± ë° ê²€í† ::

                # ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±
                report_path = storage.generate_validation_report()
                print(f"Report saved to: {report_path}")

                # ì»¤ìŠ¤í…€ ì´ë¦„ìœ¼ë¡œ ìƒì„±
                report_path = storage.generate_validation_report(
                    output_file="/reports/parquet_validation_2024.txt"
                )

                # ë³´ê³ ì„œ ì½ê¸° ë° ì¶œë ¥
                with open(report_path, 'r') as f:
                    print(f.read())

        Note:
            - ë³´ê³ ì„œëŠ” ëª¨ë“  í…Œì´ë¸”ì— ëŒ€í•œ ìš”ì•½ í†µê³„ë¥¼ í¬í•¨í•©ë‹ˆë‹¤
            - ëª¨ë“  ì˜¤ë¥˜ì™€ ê²½ê³ ë¥¼ ìƒì„¸íˆ ë‚˜ì—´í•©ë‹ˆë‹¤
            - ì¶œë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë®ì–´ì”Œì›Œì§‘ë‹ˆë‹¤
        """
        results = self.validate_all_tables()
        return self.validator.generate_report(results, output_file)

    def get_statistics(self) -> Dict[str, Any]:
        """ì „ì²´ ì €ì¥ì†Œì— ëŒ€í•œ í¬ê´„ì ì¸ í†µê³„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

        ì´ í¬ê¸°, í–‰ ìˆ˜, í…Œì´ë¸”ë³„ í†µê³„ë¥¼ í¬í•¨í•˜ì—¬ ì €ì¥ëœ ëª¨ë“  í…Œì´ë¸”ì— ëŒ€í•œ
        ì •ë³´ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤. ì €ì¥ì†Œ ì‚¬ìš©ëŸ‰ ë° ë°ì´í„° ë³¼ë¥¨ ëª¨ë‹ˆí„°ë§ì— ìœ ìš©í•©ë‹ˆë‹¤.

        Returns:
            Dict[str, Any]: ë‹¤ìŒì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬:
                - total_tables (int): ì €ì¥ëœ í…Œì´ë¸” ìˆ˜
                - total_size_mb (float): ì´ ì €ì¥ì†Œ í¬ê¸° (MB)
                - total_rows (int): ëª¨ë“  í…Œì´ë¸”ì˜ ì´ í–‰ ìˆ˜
                - tables (List[Dict]): ë‹¤ìŒ í‚¤ë¥¼ ê°€ì§„ í…Œì´ë¸”ë³„ í†µê³„:
                    - name (str): í…Œì´ë¸” ì´ë¦„
                    - rows (int): í–‰ ìˆ˜
                    - size_mb (float): íŒŒì¼ í¬ê¸° (MB)

        ì‚¬ìš© ì˜ˆì‹œ:
            ì €ì¥ì†Œ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§::

                # ì „ì²´ í†µê³„ ê°€ì ¸ì˜¤ê¸°
                stats = storage.get_statistics()
                print(f"Total tables: {stats['total_tables']}")
                print(f"Total size: {stats['total_size_mb']:.2f} MB")
                print(f"Total rows: {stats['total_rows']:,}")

                # ê°€ì¥ í° í…Œì´ë¸” ì°¾ê¸°
                tables = sorted(
                    stats['tables'],
                    key=lambda x: x['size_mb'],
                    reverse=True
                )
                print("\nTop 5 largest tables:")
                for table in tables[:5]:
                    print(f"  {table['name']}: {table['size_mb']:.2f} MB")

                # í‰ê·  í…Œì´ë¸” í¬ê¸° ê³„ì‚°
                avg_size = stats['total_size_mb'] / stats['total_tables']
                print(f"Average table size: {avg_size:.2f} MB")

        Note:
            - ë§ì€ ëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì˜ ê²½ìš° í†µê³„ ê³„ì‚°ì´ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤
            - ì½ì„ ìˆ˜ ì—†ëŠ” í…Œì´ë¸”ì€ ê±´ë„ˆëœë‹ˆë‹¤ (ê²½ê³  ë¡œê¹…)
            - í¬ê¸° ê³„ì‚°ì— ìƒ˜í”Œ CSV íŒŒì¼ì€ í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        """
        tables = self.list_tables()
        total_size = 0
        total_rows = 0

        stats = {
            'total_tables': len(tables),
            'tables': []
        }

        for table in tables:
            try:
                info = self.get_info(table)
                total_size += info.get('size_mb', 0)
                total_rows += info.get('rows', 0)
                stats['tables'].append({
                    'name': table,
                    'rows': info.get('rows', 0),
                    'size_mb': info.get('size_mb', 0)
                })
            except Exception as e:
                logging.warning(f"Failed to get info for {table}: {e}")

        stats['total_size_mb'] = total_size
        stats['total_rows'] = total_rows

        return stats
