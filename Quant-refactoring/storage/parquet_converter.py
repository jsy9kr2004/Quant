"""ê¸ˆìœµ ë°ì´í„° í†µí•©ì„ ìœ„í•œ ë ˆê±°ì‹œ Parquet ë³€í™˜ê¸°ì…ë‹ˆë‹¤.

ì´ ëª¨ë“ˆì€ ì—¬ëŸ¬ CSV ë˜ëŠ” Parquet ì†ŒìŠ¤ íŒŒì¼ì˜ ê¸ˆìœµ ë°ì´í„°ë¥¼ í†µí•©ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ
ë³€í™˜í•˜ê³  í†µí•©í•˜ëŠ” Parquet í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì†ŒìŠ¤(ì£¼ì‹ ëª©ë¡, ê°€ê²© ê¸°ë¡,
ì¬ë¬´ì œí‘œ, ë©”íŠ¸ë¦­)ì˜ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ê³  ì—°ë„ë³„ë¡œ êµ¬ì„±ëœ ë·° í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.

ë³€í™˜ê¸°ê°€ ì²˜ë¦¬í•˜ëŠ” ì‘ì—…:
    - ë°ì´í„° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì—¬ëŸ¬ CSV/Parquet íŒŒì¼ ë³‘í•©
    - ì›ì‹œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ í†µí•© ë·° í…Œì´ë¸” êµ¬ì¶•
    - ë‚ ì§œ íƒ€ì… ë³€í™˜ ë° ë°ì´í„° ì •ì œ
    - íš¨ìœ¨ì ì¸ ì¿¼ë¦¬ë¥¼ ìœ„í•œ ì—°ë„ë³„ íŒŒí‹°ì…˜ ë°ì´í„°ì…‹ ìƒì„±
    - ëˆ„ë½ë˜ê±°ë‚˜ ì†ìƒëœ ì†ŒìŠ¤ íŒŒì¼ì— ëŒ€í•œ ì˜¤ë¥˜ ì²˜ë¦¬

ì‚¬ìš© ì˜ˆì‹œ:
    ë°ì´í„° í†µí•©ì„ ìœ„í•œ ê¸°ë³¸ ì‚¬ìš©ë²•::

        from storage import Parquet

        # ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ê¸° ì´ˆê¸°í™”
        converter = Parquet(main_ctx)

        # ëª¨ë“  CSV íŒŒì¼ì„ ì¹´í…Œê³ ë¦¬ë³„ ë‹¨ì¼ íŒŒì¼ë¡œ í†µí•©
        converter.insert_csv()

        # ì›ì‹œ ë°ì´í„°ì—ì„œ í†µí•© ë·° í…Œì´ë¸” êµ¬ì¶•
        converter.rebuild_table_view()

Note:
    ì´ê²ƒì€ CSV íŒŒì¼ì„ ë‹¤ë£¨ê³  í†µí•© í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë ˆê±°ì‹œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    ìƒˆë¡œìš´ ì½”ë“œì˜ ê²½ìš°, ë‚´ì¥ ê²€ì¦ ê¸°ëŠ¥ì´ ìˆëŠ” ë³´ë‹¤ í˜„ëŒ€ì ì¸ APIë¥¼ ì œê³µí•˜ëŠ”
    ParquetStorage ì‚¬ìš©ì„ ê³ ë ¤í•˜ì‹­ì‹œì˜¤.

TODO:
    - ë” ë‚˜ì€ ê²€ì¦ì„ ìœ„í•´ ParquetStorage ì‚¬ìš©ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³ ë ¤
    - ì¥ì‹œê°„ ì‹¤í–‰ ì‘ì—…ì— ëŒ€í•œ ì§„í–‰ë¥  í‘œì‹œê¸° ì¶”ê°€
    - ì „ì²´ ì¬êµ¬ì¶• ëŒ€ì‹  ì¦ë¶„ ì—…ë°ì´íŠ¸ êµ¬í˜„
"""

import datetime
import logging
import os
from typing import Any, List, Optional

import pandas as pd
from tqdm import tqdm


class Parquet:
    """ë ˆê±°ì‹œ Parquet ë°ì´í„° ë³€í™˜ê¸° ë° í†µí•©ê¸°ì…ë‹ˆë‹¤.

    ì´ í´ë˜ìŠ¤ëŠ” ì—¬ëŸ¬ ì†ŒìŠ¤ íŒŒì¼ì˜ ê¸ˆìœµ ë°ì´í„°ë¥¼ í†µí•©ëœ CSV ë°ì´í„°ì…‹ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.
    ì£¼ì‹ ëª©ë¡, ê°€ê²© ê¸°ë¡, ì¬ë¬´ì œí‘œ, ë©”íŠ¸ë¦­ ë°ì´í„° ë³‘í•©ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ëŠ”
    êµ¬ì„±ê³¼ ìœ í‹¸ë¦¬í‹°ë¥¼ ì œê³µí•˜ëŠ” ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ê°ì²´ì™€ í•¨ê»˜ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

    ë³€í™˜ê¸°ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
        1. insert_csv(): ì¹´í…Œê³ ë¦¬ë³„ ì—¬ëŸ¬ íŒŒì¼ì„ ë‹¨ì¼ íŒŒì¼ë¡œ í†µí•©
        2. rebuild_table_view(): ê´€ë ¨ ë°ì´í„°ì…‹ì„ í†µí•© ë·° í…Œì´ë¸”ë¡œ ë³‘í•©

    Attributes:
        main_ctx: êµ¬ì„±(start_year, end_year, root_path)ê³¼ ìœ í‹¸ë¦¬í‹°(create_dir ë©”ì„œë“œ)ë¥¼
            ì œê³µí•˜ëŠ” ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ê°ì²´.
        tables (dict): ë¡œë“œëœ DataFrameì„ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ (í˜„ì¬ ë¯¸ì‚¬ìš©).
        view_path (str): í†µí•© ë·° í…Œì´ë¸”ì„ ì €ì¥í•˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        rawpq_path (str): ì›ì‹œ Parquet/CSV íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ.

    ì‚¬ìš© ì˜ˆì‹œ:
        ì´ˆê¸°í™” ë° ì „ì²´ ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰::

            # main_ctxê°€ í•„ìš”í•œ ì†ì„±ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ê°€ì •
            converter = Parquet(main_ctx)

            # 1ë‹¨ê³„: ì›ì‹œ íŒŒì¼ í†µí•©
            converter.insert_csv()

            # 2ë‹¨ê³„: ë·° í…Œì´ë¸” êµ¬ì¶•
            converter.rebuild_table_view()

    Note:
        ì´ í´ë˜ìŠ¤ëŠ” íŠ¹ì • ë””ë ‰í† ë¦¬ êµ¬ì¡°ì™€ íŒŒì¼ ëª…ëª… ê·œì¹™ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.
        ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤: balance_sheet_statement, cash_flow_statement,
        delisted_companies, earning_calendar, financial_growth,
        historical_daily_discounted_cash_flow, historical_market_capitalization,
        historical_price_full, income_statement, key_metrics, profile,
        stock_list, symbol_available_indexes.
    """

    def __init__(self, main_ctx: Any) -> None:
        """ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ë¡œ Parquet ë³€í™˜ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ì›ì‹œ Parquet íŒŒì¼ê³¼ ë·° í…Œì´ë¸”ì„ ìœ„í•œ í•„ìš”í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        í…Œì´ë¸” ë°ì´í„°ë¥¼ ìœ„í•œ ì €ì¥ì†Œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            main_ctx: ë‹¤ìŒì„ ì œê³µí•´ì•¼ í•˜ëŠ” ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ê°ì²´:
                - root_path (str): ëª¨ë“  ë°ì´í„°ë¥¼ ìœ„í•œ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
                - start_year (int): ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ì„ ìœ„í•œ ì‹œì‘ ì—°ë„
                - end_year (int): ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ì„ ìœ„í•œ ì¢…ë£Œ ì—°ë„
                - create_dir(path: str): ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ

        ì‚¬ìš© ì˜ˆì‹œ:
            ì»¨í…ìŠ¤íŠ¸ ê°ì²´ë¡œ ì´ˆê¸°í™”::

                class MainContext:
                    def __init__(self):
                        self.root_path = "/data/financial"
                        self.start_year = 2015
                        self.end_year = 2024

                    def create_dir(self, path):
                        os.makedirs(path, exist_ok=True)

                main_ctx = MainContext()
                converter = Parquet(main_ctx)
        """
        self.main_ctx = main_ctx
        self.tables = dict()
        self.view_path = self.main_ctx.root_path + "/VIEW/"
        self.rawpq_path = self.main_ctx.root_path + "/parquet/"

        # Create necessary directories
        self.main_ctx.create_dir(self.view_path)
        self.main_ctx.create_dir(self.rawpq_path)

    def rebuild_table_view(self) -> None:
        """ì›ì‹œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ í†µí•© ë·° í…Œì´ë¸”ì„ ì¬êµ¬ì¶•í•©ë‹ˆë‹¤.

        ê´€ë ¨ ë°ì´í„°ì…‹ì„ ë³‘í•©í•˜ì—¬ í†µí•© ë·° í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ”
        ë¶„ì„ ì¤€ë¹„ëœ í…Œì´ë¸”ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ë³µì¡í•œ ì¡°ì¸ê³¼ ë°ì´í„° í†µí•©ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:

        1. ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ í…Œì´ë¸”: ì£¼ì‹ ëª©ë¡, ìƒì¥ íì§€ ê¸°ì—…, í”„ë¡œí•„ ë³‘í•©
        2. ê°€ê²© í…Œì´ë¸”: ê³¼ê±° ê°€ê²©ê³¼ ì‹œê°€ì´ì•¡ ê²°í•©
        3. ì¬ë¬´ì œí‘œ í…Œì´ë¸”: ì†ìµê³„ì‚°ì„œ, ì¬ë¬´ìƒíƒœí‘œ, í˜„ê¸ˆíë¦„í‘œ ì¡°ì¸
        4. ë©”íŠ¸ë¦­ í…Œì´ë¸”: ì£¼ìš” ë©”íŠ¸ë¦­, ì„±ì¥ ë°ì´í„°, DCF í‰ê°€ ê²°í•©
        5. ì¸ë±ìŠ¤ í…Œì´ë¸”: ì‹¬ë³¼ ì¸ë±ìŠ¤ ì •ë³´ ë³µì‚¬

        ì¬ë¬´ì œí‘œì™€ ë©”íŠ¸ë¦­ì˜ ê²½ìš°, íŠ¹ì • ê¸°ê°„ì˜ íš¨ìœ¨ì ì¸ ì¿¼ë¦¬ë¥¼ ìœ„í•´
        ì—°ë„ë³„ íŒŒí‹°ì…˜ íŒŒì¼ë„ ìƒì„±í•©ë‹ˆë‹¤.

        ì‚¬ìš© ì˜ˆì‹œ:
            ëª¨ë“  ë·° í…Œì´ë¸” ì¬êµ¬ì¶•::

                converter = Parquet(main_ctx)
                converter.rebuild_table_view()

                # VIEW/ì— ë‹¤ìŒ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:
                # - symbol_list.csv
                # - price.csv
                # - financial_statement.csv
                # - financial_statement_YYYY.csv (ì—°ë„ë³„)
                # - metrics.csv
                # - metrics_YYYY.csv (ì—°ë„ë³„)
                # - indexes.csv

        Note:
            - ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì˜ ê²½ìš° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—…ì…ë‹ˆë‹¤
            - rawpq_pathì— ëª¨ë“  ì†ŒìŠ¤ íŒŒì¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
            - ê´‘ë²”ìœ„í•œ ë°ì´í„° ì •ì œ ë° íƒ€ì… ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤
            - ì „ì²´ ë° ì—°ë„ë³„ íŒŒí‹°ì…˜ ë°ì´í„°ì…‹ì„ ëª¨ë‘ ìƒì„±í•©ë‹ˆë‹¤
            - ëŒ€ìš©ëŸ‰ DataFrame ì‘ì—…ìœ¼ë¡œ ì¸í•´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤

        Raises:
            ì†ŒìŠ¤ íŒŒì¼ì´ ëˆ„ë½ë˜ê±°ë‚˜ ì†ìƒëœ ê²½ìš° ì˜ˆì™¸ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            ì˜¤ë¥˜ëŠ” ë¡œê¹…ë˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì¡íˆì§€ ì•ŠìŠµë‹ˆë‹¤.
        """
        # 1. Build Symbol List Table
        # Consolidates stock lists, delisted companies, and profile information
        symbol_list = pd.read_csv(
            self.rawpq_path + "stock_list.csv",
            usecols=['symbol', 'exchangeShortName', 'type']
        )
        delisted = pd.read_csv(
            self.rawpq_path + "delisted_companies.csv",
            usecols=['symbol', 'exchange', 'ipoDate', 'delistedDate']
        )
        profile = pd.read_csv(
            self.rawpq_path + "profile.csv",
            usecols=['symbol', 'ipoDate', 'industry', 'exchangeShortName']
        )

        # Rename exchange column for consistency
        delisted.rename(columns={'exchange': 'exchangeShortName'}, inplace=True)

        # Concatenate symbol lists and remove duplicates
        all_symbol = pd.concat([symbol_list, delisted])
        all_symbol = all_symbol.drop_duplicates('symbol', keep='first')

        # Merge with profile data to get industry and IPO information
        all_symbol = all_symbol.merge(
            profile,
            how='left',
            on=['symbol', 'exchangeShortName']
        )

        # Consolidate IPO dates from multiple sources
        all_symbol['ipoDate'] = all_symbol['ipoDate_x'].combine_first(
            all_symbol['ipoDate_y']
        )
        all_symbol = all_symbol.drop(['ipoDate_x', 'ipoDate_y'], axis=1)
        all_symbol = all_symbol.drop_duplicates('symbol', keep='first')

        # Filter for NASDAQ and NYSE only
        all_symbol = all_symbol[
            (all_symbol['exchangeShortName'] == 'NASDAQ') |
            (all_symbol['exchangeShortName'] == 'NYSE')
        ]

        # Convert date columns to datetime
        all_symbol['ipoDate'] = all_symbol['ipoDate'].astype('datetime64[ns]')
        all_symbol['delistedDate'] = all_symbol['delistedDate'].astype('datetime64[ns]')

        all_symbol = all_symbol.reset_index(drop=True)

        # Save to Parquet (main format)
        all_symbol.to_parquet(self.view_path + "symbol_list.parquet", index=False)
        logging.info("âœ… Created symbol_list.parquet")

        # Optionally save CSV for debugging
        if self.main_ctx.save_debug_csv:
            all_symbol.to_csv(self.view_path + "symbol_list.csv", index=False)
            logging.info("ğŸ› Debug: Created symbol_list.csv")

        del all_symbol

        # 2. Build Price Table
        # Combines historical prices with market capitalization
        price = pd.read_csv(
            self.rawpq_path + "historical_price_full.csv",
            usecols=['date', 'symbol', 'close', 'volume']
        )
        marketcap = pd.read_csv(
            self.rawpq_path + "historical_market_capitalization.csv",
            usecols=['date', 'symbol', 'marketCap']
        )

        # Merge price and market cap data
        price_marketcap = pd.merge(price, marketcap, how='left', on=['symbol', 'date'])
        del price
        del marketcap

        # Validate date format (should be YYYY-MM-DD)
        invalid_dates = price_marketcap[
            ~price_marketcap['date'].str.match(r'^\d{4}-\d{2}-\d{2}$')
        ]

        # Convert date column to datetime
        price_marketcap['date'] = price_marketcap['date'].astype('datetime64[ns]')

        # Save to Parquet (main format)
        price_marketcap.to_parquet(self.view_path + "price.parquet", index=False)
        logging.info("âœ… Created price.parquet")

        # Optionally save CSV for debugging
        if self.main_ctx.save_debug_csv:
            price_marketcap.to_csv(self.view_path + "price.csv", index=False)
            logging.info("ğŸ› Debug: Created price.csv")

        del price_marketcap

        # 3. Build Financial Statement Table
        # Joins income statement, balance sheet, and cash flow statement
        income_statement = pd.read_csv(self.rawpq_path + "income_statement.csv")
        balance_sheet_statement = pd.read_csv(
            self.rawpq_path + "balance_sheet_statement.csv"
        )
        cash_flow_statement = pd.read_csv(
            self.rawpq_path + "cash_flow_statement.csv"
        )

        # Perform outer join to preserve all records from all statements
        financial_statement = income_statement.merge(
            balance_sheet_statement,
            how='outer',
            on=['date', 'symbol']
        ).merge(
            cash_flow_statement,
            how='outer',
            on=['date', 'symbol']
        )

        # Convert date columns with error handling
        financial_statement['date'] = pd.to_datetime(
            financial_statement['date'],
            errors='coerce'
        )
        financial_statement['fillingDate'] = financial_statement['fillingDate'].astype(
            'datetime64[ns]'
        )

        # Save full financial statement table (Parquet)
        financial_statement.to_parquet(
            self.view_path + "financial_statement.parquet",
            index=False
        )
        logging.info("âœ… Created financial_statement.parquet")

        # Optionally save CSV for debugging
        if self.main_ctx.save_debug_csv:
            financial_statement.to_csv(
                self.view_path + "financial_statement.csv",
                index=False
            )
            logging.info("ğŸ› Debug: Created financial_statement.csv")

        # Create year-partitioned files for efficient querying
        for year in range(self.main_ctx.start_year - 1, self.main_ctx.end_year + 1):
            fs_peryear = financial_statement[
                financial_statement['date'].between(
                    datetime.datetime(year, 1, 1),
                    datetime.datetime(year, 12, 31)
                )
            ]
            # Save as Parquet
            fs_peryear.to_parquet(
                self.view_path + f"financial_statement_{year}.parquet",
                index=False
            )

            # Optionally save CSV for debugging
            if self.main_ctx.save_debug_csv:
                fs_peryear.to_csv(
                    self.view_path + f"financial_statement_{year}.csv",
                    index=False
                )
        logging.info("âœ… Created financial_statement per year (Parquet" +
                    (" + CSV)" if self.main_ctx.save_debug_csv else ")"))

        del income_statement
        del balance_sheet_statement
        del cash_flow_statement
        del financial_statement

        # 4. Build Metrics Table
        # Combines key metrics, financial growth, and DCF valuations
        key_metrics = pd.read_csv(self.rawpq_path + "key_metrics.csv")
        financial_growth = pd.read_csv(self.rawpq_path + "financial_growth.csv")
        historical_daily_discounted_cash_flow = pd.read_csv(
            self.rawpq_path + "historical_daily_discounted_cash_flow.csv"
        )

        # Merge metrics data with outer join to preserve all records
        metrics = key_metrics.merge(
            financial_growth,
            how='outer',
            on=['date', 'symbol']
        ).merge(
            historical_daily_discounted_cash_flow,
            how='left',
            on=['date', 'symbol']
        )

        # Convert date column
        metrics['date'] = metrics['date'].astype('datetime64[ns]')

        # Save to Parquet (main format)
        metrics.to_parquet(self.view_path + "metrics.parquet", index=False)
        logging.info("âœ… Created metrics.parquet")

        # Optionally save CSV for debugging
        if self.main_ctx.save_debug_csv:
            metrics.to_csv(self.view_path + "metrics.csv", index=False)
            logging.info("ğŸ› Debug: Created metrics.csv")

        # Create year-partitioned metrics files
        for year in range(self.main_ctx.start_year - 1, self.main_ctx.end_year + 1):
            metrics_peryear = metrics[
                metrics['date'].between(
                    datetime.datetime(year, 1, 1),
                    datetime.datetime(year, 12, 31)
                )
            ]
            # Save as Parquet
            metrics_peryear.to_parquet(
                self.view_path + f"metrics_{year}.parquet",
                index=False
            )

            # Optionally save CSV for debugging
            if self.main_ctx.save_debug_csv:
                metrics_peryear.to_csv(
                    self.view_path + f"metrics_{year}.csv",
                    index=False
                )

        logging.info("âœ… Created metrics per year (Parquet" +
                    (" + CSV)" if self.main_ctx.save_debug_csv else ")"))

        del financial_growth
        del key_metrics
        del metrics

        # 5. Copy Indexes Table
        # Simple copy operation for index membership data
        indexes = pd.read_csv(self.rawpq_path + "symbol_available_indexes.csv")

        # Save to Parquet (main format)
        indexes.to_parquet(self.view_path + "indexes.parquet", index=False)
        logging.info("âœ… Created indexes.parquet")

        # Optionally save CSV for debugging
        if self.main_ctx.save_debug_csv:
            indexes.to_csv(self.view_path + "indexes.csv", index=False)
            logging.info("ğŸ› Debug: Created indexes.csv")

    def insert_csv(self) -> None:
        """ì—¬ëŸ¬ CSV/Parquet íŒŒì¼ì„ ì¹´í…Œê³ ë¦¬ë³„ ë‹¨ì¼ íŒŒì¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.

        ì‚¬ì „ ì •ì˜ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ìŠ¤ìº”í•˜ê³  ê° ì¹´í…Œê³ ë¦¬ ë‚´ì˜ ëª¨ë“  CSV ë˜ëŠ”
        Parquet íŒŒì¼ì„ ë‹¨ì¼ í†µí•© CSV íŒŒì¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤. ì´ê²ƒì€ ì¼ë°˜ì ìœ¼ë¡œ
        ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤.

        ë©”ì„œë“œê°€ ì²˜ë¦¬í•˜ëŠ” ë°ì´í„° ì¹´í…Œê³ ë¦¬:
            - balance_sheet_statement
            - cash_flow_statement
            - delisted_companies
            - earning_calendar
            - financial_growth
            - historical_daily_discounted_cash_flow
            - historical_market_capitalization
            - historical_price_full (ì»¬ëŸ¼ í•„í„°ë§ í¬í•¨)
            - income_statement
            - key_metrics
            - profile
            - stock_list (ë³´ì¡´ë¨, ì¬ìƒì„± ì•ˆ í•¨)
            - symbol_available_indexes (ë³´ì¡´ë¨, ì¬ìƒì„± ì•ˆ í•¨)

        ê° ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´:
            1. ì¹´í…Œê³ ë¦¬ ë””ë ‰í† ë¦¬ì—ì„œ .csv ë˜ëŠ” .parquet íŒŒì¼ ìŠ¤ìº”
            2. ëª¨ë“  íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ê¸°
            3. ë‹¨ì¼ DataFrameìœ¼ë¡œ ì—°ê²°
            4. í†µí•© ê²°ê³¼ë¥¼ ë‹¨ì¼ CSV íŒŒì¼ë¡œ ì €ì¥

        ì‚¬ìš© ì˜ˆì‹œ:
            ëª¨ë“  ì†ŒìŠ¤ íŒŒì¼ í†µí•©::

                converter = Parquet(main_ctx)
                converter.insert_csv()

                # ë‹¤ìŒì—ì„œ íŒŒì¼ì„ ì½ìŠµë‹ˆë‹¤:
                # {root_path}/income_statement/*.csv
                # {root_path}/balance_sheet_statement/*.parquet
                # ë“±.

                # ë‹¤ìŒì„ ìƒì„±í•©ë‹ˆë‹¤:
                # {root_path}/parquet/income_statement.csv
                # {root_path}/parquet/balance_sheet_statement.csv
                # ë“±.

        Note:
            - ê¸°ì¡´ í†µí•© íŒŒì¼ì€ ì¬ìƒì„± ì „ì— ì œê±°ë©ë‹ˆë‹¤
              (stock_listì™€ symbol_available_indexes ì œì™¸)
            - historical_price_fullì˜ ê²½ìš°, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´
              íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¡œë“œë©ë‹ˆë‹¤ (date, symbol, close, volume)
            - ì½ê¸° ì˜¤ë¥˜ê°€ ìˆëŠ” íŒŒì¼ì€ ê²½ê³ ì™€ í•¨ê»˜ ê±´ë„ˆëœë‹ˆë‹¤
            - tqdm ì§„í–‰ë¥  í‘œì‹œì¤„ë¡œ ì§„í–‰ ìƒí™©ì´ í‘œì‹œë©ë‹ˆë‹¤
            - ë¹ˆ ë””ë ‰í† ë¦¬ëŠ” ìš°ì•„í•˜ê²Œ ì²˜ë¦¬ë©ë‹ˆë‹¤

        TODO:
            - ì—°ë„ë³„ íŒŒí‹°ì…˜ ê³¼ê±° ê°€ê²© ë°ì´í„° ì§€ì› ì¶”ê°€
            - ì „ì²´ í†µí•© ëŒ€ì‹  ì¦ë¶„ ì—…ë°ì´íŠ¸ êµ¬í˜„
        """
        # Define data categories to process
        dir_list = [
            "balance_sheet_statement",
            "cash_flow_statement",
            "delisted_companies",
            "earning_calendar",
            "financial_growth",
            "historical_daily_discounted_cash_flow",
            "historical_market_capitalization",
            "historical_price_full",
            "income_statement",
            "key_metrics",
            "profile",
            "stock_list",
            "symbol_available_indexes"
        ]

        logging.info("directory list : {}".format(dir_list))

        # Process each category with progress bar
        for directory in tqdm(dir_list):
            csv_save_path = self.rawpq_path + directory + ".csv"

            # Remove existing consolidated file (except for preserved lists)
            if (directory != 'stock_list') and (directory != 'symbol_available_indexes'):
                if os.path.exists(csv_save_path):
                    os.remove(csv_save_path)

            # Find all CSV and Parquet files in category directory
            file_list = [
                self.main_ctx.root_path + "/" + directory + "/" + file
                for file in os.listdir(self.main_ctx.root_path + "/" + directory)
                if (file.endswith(".parquet") or file.endswith(".csv"))
            ]

            # Read and consolidate all files for this category
            df_list = []
            for filename in file_list:
                try:
                    # Special handling for historical_price_full to reduce memory usage
                    if directory == 'historical_price_full':
                        if filename.endswith('.csv'):
                            df = pd.read_csv(
                                filename,
                                usecols=['date', 'symbol', 'close', 'volume']
                            )
                        elif filename.endswith('.parquet'):
                            df = pd.read_parquet(
                                filename,
                                columns=['date', 'symbol', 'close', 'volume']
                            )
                    else:
                        # Load all columns for other categories
                        if filename.endswith('.csv'):
                            df = pd.read_csv(filename, low_memory=False)
                        elif filename.endswith('.parquet'):
                            df = pd.read_parquet(filename)

                    df_list.append(df)

                except Exception as e:
                    # Log warning and continue if file cannot be read
                    logging.warning(f"Error reading {filename}: {str(e)}")
                    continue

            # Concatenate all DataFrames and save to single CSV
            if df_list:
                # Concatenate all at once for efficiency
                df_all_years = pd.concat(df_list, ignore_index=True)
                df_all_years.to_csv(csv_save_path, index=False)
                logging.info("create df in tables dict : {}".format(directory))
            else:
                logging.warning(f"No data found for directory: {directory}")
