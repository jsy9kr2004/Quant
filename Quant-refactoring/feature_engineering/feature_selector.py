"""
Feature Selection

í”¼ì²˜ ì„ íƒ ë° ê³¼ì í•© ë°©ì§€
"""

import numpy as np
import pandas as pd
import logging
from sklearn.feature_selection import (
    SelectKBest, f_classif, f_regression,
    mutual_info_classif, mutual_info_regression,
    RFE, SelectFromModel
)
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from typing import List, Optional, Union


class FeatureSelector:
    """
    í”¼ì²˜ ì„ íƒ ë° ê³¼ì í•© ë°©ì§€

    ë°©ë²•:
    1. Univariate Selection (í†µê³„ì  í…ŒìŠ¤íŠ¸)
    2. Recursive Feature Elimination (RFE)
    3. Feature Importance (Tree-based)
    4. Correlation ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    """

    def __init__(self,
                 method: str = 'mutual_info',
                 top_k: int = 50,
                 correlation_threshold: float = 0.95,
                 task: str = 'classification'):
        """
        Args:
            method: í”¼ì²˜ ì„ íƒ ë°©ë²•
                   - 'mutual_info': Mutual Information
                   - 'f_test': F-test (ANOVA)
                   - 'rfe': Recursive Feature Elimination
                   - 'tree_importance': Tree-based Feature Importance
            top_k: ì„ íƒí•  í”¼ì²˜ ê°œìˆ˜
            correlation_threshold: ìƒê´€ê´€ê³„ ì„ê³„ê°’ (ì´ ê°’ ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼)
            task: 'classification' or 'regression'
        """
        self.method = method
        self.top_k = top_k
        self.correlation_threshold = correlation_threshold
        self.task = task
        self.selected_features = None
        self.feature_scores = None

    def select_features(self,
                       X: Union[np.ndarray, pd.DataFrame],
                       y: Union[np.ndarray, pd.Series],
                       feature_names: Optional[List[str]] = None) -> List[str]:
        """
        í”¼ì²˜ ì„ íƒ ìˆ˜í–‰

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            feature_names: í”¼ì²˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ íƒëœ í”¼ì²˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        # DataFrameì„ arrayë¡œ ë³€í™˜
        if isinstance(X, pd.DataFrame):
            if feature_names is None:
                feature_names = X.columns.tolist()
            X = X.values

        if isinstance(y, pd.Series):
            y = y.values

        if feature_names is None:
            feature_names = [f"feature_{i}" for i in range(X.shape[1])]

        logging.info(f"{'='*60}")
        logging.info(f"í”¼ì²˜ ì„ íƒ ì‹œì‘")
        logging.info(f"{'='*60}")
        logging.info(f"ì›ë³¸ í”¼ì²˜ ìˆ˜: {X.shape[1]}")
        logging.info(f"ë°©ë²•: {self.method}")
        logging.info(f"ëª©í‘œ í”¼ì²˜ ìˆ˜: {self.top_k}")

        # 1ë‹¨ê³„: ìƒê´€ê´€ê³„ ë†’ì€ í”¼ì²˜ ì œê±°
        X_reduced, remaining_features = self._remove_correlated_features(
            pd.DataFrame(X, columns=feature_names)
        )

        logging.info(f"ìƒê´€ê´€ê³„ ì œê±° í›„: {len(remaining_features)}")

        # 2ë‹¨ê³„: ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒ
        if self.method == 'mutual_info':
            selected = self._select_by_mutual_info(X_reduced, y, remaining_features)
        elif self.method == 'f_test':
            selected = self._select_by_f_test(X_reduced, y, remaining_features)
        elif self.method == 'rfe':
            selected = self._select_by_rfe(X_reduced, y, remaining_features)
        elif self.method == 'tree_importance':
            selected = self._select_by_tree_importance(X_reduced, y, remaining_features)
        else:
            logging.warning(f"Unknown method '{self.method}', using first {self.top_k} features")
            selected = remaining_features[:self.top_k]

        self.selected_features = selected

        logging.info(f"âœ… ìµœì¢… ì„ íƒëœ í”¼ì²˜ ìˆ˜: {len(selected)}")
        logging.info(f"{'='*60}\n")

        return selected

    def _remove_correlated_features(self, df: pd.DataFrame) -> tuple:
        """
        ìƒê´€ê´€ê³„ ë†’ì€ í”¼ì²˜ ì œê±°

        Args:
            df: í”¼ì²˜ ë°ì´í„°í”„ë ˆì„

        Returns:
            (ì¶•ì†Œëœ ë°ì´í„°, ë‚¨ì€ í”¼ì²˜ ì´ë¦„)
        """
        # ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°
        corr_matrix = df.corr().abs()

        # ìƒì‚¼ê° í–‰ë ¬ (ì¤‘ë³µ ì œê±°)
        upper = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )

        # ìƒê´€ê´€ê³„ ë†’ì€ ì»¬ëŸ¼ ì°¾ê¸°
        to_drop = [
            column for column in upper.columns
            if any(upper[column] > self.correlation_threshold)
        ]

        df_reduced = df.drop(columns=to_drop)

        logging.info(f"ìƒê´€ê´€ê³„ë¡œ ì œê±°ëœ í”¼ì²˜: {len(to_drop)}")
        if to_drop and len(to_drop) < 20:
            logging.debug(f"ì œê±°ëœ í”¼ì²˜: {to_drop}")

        return df_reduced.values, df_reduced.columns.tolist()

    def _select_by_mutual_info(self,
                               X: np.ndarray,
                               y: np.ndarray,
                               feature_names: List[str]) -> List[str]:
        """
        Mutual Information ê¸°ë°˜ ì„ íƒ

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            feature_names: í”¼ì²˜ ì´ë¦„

        Returns:
            ì„ íƒëœ í”¼ì²˜ ì´ë¦„
        """
        k = min(self.top_k, X.shape[1])

        if self.task == 'classification':
            selector = SelectKBest(mutual_info_classif, k=k)
        else:
            selector = SelectKBest(mutual_info_regression, k=k)

        selector.fit(X, y)

        # ì ìˆ˜ ì €ì¥
        self.feature_scores = pd.DataFrame({
            'feature': feature_names,
            'score': selector.scores_
        }).sort_values('score', ascending=False)

        selected_idx = selector.get_support(indices=True)
        selected = [feature_names[i] for i in selected_idx]

        logging.info(f"Mutual Information ìƒìœ„ 10ê°œ í”¼ì²˜:")
        for i, row in self.feature_scores.head(10).iterrows():
            logging.info(f"   {row['feature']}: {row['score']:.4f}")

        return selected

    def _select_by_f_test(self,
                         X: np.ndarray,
                         y: np.ndarray,
                         feature_names: List[str]) -> List[str]:
        """
        F-test ê¸°ë°˜ ì„ íƒ

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            feature_names: í”¼ì²˜ ì´ë¦„

        Returns:
            ì„ íƒëœ í”¼ì²˜ ì´ë¦„
        """
        k = min(self.top_k, X.shape[1])

        if self.task == 'classification':
            selector = SelectKBest(f_classif, k=k)
        else:
            selector = SelectKBest(f_regression, k=k)

        selector.fit(X, y)

        # ì ìˆ˜ ì €ì¥
        self.feature_scores = pd.DataFrame({
            'feature': feature_names,
            'score': selector.scores_
        }).sort_values('score', ascending=False)

        selected_idx = selector.get_support(indices=True)
        selected = [feature_names[i] for i in selected_idx]

        logging.info(f"F-test ìƒìœ„ 10ê°œ í”¼ì²˜:")
        for i, row in self.feature_scores.head(10).iterrows():
            logging.info(f"   {row['feature']}: {row['score']:.4f}")

        return selected

    def _select_by_rfe(self,
                      X: np.ndarray,
                      y: np.ndarray,
                      feature_names: List[str]) -> List[str]:
        """
        Recursive Feature Elimination

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            feature_names: í”¼ì²˜ ì´ë¦„

        Returns:
            ì„ íƒëœ í”¼ì²˜ ì´ë¦„
        """
        k = min(self.top_k, X.shape[1])

        if self.task == 'classification':
            estimator = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        else:
            estimator = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)

        selector = RFE(estimator, n_features_to_select=k, step=max(1, X.shape[1] // 20))
        selector.fit(X, y)

        # ìˆœìœ„ ì €ì¥
        self.feature_scores = pd.DataFrame({
            'feature': feature_names,
            'rank': selector.ranking_
        }).sort_values('rank')

        selected_idx = selector.get_support(indices=True)
        selected = [feature_names[i] for i in selected_idx]

        logging.info(f"RFE ìƒìœ„ 10ê°œ í”¼ì²˜:")
        for i, row in self.feature_scores.head(10).iterrows():
            logging.info(f"   {row['feature']}: rank {row['rank']}")

        return selected

    def _select_by_tree_importance(self,
                                   X: np.ndarray,
                                   y: np.ndarray,
                                   feature_names: List[str]) -> List[str]:
        """
        Tree-based ì¤‘ìš”ë„

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            feature_names: í”¼ì²˜ ì´ë¦„

        Returns:
            ì„ íƒëœ í”¼ì²˜ ì´ë¦„
        """
        if self.task == 'classification':
            rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        else:
            rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)

        rf.fit(X, y)

        importances = rf.feature_importances_

        # ì¤‘ìš”ë„ ì €ì¥
        self.feature_scores = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)

        # ìƒìœ„ kê°œ ì„ íƒ
        k = min(self.top_k, X.shape[1])
        selected = self.feature_scores.head(k)['feature'].tolist()

        logging.info(f"Tree Importance ìƒìœ„ 10ê°œ í”¼ì²˜:")
        for i, row in self.feature_scores.head(10).iterrows():
            logging.info(f"   {row['feature']}: {row['importance']:.4f}")

        return selected

    def transform(self,
                 X: Union[np.ndarray, pd.DataFrame],
                 feature_names: Optional[List[str]] = None) -> Union[np.ndarray, pd.DataFrame]:
        """
        ì„ íƒëœ í”¼ì²˜ë§Œ ì¶”ì¶œ

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            feature_names: í”¼ì²˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ íƒëœ í”¼ì²˜ë§Œ í¬í•¨ëœ ë°ì´í„°
        """
        if self.selected_features is None:
            raise ValueError("ë¨¼ì € select_features()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”")

        if isinstance(X, pd.DataFrame):
            return X[self.selected_features]
        else:
            if feature_names is None:
                # ì¸ë±ìŠ¤ë¡œ ì„ íƒ
                return X[:, :len(self.selected_features)]
            else:
                # ì´ë¦„ìœ¼ë¡œ ì„ íƒ
                df = pd.DataFrame(X, columns=feature_names)
                return df[self.selected_features].values

    def fit_transform(self,
                     X: Union[np.ndarray, pd.DataFrame],
                     y: Union[np.ndarray, pd.Series],
                     feature_names: Optional[List[str]] = None) -> Union[np.ndarray, pd.DataFrame]:
        """
        í”¼ì²˜ ì„ íƒ + ë³€í™˜ì„ í•œë²ˆì— ìˆ˜í–‰

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            feature_names: í”¼ì²˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ íƒëœ í”¼ì²˜ë§Œ í¬í•¨ëœ ë°ì´í„°
        """
        self.select_features(X, y, feature_names)
        return self.transform(X, feature_names)

    def get_feature_scores(self) -> pd.DataFrame:
        """í”¼ì²˜ ì ìˆ˜/ìˆœìœ„ ë°˜í™˜"""
        if self.feature_scores is None:
            raise ValueError("ë¨¼ì € select_features()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”")
        return self.feature_scores

    def save_selected_features(self, path: str):
        """ì„ íƒëœ í”¼ì²˜ ì €ì¥"""
        if self.selected_features is None:
            raise ValueError("ë¨¼ì € select_features()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”")

        df = pd.DataFrame({'feature': self.selected_features})
        df.to_csv(path, index=False)
        logging.info(f"ğŸ’¾ ì„ íƒëœ í”¼ì²˜ ì €ì¥: {path}")

    def load_selected_features(self, path: str):
        """ì„ íƒëœ í”¼ì²˜ ë¡œë“œ"""
        df = pd.read_csv(path)
        self.selected_features = df['feature'].tolist()
        logging.info(f"ğŸ“‚ ì„ íƒëœ í”¼ì²˜ ë¡œë“œ: {path} ({len(self.selected_features)}ê°œ)")
