# ğŸ“Š Robust Validation Guide

ë°±í…ŒìŠ¤íŒ… ë° ëª¨ë¸ ê²€ì¦ ê°œì„  ê°€ì´ë“œ

## ğŸ¯ ì£¼ìš” ë¬¸ì œì ê³¼ í•´ê²° ë°©ì•ˆ

### 1ï¸âƒ£ Cross-validation ì—†ìŒ (ë‹¨ìˆœ train/test split)

#### ë¬¸ì œì 
- ë‹¨ìˆœíˆ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ ë°±í…ŒìŠ¤íŒ…ë§Œ ìˆ˜í–‰
- íŠ¹ì • ê¸°ê°„ì˜ ë°ì´í„°ì—ë§Œ ê³¼ì í•©ë  ìœ„í—˜
- ëª¨ë¸ì˜ ì•ˆì •ì„±ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ì—†ìŒ

#### í•´ê²° ë°©ì•ˆ
**Time Series Cross-Validation êµ¬í˜„**

```python
from validation.time_series_cv import TimeSeriesCV
from models.xgboost_model import XGBoostModel

# 1. TimeSeriesCV ì´ˆê¸°í™”
cv = TimeSeriesCV(n_splits=5)

# 2. ëª¨ë¸ ìƒì„±
model = XGBoostModel(n_estimators=100, max_depth=5)
model.build_model({})

# 3. êµì°¨ ê²€ì¦ ìˆ˜í–‰
avg_scores, all_scores = cv.cross_validate_model(
    model=model,
    X=X_train,
    y=y_train,
    dates=dates,
    verbose=True
)

print(f"í‰ê·  Accuracy: {avg_scores['accuracy_mean']:.4f}")
print(f"í‘œì¤€í¸ì°¨: {avg_scores['accuracy_std']:.4f}")
```

**BaseModelì— í†µí•©ëœ ë©”ì„œë“œ ì‚¬ìš©**

```python
from models.xgboost_model import XGBoostModel

model = XGBoostModel(n_estimators=100)
model.build_model({})

# êµì°¨ê²€ì¦ + ì „ì²´ ë°ì´í„° í•™ìŠµ
avg_scores, all_scores = model.fit_with_cv(
    X=X_train,
    y=y_train,
    dates=dates,
    cv_splits=5
)
```

---

### 2ï¸âƒ£ Walk-forward validation ì—†ìŒ

#### ë¬¸ì œì 
- í•œë²ˆ í•™ìŠµí•œ ëª¨ë¸ì„ ê³„ì† ì‚¬ìš© (ëª¨ë¸ ì¬í•™ìŠµ ì—†ìŒ)
- ì‹œì¥ í™˜ê²½ ë³€í™”ì— ì ì‘í•˜ì§€ ëª»í•¨
- ì‹¤ì œ íŠ¸ë ˆì´ë”© í™˜ê²½ê³¼ ë‹¤ë¦„

#### í•´ê²° ë°©ì•ˆ
**Walk-Forward Validation êµ¬í˜„**

```python
from validation.walk_forward import WalkForwardValidator
from models.xgboost_model import XGBoostModel
from datetime import datetime

# 1. Walk-Forward Validator ì´ˆê¸°í™”
wfv = WalkForwardValidator(
    train_months=24,        # 24ê°œì›” ë°ì´í„°ë¡œ í•™ìŠµ
    test_months=3,          # 3ê°œì›” í…ŒìŠ¤íŠ¸
    retrain_frequency=3,    # 3ê°œì›”ë§ˆë‹¤ ì¬í•™ìŠµ
    anchored=False          # Rolling window (False) or Anchored window (True)
)

# 2. ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
results = wfv.validate(
    model_class=XGBoostModel,
    df=data_df,
    date_col='date',
    feature_cols=['feature_1', 'feature_2', ...],
    target_col='target',
    start_date=datetime(2020, 1, 1),
    end_date=datetime(2023, 12, 31),
    model_params={'n_estimators': 100, 'max_depth': 5}
)

print(results)
```

**ì°¨ì´ì  ì„¤ëª…**

```python
# Rolling Window (anchored=False)
# Period 1: Train(2020-01~2021-12) â†’ Test(2022-01~2022-03)
# Period 2: Train(2020-04~2022-03) â†’ Test(2022-04~2022-06)
# Period 3: Train(2020-07~2022-06) â†’ Test(2022-07~2022-09)

# Anchored Window (anchored=True)
# Period 1: Train(2020-01~2021-12) â†’ Test(2022-01~2022-03)
# Period 2: Train(2020-01~2022-03) â†’ Test(2022-04~2022-06)  â† í•™ìŠµ ì‹œì‘ì  ê³ ì •
# Period 3: Train(2020-01~2022-06) â†’ Test(2022-07~2022-09)
```

---

### 3ï¸âƒ£ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ëª¨ë‹ˆí„°ë§ ì—†ìŒ

#### ë¬¸ì œì 
- ëª¨ë¸ì´ ì–¸ì œ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§€ëŠ”ì§€ ê°ì§€í•˜ëŠ” ë¡œì§ì´ ì—†ìŒ
- ëª¨ë¸ drift íƒì§€ ë¶ˆê°€
- ì¬í•™ìŠµ ì‹œì ì„ ê²°ì •í•  ìˆ˜ ì—†ìŒ

#### í•´ê²° ë°©ì•ˆ
**Performance Monitor êµ¬í˜„**

```python
from monitoring.performance_monitor import PerformanceMonitor

# 1. Performance Monitor ì´ˆê¸°í™”
monitor = PerformanceMonitor(
    window_size=10,          # ìµœê·¼ 10ê°œ ê¸°ê°„ ì¶”ì 
    alert_threshold=0.10,    # 10% ì„±ëŠ¥ ì €í•˜ ì‹œ ì•Œë¦¼
    drift_threshold=0.05     # p-value < 0.05ì´ë©´ drift
)

# 2. ë°±í…ŒìŠ¤íŒ… ë£¨í”„ì—ì„œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
for period in periods:
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    metrics = model.evaluate(X_test, y_test)

    # ì„±ëŠ¥ ì—…ë°ì´íŠ¸
    monitor.update_performance(metrics, period_label=period)

    # í”¼ì²˜ ë“œë¦¬í”„íŠ¸ ì²´í¬
    drift_features = monitor.check_feature_drift(
        X_new=X_test.values,
        feature_names=feature_cols
    )

    if drift_features:
        print(f"âš ï¸ {len(drift_features)}ê°œ í”¼ì²˜ì—ì„œ ë“œë¦¬í”„íŠ¸ ê°ì§€")

    # ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€
    if monitor.should_retrain():
        print("ğŸ”„ ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•œ ì¬í•™ìŠµ í•„ìš”!")
        # ëª¨ë¸ ì¬í•™ìŠµ ë¡œì§

# 3. ì„±ëŠ¥ ìš”ì•½
monitor.print_summary()
```

**ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥**
- âœ… Rolling window ì„±ëŠ¥ ì¶”ì 
- âœ… Baseline ëŒ€ë¹„ ì„±ëŠ¥ ì €í•˜ ìë™ ê°ì§€
- âœ… ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€ (Kolmogorov-Smirnov test)
- âœ… ì¬í•™ìŠµ ê¶Œì¥ ì•Œë¦¼

---

### 4ï¸âƒ£ tsfreshë¡œ ìˆ˜ë°± ê°œ í”¼ì²˜ ìƒì„± â†’ ê³¼ì í•© ê°€ëŠ¥ì„±

#### ë¬¸ì œì 
- `EfficientFCParameters()`ëŠ” 794ê°œì˜ í”¼ì²˜ë¥¼ ìƒì„±
- ìƒ˜í”Œ ìˆ˜ < í”¼ì²˜ ìˆ˜ì¸ ê²½ìš° ê³¼ì í•© ì‹¬ê°
- ë¶ˆí•„ìš”í•œ í”¼ì²˜ë“¤ì´ ë…¸ì´ì¦ˆë¡œ ì‘ìš©

#### í•´ê²° ë°©ì•ˆ

**Option 1: Feature Selection ì‚¬ìš©**

```python
from feature_engineering.feature_selector import FeatureSelector

# 1. Feature Selector ì´ˆê¸°í™”
selector = FeatureSelector(
    method='mutual_info',           # 'mutual_info', 'f_test', 'rfe', 'tree_importance'
    top_k=50,                       # ìƒìœ„ 50ê°œ í”¼ì²˜ ì„ íƒ
    correlation_threshold=0.95,     # ìƒê´€ê´€ê³„ 0.95 ì´ìƒì´ë©´ ì œê±°
    task='classification'           # or 'regression'
)

# 2. í”¼ì²˜ ì„ íƒ
selected_features = selector.select_features(
    X=X_train,
    y=y_train,
    feature_names=feature_names
)

print(f"ì„ íƒëœ í”¼ì²˜: {selected_features}")

# 3. ë°ì´í„° ë³€í™˜
X_train_selected = selector.transform(X_train, feature_names)
X_test_selected = selector.transform(X_test, feature_names)

# 4. í”¼ì²˜ ì ìˆ˜ í™•ì¸
feature_scores = selector.get_feature_scores()
print(feature_scores.head(10))
```

**Option 2: tsfresh MinimalFCParameters ì‚¬ìš©**

```python
from tsfresh.feature_extraction import MinimalFCParameters

# 794ê°œ â†’ ì•½ 20ê°œë¡œ ì¶•ì†Œ
settings = MinimalFCParameters()
extracted_features = extract_features(
    long_format_df,
    default_fc_parameters=settings
)
```

**Option 3: ì‚¬ìš©ì ì •ì˜ í”¼ì²˜**

```python
from tsfresh.feature_extraction import extract_features

# í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
fc_parameters = {
    "mean": None,
    "median": None,
    "std": None,
    "min": None,
    "max": None,
    "quantile": [{"q": 0.25}, {"q": 0.75}],
    "linear_trend": [{"attr": "slope"}],
    "autocorrelation": [{"lag": 1}, {"lag": 2}]
}

extracted_features = extract_features(
    long_format_df,
    default_fc_parameters=fc_parameters
)
```

---

## ğŸš€ í†µí•© ì‚¬ìš©: RobustBacktester

ëª¨ë“  ê¸°ëŠ¥ì„ í†µí•©í•œ `RobustBacktester` ì‚¬ìš© ì˜ˆì œ:

```python
from robust_backtester import RobustBacktester
from models.xgboost_model import XGBoostModel
from datetime import datetime

# 1. RobustBacktester ì´ˆê¸°í™”
backtester = RobustBacktester(
    train_months=24,                    # 24ê°œì›” í•™ìŠµ
    test_months=3,                      # 3ê°œì›” í…ŒìŠ¤íŠ¸
    retrain_frequency=3,                # 3ê°œì›”ë§ˆë‹¤ ì¬í•™ìŠµ
    cv_splits=5,                        # 5-fold CV
    top_k_features=50,                  # ìƒìœ„ 50ê°œ í”¼ì²˜
    feature_selection_method='mutual_info',
    performance_window=10,              # ìµœê·¼ 10ê°œ ê¸°ê°„ ì¶”ì 
    alert_threshold=0.10                # 10% ì„±ëŠ¥ ì €í•˜ ì‹œ ì•Œë¦¼
)

# 2. ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
results = backtester.run_backtest(
    model_class=XGBoostModel,
    df=data_df,
    date_col='date',
    feature_cols=feature_columns,
    target_col='target',
    start_date=datetime(2020, 1, 1),
    end_date=datetime(2023, 12, 31),
    model_params={'n_estimators': 100, 'max_depth': 5},
    use_feature_selection=True,         # í”¼ì²˜ ì„ íƒ ì‚¬ìš©
    use_cv=True,                        # êµì°¨ê²€ì¦ ì‚¬ìš©
    use_monitoring=True,                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‚¬ìš©
    save_results=True,
    output_dir='./results'
)

# 3. ê²°ê³¼ í™•ì¸
print(results)
print(f"\nì„ íƒëœ í”¼ì²˜: {backtester.get_selected_features()}")
print(f"\nì„±ëŠ¥ ìš”ì•½: {backtester.get_performance_summary()}")
```

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Quant-refactoring/
â”œâ”€â”€ validation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ time_series_cv.py          # Time Series Cross-Validation
â”‚   â””â”€â”€ walk_forward.py            # Walk-Forward Validation
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ performance_monitor.py     # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
â”œâ”€â”€ feature_engineering/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ feature_selector.py        # í”¼ì²˜ ì„ íƒ
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base_model.py              # BaseModel (CV ë©”ì„œë“œ ì¶”ê°€)
â”‚   â”œâ”€â”€ xgboost_model.py
â”‚   â”œâ”€â”€ lightgbm_model.py
â”‚   â””â”€â”€ catboost_model.py
â”œâ”€â”€ robust_backtester.py           # í†µí•© ë°±í…ŒìŠ¤í„°
â””â”€â”€ examples/
    â”œâ”€â”€ robust_backtest_example.py # í†µí•© ì˜ˆì œ
    â””â”€â”€ validation_examples.py     # ê°œë³„ ê¸°ëŠ¥ ì˜ˆì œ
```

---

## ğŸ”§ ì‹¤í–‰ ì˜ˆì œ

### ì˜ˆì œ 1: í†µí•© ë°±í…ŒìŠ¤íŒ…

```bash
cd Quant-refactoring
python examples/robust_backtest_example.py
```

### ì˜ˆì œ 2: ê°œë³„ ê²€ì¦ ë°©ë²•

```bash
python examples/validation_examples.py
```

---

## ğŸ“Š ì¶œë ¥ ê²°ê³¼

ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰ ì‹œ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

```
results/
â”œâ”€â”€ backtest_results_20231215_143022.csv      # ë°±í…ŒìŠ¤íŒ… ê²°ê³¼
â”œâ”€â”€ selected_features_20231215_143022.csv     # ì„ íƒëœ í”¼ì²˜
â””â”€â”€ performance_alerts_20231215_143022.csv    # ì„±ëŠ¥ ì•Œë¦¼
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë°ì´í„° ëˆ„ì¶œ ë°©ì§€**: ëª¨ë“  ê²€ì¦ì€ ì‹œê³„ì—´ ìˆœì„œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
2. **ì¶©ë¶„í•œ ë°ì´í„°**: Walk-forwardì—ëŠ” ìµœì†Œ 3ë…„ ì´ìƒì˜ ë°ì´í„° ê¶Œì¥
3. **ê³„ì‚° ì‹œê°„**: CV + Walk-forwardëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. **ë©”ëª¨ë¦¬**: í° ë°ì´í„°ì…‹ì˜ ê²½ìš° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ì˜

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

- [Advances in Financial Machine Learning](https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482086) - Marcos LÃ³pez de Prado
- [Sklearn Time Series Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)
- [Walk-Forward Optimization](https://www.investopedia.com/terms/w/walkforward.asp)

---

## ğŸ¤ ê¸°ì—¬

ë²„ê·¸ ë¦¬í¬íŠ¸ ë° ê°œì„  ì œì•ˆì€ ì´ìŠˆë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”.
