"""Financial Modeling Prep (FMP) ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆì…ë‹ˆë‹¤.

ì´ ëª¨ë“ˆì€ Financial Modeling Prep APIì—ì„œ ê¸ˆìœµ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ë©”ì¸ FMP ë°ì´í„°
ìˆ˜ì§‘ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬, ë°ì´í„° ìˆ˜ì§‘, íŒŒì¼ ì •ë¦¬, ë‹¤ìš´ë¡œë“œëœ
ë°ì´í„° ê²€ì¦ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

FMP í´ë˜ìŠ¤ëŠ” ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¡°ìœ¨í•©ë‹ˆë‹¤:
1. ì£¼ì‹ ë° ìƒì¥íì§€ íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
2. NASDAQ ë° NYSE ê±°ë˜ì†Œì˜ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ êµ¬ì¶•
3. ëª¨ë“  ì‹¬ë³¼ì— ëŒ€í•œ ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘
4. ì˜¤ë˜ëœ íŒŒì¼ ê²€ì¦ ë° ì •ë¦¬

ì‚¬ìš© ì˜ˆì‹œ:
    from context import MainContext

    main_ctx = MainContext()
    fmp = FMP(main_ctx)
    fmp.collect()  # ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
"""

from data_collector.fmp_api import FMPAPI
from data_collector.fmp_fetch_worker import fetch_fmp

import datetime
import dateutil.utils
import logging
import os
import pandas as pd
from dateutil.relativedelta import relativedelta
from typing import List, Optional


class FMP:
    """Financial Modeling Prep APIì—ì„œ ê¸ˆìœµ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

    ì´ í´ë˜ìŠ¤ëŠ” í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°, NASDAQ ë° NYSE ê±°ë˜ì†Œì˜ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ êµ¬ì¶•,
    ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘, ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ë¥¼ í¬í•¨í•œ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš°ë¥¼
    ê´€ë¦¬í•©ë‹ˆë‹¤.

    Attributes:
        main_ctx: ì„¤ì •ê³¼ ê³µìœ  ë¦¬ì†ŒìŠ¤ë¥¼ í¬í•¨í•˜ëŠ” ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ê°ì²´.
        symbol_list (list): ìƒì¥íì§€ íšŒì‚¬ë¥¼ í¬í•¨í•œ ì „ì²´ ì£¼ì‹ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸.
        current_list (list): í˜„ì¬ í™œì„± ì£¼ì‹ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸.
        logger: ì´ í´ë˜ìŠ¤ì˜ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤.

    ì‚¬ìš© ì˜ˆì‹œ:
        from context import MainContext
        main_ctx = MainContext()
        fmp = FMP(main_ctx)
        fmp.collect()
    """

    def __init__(self, main_ctx) -> None:
        """FMP ë°ì´í„° ìˆ˜ì§‘ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            main_ctx: API í‚¤, ë£¨íŠ¸ ê²½ë¡œ, ë¡œê¹… ì„¤ì • ë“±ì˜ ì„¤ì •ì„ í¬í•¨í•˜ëŠ”
                ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ê°ì²´.
        """
        self.main_ctx = main_ctx
        self.symbol_list: List[str] = pd.DataFrame()
        self.current_list: List[str] = pd.DataFrame()

        self.logger = self.main_ctx.get_logger('fmp')

    def __get_api_list(self) -> List[FMPAPI]:
        """íƒ€ê²Ÿ API ë¦¬ìŠ¤íŠ¸ CSVì—ì„œ FMPAPI ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ì„¤ì •ëœ CSV íŒŒì¼ì—ì„œ API ë¦¬ìŠ¤íŠ¸ë¥¼ ì½ê³  ê° URL í•­ëª©ì— ëŒ€í•´ FMPAPI ê°ì²´ë¥¼
        ìƒì„±í•©ë‹ˆë‹¤. ë¹ˆ í–‰ì€ ì œê±°ë©ë‹ˆë‹¤.

        Returns:
            List[FMPAPI]: ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•´ ì´ˆê¸°í™”ëœ FMPAPI ê°ì²´ ë¦¬ìŠ¤íŠ¸.

        Raises:
            FileNotFoundError: íƒ€ê²Ÿ API ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°.
            KeyError: CSVì— 'URL' ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°.
        """
        url_df = pd.read_csv(self.main_ctx.target_api_list, header=0, usecols=["URL"])
        url_df = url_df.dropna()
        url_list = url_df['URL'].tolist()
        api_list = [FMPAPI(self.main_ctx, url) for url in url_list]

        return api_list

    def __fetch_ticker_list(self, api_list: List[FMPAPI]) -> None:
        """FMP APIì—ì„œ ì£¼ì‹ ë¦¬ìŠ¤íŠ¸ ë° ìƒì¥íì§€ íšŒì‚¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

        API ë¦¬ìŠ¤íŠ¸ë¥¼ í•„í„°ë§í•˜ì—¬ stock_list ë° delisted_companies ì—”ë“œí¬ì¸íŠ¸ë¥¼
        ì°¾ê³ , ë‘˜ ë‹¤ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì¦í•œ í›„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

        Args:
            api_list (List[FMPAPI]): í•„í„°ë§í•  FMPAPI ê°ì²´ì˜ ì „ì²´ ë¦¬ìŠ¤íŠ¸.

        Raises:
            Exception: ë¦¬ìŠ¤íŠ¸ì—ì„œ stock_list ë˜ëŠ” delisted_companies APIë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°.
        """
        self.logger.info('fetching ticker list start (stock_list, delisted_companies)')

        # stock_list API ì°¾ê¸° ë° ê²€ì¦
        stock_list_api = [api for api in api_list if api.converted_category == 'stock_list']
        if len(stock_list_api) == 0:
            self.logger.error('stock listëŠ” ë°›ì•„ì˜¨ë‹¤ëŠ” ì „ì œ')
            raise Exception('stock listëŠ” ë°›ì•„ì˜¨ë‹¤ëŠ” ì „ì œ')
        stock_list_api = stock_list_api[0]

        # delisted_companies API ì°¾ê¸° ë° ê²€ì¦
        delisted_companies_api = [api for api in api_list if api.converted_category == 'delisted_companies']
        if len(delisted_companies_api) == 0:
            self.logger.error('delisted companiesëŠ” ë°›ì•„ì˜¨ë‹¤ëŠ” ì „ì œ')
            raise Exception('delisted companiesëŠ” ë°›ì•„ì˜¨ë‹¤ëŠ” ì „ì œ')
        delisted_companies_api = delisted_companies_api[0]

        self.logger.info('fetching ticker list done')

        return fetch_fmp(self.main_ctx, [stock_list_api, delisted_companies_api])

    def __set_symbol(self) -> None:
        """stock_list ë° delisted_companies ë°ì´í„°ì—ì„œ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

        ì´ ë©”ì„œë“œëŠ”:
        1. stock_list CSV íŒŒì¼ ë¡œë“œ
        2. NASDAQ ë° NYSE ê±°ë˜ì†Œì˜ ì£¼ì‹ë§Œ í•„í„°ë§
        3. ë™ì¼í•œ ê±°ë˜ì†Œì˜ ìƒì¥íì§€ íšŒì‚¬ì™€ ë³‘í•©
        4. ë‘ ê°œì˜ ë¦¬ìŠ¤íŠ¸ ìƒì„±:
           - symbol_list: ëª¨ë“  ì‹¬ë³¼ (ìƒì¥íì§€ í¬í•¨)
           - current_list: ìµœê·¼ í™œì„± ì‹¬ë³¼ (1ê°œì›” ë‚´ ìƒì¥íì§€ ë˜ëŠ” í˜„ì¬ í™œì„±)

        ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ëŠ” ë””ë²„ê¹… ë° ê²€ì¦ ëª©ì ìœ¼ë¡œ CSV íŒŒì¼(allsymbol.csv ë° current_list.csv)ì—
        ì €ì¥ë©ë‹ˆë‹¤.

        Raises:
            FileNotFoundError: stock_list.csvê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°.
        """
        self.logger.info('set symbol list start')
        path = self.main_ctx.root_path + "/stock_list/stock_list.csv"
        if os.path.isfile(path):
            symbol_list = pd.read_csv(path)
        else:
            self.logger.error(f'file({path}) is not existed')
            return

        # NASDAQ ë° NYSE ê±°ë˜ì†Œì˜ ì£¼ì‹ë§Œ í•„í„°ë§
        # ì°¸ê³ : read_csvëŠ” ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸ ì—´ì„ ì‚­ì œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        filtered_symbol = symbol_list[(symbol_list['type'] == "stock")
                                      & ((symbol_list['exchangeShortName'] == 'NASDAQ')
                                         | (symbol_list['exchangeShortName'] == 'NYSE'))]
        filtered_symbol = filtered_symbol.dropna(subset=['symbol'])
        filtered_symbol = filtered_symbol.reset_index(drop=True)
        filtered_symbol = filtered_symbol.drop(['price', 'exchange', 'name'], axis=1)
        all_symbol = filtered_symbol

        # NASDAQ ë° NYSEì˜ ìƒì¥íì§€ íšŒì‚¬ì™€ ë³‘í•©
        file_list = os.listdir(self.main_ctx.root_path + "/delisted_companies/")
        for file in file_list:
            if os.path.splitext(file)[1] == ".csv":
                delisted = pd.read_csv(self.main_ctx.root_path + "/delisted_companies/" + file)
                if delisted.empty == True:
                    continue
                # NASDAQ ë° NYSE ê±°ë˜ì†Œë§Œ í•„í„°ë§
                delisted = delisted[((delisted['exchange'] == 'NASDAQ') | (delisted['exchange'] == 'NYSE'))]
                delisted = delisted.dropna(subset=['symbol'])
                delisted = delisted.reset_index(drop=True)
                delisted.rename(columns={'exchange':'exchangeShortName'}, inplace=True)
                delisted = delisted.drop(['companyName'], axis=1)
                all_symbol = pd.concat([all_symbol, delisted])

        # ëª¨ë“  ì‹¬ë³¼ì„ CSVì— ì €ì¥í•˜ê³  ì™„ì „í•œ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        all_symbol.to_csv('./allsymbol.csv', index=False)
        all_symbol = all_symbol.drop_duplicates('symbol', keep='first')
        all_symbol = all_symbol.reset_index(drop=True)
        self.symbol_list = all_symbol["symbol"].to_list()

        # í˜„ì¬ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ìµœê·¼ í™œì„± ì£¼ì‹)
        # TODO: ê°€ì¥ ìµœê·¼ ìƒì¥íì§€ ë‚ ì§œì—ì„œ 1ê°œì›”ì„ ë¹¼ëŠ” ëª©ì ì„ ëª…í™•íˆ í•  ê²ƒ
        all_symbol["delistedDate"] = pd.to_datetime(all_symbol["delistedDate"])
        recent_date = all_symbol["delistedDate"].max()
        recent_date -= relativedelta(months=1)  # ê°€ì¥ ìµœê·¼ ìƒì¥íì§€ ë‚ ì§œì—ì„œ 1ê°œì›” ë¹¼ê¸°

        # ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì— í•´ë‹¹í•˜ëŠ” ì‹¬ë³¼ ì¿¼ë¦¬:
        # 1. ìµœê·¼ì— ìƒì¥íì§€ë¨ (1ê°œì›” ì´ë‚´)
        # 2. ìƒì¥íì§€ë˜ì§€ ì•ŠìŒ (NaT ë˜ëŠ” None)
        query = '(delistedDate >= "{}") or (delistedDate == "NaT") or (delistedDate == "None")'.format(recent_date)
        current_symbol = all_symbol.query(query)
        current_symbol.to_csv('./current_list.csv', index=False)
        current_symbol = current_symbol.drop_duplicates('symbol', keep='first')
        current_symbol = current_symbol.reset_index(drop=True)
        self.current_list = current_symbol["symbol"].to_list()

        self.logger.info("in set_symbol() lit = " + str(self.symbol_list))
        self.logger.info('set symbol list done')

    def __fetch_data(self, api_list: List[FMPAPI]) -> None:
        """stock_list ë° delisted_companiesë¥¼ ì œì™¸í•œ ëª¨ë“  APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

        í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ APIë¥¼ í•„í„°ë§í•˜ê³  ë‚˜ë¨¸ì§€ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        ì‹¬ë³¼ì´ í•„ìš”í•œ APIì—ëŠ” ê°€ì ¸ì˜¤ê¸° ì „ì— symbol_listê°€ í• ë‹¹ë©ë‹ˆë‹¤.

        Args:
            api_list (List[FMPAPI]): FMPAPI ê°ì²´ì˜ ì „ì²´ ë¦¬ìŠ¤íŠ¸.
        """
        self.logger.info('fetching the rest start')

        # í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ APIë§Œ í•„í„°ë§
        rest_api_list = [api for api in api_list if api.converted_category not in ['stock_list', 'delisted_companies']]

        # ì‹¬ë³¼ì´ í•„ìš”í•œ APIì— ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ í• ë‹¹
        for api in rest_api_list:
            if api.need_symbol:
                api.symbol_list = self.symbol_list

        self.logger.info('fetching the rest done')

        return fetch_fmp(self.main_ctx, rest_api_list)

    @staticmethod
    def remove_files(path: str, only_csv: bool = True) -> None:
        """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ì„ ì œê±°í•©ë‹ˆë‹¤.

        Args:
            path (str): ì •ë¦¬í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ.
            only_csv (bool, optional): Trueì¸ ê²½ìš° CSV íŒŒì¼ë§Œ ì œê±°.
                Falseì¸ ê²½ìš° ëª¨ë“  íŒŒì¼ ì œê±°. ê¸°ë³¸ê°’ì€ True.
        """
        if os.path.isdir(path) is False:
            return
        for file in os.listdir(path):
            if only_csv is True and not (file.endswith(".csv") or file.endswith(".csvx")):
                continue
            os.remove(os.path.join(path, file))

    def remove_current_list_files(self, base_path: str, check_target: bool = True) -> None:
        """í˜„ì¬ ë¦¬ìŠ¤íŠ¸ì˜ ì‹¬ë³¼ì— ëŒ€í•œ ì˜¤ë˜ëœ íŒŒì¼ì„ ì œê±°í•©ë‹ˆë‹¤.

        current_list ì‹¬ë³¼ì„ ìˆœíšŒí•˜ë©° ë‹¤ìŒ ê²½ìš° ë°ì´í„° íŒŒì¼ì„ ì œê±°í•©ë‹ˆë‹¤:
        1. check_targetì´ False: ë¬´ì¡°ê±´ ëª¨ë“  íŒŒì¼ ì œê±°
        2. check_targetì´ True: 75ì¼ë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼ë§Œ ì œê±°

        Args:
            base_path (str): ì‹¬ë³¼ ë°ì´í„° íŒŒì¼ì´ í¬í•¨ëœ ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
            check_target (bool, optional): Trueì¸ ê²½ìš° ì œê±° ì „ì— íŒŒì¼ ë‚ ì§œ í™•ì¸.
                ê¸°ë³¸ê°’ì€ True.

        Note:
            íŒŒì¼ì˜ ë‚ ì§œë¥¼ í™•ì¸í•˜ë ¤ë©´ 'date' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. date ì»¬ëŸ¼ì´ ì—†ê±°ë‚˜
            ë¹ˆ date ê°’ì„ ê°€ì§„ íŒŒì¼ì€ ì¦‰ì‹œ ì œê±°ë©ë‹ˆë‹¤.
        """
        logging.info("[Check Remove Files] Path : " + str(base_path))
        if os.path.isdir(base_path) is False:
            return
        today = dateutil.utils.today()

        for symbol in self.current_list:
            path = base_path + "/" + str(symbol) + ".csv"
            if os.path.isfile(path):
                if check_target is True:
                    # ë‚ ì§œë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì „ì²´ íŒŒì¼ ì½ê¸°
                    # TODO: ì²« ë²ˆì§¸/ë§ˆì§€ë§‰ í–‰ë§Œ ì½ëŠ” ë” íš¨ìœ¨ì ì¸ ë°©ë²• ì°¾ê¸°
                    row = pd.read_csv(path)

                    # íŒŒì¼ì— date ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                    if "date" in row.columns:
                        if row["date"].empty is True:
                            os.remove(path)
                            continue
                    else:
                        # date ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ íŒŒì¼ ì œê±°
                        os.remove(path)
                        continue

                    # íŒŒì¼ì´ 75ì¼ë³´ë‹¤ ì˜¤ë˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    update_date = datetime.datetime.strptime(row["date"].max(), "%Y-%m-%d")
                    if (today - update_date) < datetime.timedelta(days=75):
                        continue

                os.remove(path)

    @staticmethod
    def remove_current_year(base_path: str) -> None:
        """ì§€ì •ëœ ê¸°ë³¸ ê²½ë¡œì—ì„œ í˜„ì¬ ì—°ë„ì˜ íŒŒì¼ì„ ì œê±°í•©ë‹ˆë‹¤.

        í˜„ì¬ ì—°ë„ ì ‘ë¯¸ì‚¬ë¥¼ ê°€ì§„ .csv ë° .csvx íŒŒì¼ì„ ëª¨ë‘ ì œê±°í•©ë‹ˆë‹¤.

        Args:
            base_path (str): ê¸°ë³¸ ê²½ë¡œ íŒ¨í„´ (ì˜ˆ: 'path/to/data_').
                í˜„ì¬ ì—°ë„ê°€ ì¶”ê°€ë˜ì–´ 'path/to/data_2025.csv' í˜•íƒœê°€ ë©ë‹ˆë‹¤.

        ì‚¬ìš© ì˜ˆì‹œ:
            FMP.remove_current_year('/data/historical_price_full/AAPL_')
            # ì œê±°: /data/historical_price_full/AAPL_2025.csv
            #       /data/historical_price_full/AAPL_2025.csvx
        """
        today = dateutil.utils.today()
        year = today.strftime("%Y")
        if os.path.isfile(base_path + str(year) + ".csv"):
            os.remove(base_path + str(year) + ".csv")
        if os.path.isfile(base_path + str(year) + ".csvx"):
            os.remove(base_path + str(year) + ".csvx")

    def skip_remove_check(self) -> bool:
        """ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ ì œê±°ë¥¼ ê±´ë„ˆë›¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.

        './config/update_date.txt'ì—ì„œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œë¥¼ ì½ê³ 
        ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì´í›„ 1ì¼ ë¯¸ë§Œì´ ê²½ê³¼í–ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

        Returns:
            bool: ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì´í›„ 1ì¼ ë¯¸ë§Œì¸ ê²½ìš° True (ì œê±° ê±´ë„ˆë›°ê¸°),
                ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False.

        ì‚¬ìš© ì˜ˆì‹œ:
            if not fmp.skip_remove_check():
                fmp.remove_first_loop()
        """
        today = datetime.datetime.today()
        if os.path.isfile("./config/update_date.txt"):
            fd = open("./config/update_date.txt", "r")
            update_date = fd.readline()
            fd.close()
            update_date = datetime.datetime.strptime(update_date, "%Y-%m-%d")

            # 1ì¼ ë¯¸ë§Œ ê²½ê³¼í–ˆëŠ”ì§€ í™•ì¸
            if (today - update_date) < datetime.timedelta(days=1):
                self.logger.info('Skip Remove Files')
                return True
        return False

    def validation_check(self) -> bool:
        """ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì— API ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.

        ë°ì´í„° ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  CSV íŒŒì¼ì—ì„œ FMP API ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤:
        1. "Limit Reach" - API ìš”ì²­ ì œí•œ ì´ˆê³¼
        2. "Error Message" - APIì˜ ì¼ë°˜ ì˜¤ë¥˜

        ì´ëŸ¬í•œ ë©”ì‹œì§€ê°€ í¬í•¨ëœ íŒŒì¼ì€ ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ í¬í•¨í•˜ë¯€ë¡œ _quarantine í´ë”ë¡œ ì´ë™ë©ë‹ˆë‹¤.
        ë˜í•œ mixed type warningsë¥¼ ê°ì§€í•˜ì—¬ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.

        Returns:
            bool: ê²©ë¦¬ëœ íŒŒì¼ì´ ì—†ìœ¼ë©´ True (ëª¨ë‘ ìœ íš¨), ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False.
        """
        import shutil
        import warnings

        basepath = self.main_ctx.root_path
        quarantine_base = os.path.join(basepath, '_quarantine')
        os.makedirs(quarantine_base, exist_ok=True)

        flag = True
        quarantine_count = 0
        pass_count = 0
        mixed_type_count = 0
        retry_list = []

        logging.info("ğŸ” Starting validation check with quarantine system")
        logging.info(f"   Base path: {basepath}")
        logging.info(f"   Quarantine path: {quarantine_base}")

        for dir_name in os.listdir(basepath):
            dir_path = os.path.join(basepath, dir_name)

            # Skip quarantine directory and non-directories
            if dir_name == '_quarantine' or not os.path.isdir(dir_path):
                continue

            cur_path = dir_path
            par_list = [file for file in os.listdir(cur_path) if file.endswith('csv')]

            category_quarantine_count = 0
            category_mixed_type_count = 0

            for p in par_list:
                file_path = os.path.join(cur_path, p)
                has_error = False
                has_mixed_type = False

                try:
                    # Catch DtypeWarning for mixed types
                    with warnings.catch_warnings(record=True) as w:
                        warnings.simplefilter("always", category=pd.errors.DtypeWarning)
                        df = pd.read_csv(file_path, low_memory=True)

                        # Check for mixed type warnings
                        if len(w) > 0:
                            has_mixed_type = True
                            mixed_type_count += 1
                            category_mixed_type_count += 1

                            # Log detailed mixed type info
                            for warning_item in w:
                                logging.warning(f"âš ï¸  Mixed type detected: {file_path}")
                                logging.warning(f"    Warning: {warning_item.message}")

                                # Sample first few rows for debugging
                                if len(df) > 0:
                                    logging.debug(f"    First row sample: {df.iloc[0].to_dict()}")

                    # ë°ì´í„°ì—ì„œ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸
                    if df.filter(regex='Limit').empty is False or df.filter(regex='Error').empty is False:
                        has_error = True

                        # Extract symbol from filename (usually format: SYMBOL.csv)
                        symbol = os.path.splitext(p)[0]

                        # Create quarantine subdirectory
                        category_quarantine_path = os.path.join(quarantine_base, dir_name)
                        os.makedirs(category_quarantine_path, exist_ok=True)

                        # Move file to quarantine
                        quarantine_file_path = os.path.join(category_quarantine_path, p)
                        shutil.move(file_path, quarantine_file_path)

                        quarantine_count += 1
                        category_quarantine_count += 1
                        flag = False

                        # Determine error type
                        error_type = []
                        if not df.filter(regex='Limit').empty:
                            error_type.append('Limit Reach')
                        if not df.filter(regex='Error').empty:
                            error_type.append('Error Message')
                        if has_mixed_type:
                            error_type.append('Mixed Type')

                        # Add to retry list
                        retry_list.append({
                            'category': dir_name,
                            'symbol': symbol,
                            'original_path': file_path,
                            'quarantine_path': quarantine_file_path,
                            'error_type': ', '.join(error_type)
                        })

                        logging.info(f"ğŸ”’ Quarantined: {dir_name}/{p} -> {error_type}")
                    else:
                        pass_count += 1

                        # Log mixed type files that passed API error check
                        if has_mixed_type:
                            logging.info(f"âš ï¸  Mixed type (no API error): {dir_name}/{p}")

                except Exception as e:
                    logging.error(f"âŒ Error processing {file_path}: {e}")
                    continue

            logging.info("[ {} ] Quarantined: {} | Mixed types: {} | Valid: {} | Total: {}".format(
                cur_path,
                category_quarantine_count,
                category_mixed_type_count,
                pass_count,
                category_quarantine_count + pass_count
            ))
            quarantine_count = 0
            pass_count = 0

        # Save retry list to CSV
        if retry_list:
            retry_csv_path = os.path.join(basepath, '_retry_list.csv')
            retry_df = pd.DataFrame(retry_list)
            retry_df.to_csv(retry_csv_path, index=False)
            logging.info(f"ğŸ“ Created retry list: {retry_csv_path} ({len(retry_list)} files)")
            logging.info(f"   Use this list to re-fetch problematic files")
        else:
            logging.info("âœ… No files needed quarantine - all files valid!")

        logging.info(f"ğŸ Validation complete: Mixed type warnings: {mixed_type_count}")

        return flag

    def remove_first_loop(self) -> None:
        """ìƒˆë¡œê³ ì¹¨ì„ ì¤€ë¹„í•˜ê¸° ìœ„í•´ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ ê´€ë ¨ íŒŒì¼ì„ ì œê±°í•©ë‹ˆë‹¤.

        2ë‹¨ê³„ ì œê±° í”„ë¡œì„¸ìŠ¤ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤. ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ì— ì˜í–¥ì„ ì£¼ëŠ” íŒŒì¼ì„
        ì œê±°í•˜ì—¬ í˜„ì¬ ë°ì´í„°ë¡œ ë‹¤ì‹œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

        ì œê±° ëŒ€ìƒ:
        - allsymbol.csv: ê²°í•©ëœ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ ìºì‹œ
        - current_list.csv: í˜„ì¬ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ ìºì‹œ
        - delisted_companies/ ë””ë ‰í† ë¦¬ ë‚´ìš©
        - stock_list/ ë””ë ‰í† ë¦¬ ë‚´ìš©

        See Also:
            remove_second_loop: ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ê°€ ì¬êµ¬ì¶•ëœ í›„ì˜ ë‘ ë²ˆì§¸ ë‹¨ê³„.
        """
        if os.path.isfile("./allsymbol.csv"):
            os.remove("./allsymbol.csv")
        if os.path.isfile("./current_list.csv"):
            os.remove("./current_list.csv")
        self.remove_files(self.main_ctx.root_path+"/delisted_companies")
        self.remove_files(self.main_ctx.root_path+"/stock_list")

    def remove_second_loop(self) -> None:
        """ìƒˆë¡œê³ ì¹¨ëœ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° íŒŒì¼ì„ ì œê±°í•©ë‹ˆë‹¤.

        2ë‹¨ê³„ ì œê±° í”„ë¡œì„¸ìŠ¤ì˜ ë‘ ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤. ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ê°€ ìƒˆë¡œê³ ì¹¨ëœ í›„,
        í˜„ì¬ ë¦¬ìŠ¤íŠ¸ì˜ ì‹¬ë³¼ì— ëŒ€í•œ ì˜¤ë˜ëœ ë°ì´í„° íŒŒì¼ì„ ì œê±°í•©ë‹ˆë‹¤.

        ì œê±° ëŒ€ìƒ:
        - symbol_available_indexes
        - í˜„ì¬ ì—°ë„ ìˆ˜ìµ ìº˜ë¦°ë” íŒŒì¼
        - 75ì¼ë³´ë‹¤ ì˜¤ë˜ëœ ì¬ë¬´ì œí‘œ íŒŒì¼
        - í˜„ì¬ ì—°ë„ íˆìŠ¤í† ë¦¬ì»¬ ê°€ê²© íŒŒì¼
        - DCF ë° ì‹œê°€ì´ì•¡ íŒŒì¼ (ì„±ëŠ¥ ë³‘ëª© - FIXME ì°¸ì¡°)
        - í”„ë¡œí•„ ë°ì´í„° (í˜„ì¬ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ)

        FIXME: DCF ë° ì‹œê°€ì´ì•¡ì— ëŒ€í•œ remove_current_list_filesê°€ ëŠë¦½ë‹ˆë‹¤.
            ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ëŒ€í•œ ëŒ€ì²´ ì ‘ê·¼ ë°©ì‹ì„ ê³ ë ¤í•˜ì‹­ì‹œì˜¤.

        See Also:
            remove_first_loop: ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ ìƒˆë¡œê³ ì¹¨ ì „ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„.
        """
        self.remove_files(self.main_ctx.root_path+"/symbol_available_indexes")
        self.remove_current_year(self.main_ctx.root_path+"/earning_calendar/earning_calendar_")

        # 75ì¼ë³´ë‹¤ ì˜¤ë˜ëœ ì¬ë¬´ì œí‘œ íŒŒì¼ ì œê±°
        self.remove_current_list_files(self.main_ctx.root_path+"/income_statement")
        self.remove_current_list_files(self.main_ctx.root_path+"/balance_sheet_statement")
        self.remove_current_list_files(self.main_ctx.root_path+"/cash_flow_statement")
        self.remove_current_list_files(self.main_ctx.root_path+"/key_metrics")
        self.remove_current_list_files(self.main_ctx.root_path+"/financial_growth")

        # ê° ì‹¬ë³¼ì— ëŒ€í•œ í˜„ì¬ ì—°ë„ íˆìŠ¤í† ë¦¬ì»¬ ê°€ê²© íŒŒì¼ ì œê±°
        for symbol in self.current_list:
            self.remove_current_year(self.main_ctx.root_path+"/historical_price_full/" + str(symbol) + "_")

        # FIXME: ì´ ë‘ ì‘ì—…ì€ ê°€ì¥ ëŠë¦° ì‘ì—… ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì‹­ì‹œì˜¤.
        # ê°€ì¥ ì‹œê°„ ì†Œëª¨ì ì¸ ì‘ì—… ìƒìœ„ 2ê°œ - ë” ë‚˜ì€ ì ‘ê·¼ ë°©ì‹ í•„ìš”
        self.remove_current_list_files(self.main_ctx.root_path+"/historical_daily_discounted_cash_flow")
        self.remove_current_list_files(self.main_ctx.root_path+"/historical_market_capitalization", False)

        # í”„ë¡œí•„ ë°ì´í„°ëŠ” ì˜ë„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        self.remove_current_list_files(self.main_ctx.root_path+"/profile", False)

    def collect(self) -> None:
        """ì™„ì „í•œ FMP ë°ì´í„° ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

        ì´ê²ƒì€ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ë©”ì¸ ì§„ì…ì ì…ë‹ˆë‹¤. ë‹¤ìŒ ìˆœì„œë¡œ ì „ì²´
        í”„ë¡œì„¸ìŠ¤ë¥¼ ì¡°ìœ¨í•©ë‹ˆë‹¤:

        1. ì„¤ì •ì—ì„œ API ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        2. í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (stock_list ë° delisted_companies)
        3. NASDAQ ë° NYSEë¥¼ ìœ„í•œ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ êµ¬ì¶•
        4. ë‹¤ë¥¸ ëª¨ë“  APIì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        5. ì—…ë°ì´íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡
        6. ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦

        ì´ ë©”ì„œë“œëŠ” 2íšŒ ì²˜ë¦¬ ì ‘ê·¼ ë°©ì‹ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
        - ì²« ë²ˆì§¸ íšŒ: ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•´ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        - ë‘ ë²ˆì§¸ íšŒ: ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

        ì´ ì„¤ê³„ëŠ” API ì¹´í…Œê³ ë¦¬ë¥¼ í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê³  ìœ ì—°í•œ API ê´€ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
        ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ ì‹¬ë³¼ì´ í•„ìš”í•œ APIëŠ” ì˜¤ë¥˜ ì—†ì´ ë°˜í™˜ë©ë‹ˆë‹¤.

        Note:
            íŒŒì¼ ì œê±° ë‹¨ê³„(remove_first_loop, remove_second_loop)ëŠ” í˜„ì¬ ì£¼ì„ ì²˜ë¦¬ë˜ì–´
            ìˆìŠµë‹ˆë‹¤. ì˜¤ë˜ëœ íŒŒì¼ì˜ ìë™ ì •ë¦¬ë¥¼ í™œì„±í™”í•˜ë ¤ë©´ ì£¼ì„ì„ í•´ì œí•˜ì‹­ì‹œì˜¤.

        Raises:
            SystemExit: ê²€ì¦ í™•ì¸ì´ ì‹¤íŒ¨í•˜ë©´ ë‹¤ì‹œ ê°€ì ¸ì™€ì•¼ í•˜ëŠ” ì†ìƒëœ ë‹¤ìš´ë¡œë“œë¥¼
                ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        """

        api_list = self.__get_api_list()

        # ì²« ë²ˆì§¸ ë£¨í”„: ìƒˆ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê¸° ì „ì— ì´ì „ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì œê±°
        if self.skip_remove_check() is False:
            self.remove_first_loop()

        # í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê³  ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ êµ¬ì¶•
        self.__fetch_ticker_list(api_list)
        self.__set_symbol()  # TODO: ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ì— ETF ì‹¬ë³¼ ì¶”ê°€
        print("after set_symbol : {}".format(self.symbol_list))

        # ë‘ ë²ˆì§¸ ë£¨í”„: ìƒˆ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜¤ë˜ëœ ë°ì´í„° íŒŒì¼ ì œê±°
        if self.skip_remove_check() is False:
            self.remove_second_loop()

        # ë‚˜ë¨¸ì§€ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        self.__fetch_data(api_list)

        # ì‹¤ìˆ˜ë¡œ ì¬ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì—…ë°ì´íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡
        write_fd = open("./config/update_date.txt", "w")
        today = datetime.date.today()
        write_fd.write(str(today))
        write_fd.close()

        # API ì˜¤ë¥˜ì— ëŒ€í•œ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦
        if self.validation_check() is False:
            logging.critical("Validation Check False!! Please run the program again after a few minutes!!")
            exit()
