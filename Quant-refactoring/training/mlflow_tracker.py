"""
MLflow experiment tracking for model management
"""

import logging
import mlflow
import mlflow.sklearn
import mlflow.xgboost
import mlflow.lightgbm
import mlflow.catboost
from pathlib import Path
from typing import Dict, Any, Optional
import pandas as pd


class MLflowTracker:
    """
    MLflowë¥¼ ì‚¬ìš©í•œ ì‹¤í—˜ ì¶”ì 

    Features:
    - Automatic experiment tracking
    - Parameter and metric logging
    - Model versioning
    - Artifact management
    - Easy comparison between runs
    """

    def __init__(self,
                 experiment_name: str = "quant_trading",
                 tracking_uri: Optional[str] = None):
        """
        Initialize MLflow tracker

        Args:
            experiment_name: ì‹¤í—˜ ì´ë¦„
            tracking_uri: MLflow ì„œë²„ URI (Noneì´ë©´ ë¡œì»¬)
        """
        self.experiment_name = experiment_name

        # MLflow ì„¤ì •
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)
        else:
            # ë¡œì»¬ ë””ë ‰í† ë¦¬ì— ì €ì¥
            mlflow.set_tracking_uri("file:./mlruns")

        # ì‹¤í—˜ ì„¤ì • (ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ê³ , ì—†ìœ¼ë©´ ìƒì„±)
        try:
            self.experiment_id = mlflow.create_experiment(experiment_name)
        except:
            experiment = mlflow.get_experiment_by_name(experiment_name)
            self.experiment_id = experiment.experiment_id

        mlflow.set_experiment(experiment_name)

        logging.info(f"ğŸ“Š MLflow tracking initialized")
        logging.info(f"   Experiment: {experiment_name}")
        logging.info(f"   Tracking URI: {mlflow.get_tracking_uri()}")

    def start_run(self, run_name: Optional[str] = None, tags: Optional[Dict] = None):
        """
        ìƒˆë¡œìš´ ì‹¤í—˜ Run ì‹œì‘

        Args:
            run_name: Run ì´ë¦„
            tags: íƒœê·¸ ë”•ì…”ë„ˆë¦¬
        """
        self.active_run = mlflow.start_run(run_name=run_name)

        if tags:
            for key, value in tags.items():
                mlflow.set_tag(key, value)

        logging.info(f"ğŸš€ MLflow run started: {run_name or 'unnamed'}")

        return self.active_run

    def log_params(self, params: Dict[str, Any]):
        """íŒŒë¼ë¯¸í„° ë¡œê¹…"""
        for key, value in params.items():
            mlflow.log_param(key, value)

    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None):
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        for key, value in metrics.items():
            mlflow.log_metric(key, value, step=step)

    def log_model(self, model, model_type: str, artifact_path: str = "model"):
        """
        ëª¨ë¸ ë¡œê¹…

        Args:
            model: ëª¨ë¸ ê°ì²´
            model_type: 'xgboost', 'lightgbm', 'catboost', 'sklearn'
            artifact_path: ì•„í‹°íŒ©íŠ¸ ê²½ë¡œ
        """
        if model_type == 'xgboost':
            mlflow.xgboost.log_model(model, artifact_path)
        elif model_type == 'lightgbm':
            mlflow.lightgbm.log_model(model, artifact_path)
        elif model_type == 'catboost':
            mlflow.catboost.log_model(model, artifact_path)
        else:
            mlflow.sklearn.log_model(model, artifact_path)

        logging.info(f"ğŸ’¾ Model logged to MLflow")

    def log_artifact(self, file_path: str):
        """íŒŒì¼ ì•„í‹°íŒ©íŠ¸ ë¡œê¹…"""
        mlflow.log_artifact(file_path)
        logging.info(f"ğŸ“ Artifact logged: {file_path}")

    def log_dataframe(self, df: pd.DataFrame, name: str):
        """DataFrame ë¡œê¹… (CSVë¡œ ì €ì¥)"""
        temp_file = f"/tmp/{name}.csv"
        df.to_csv(temp_file, index=False)
        mlflow.log_artifact(temp_file)
        logging.info(f"ğŸ“Š DataFrame logged: {name}")

    def end_run(self):
        """Run ì¢…ë£Œ"""
        mlflow.end_run()
        logging.info("âœ… MLflow run ended")

    def log_training_run(self,
                        model_name: str,
                        model,
                        model_type: str,
                        params: Dict[str, Any],
                        train_metrics: Dict[str, float],
                        test_metrics: Optional[Dict[str, float]] = None,
                        feature_importance: Optional[pd.DataFrame] = None,
                        tags: Optional[Dict] = None):
        """
        í•™ìŠµ Run í†µí•© ë¡œê¹… (í¸ì˜ í•¨ìˆ˜)

        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            model: ëª¨ë¸ ê°ì²´
            model_type: ëª¨ë¸ íƒ€ì…
            params: íŒŒë¼ë¯¸í„°
            train_metrics: í•™ìŠµ ë©”íŠ¸ë¦­
            test_metrics: í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­
            feature_importance: íŠ¹ì§• ì¤‘ìš”ë„ DataFrame
            tags: íƒœê·¸
        """
        with mlflow.start_run(run_name=model_name) as run:
            # íƒœê·¸
            if tags:
                for key, value in tags.items():
                    mlflow.set_tag(key, value)

            mlflow.set_tag("model_type", model_type)

            # íŒŒë¼ë¯¸í„°
            self.log_params(params)

            # í•™ìŠµ ë©”íŠ¸ë¦­
            for key, value in train_metrics.items():
                mlflow.log_metric(f"train_{key}", value)

            # í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­
            if test_metrics:
                for key, value in test_metrics.items():
                    mlflow.log_metric(f"test_{key}", value)

            # ëª¨ë¸
            self.log_model(model, model_type)

            # íŠ¹ì§• ì¤‘ìš”ë„
            if feature_importance is not None:
                self.log_dataframe(feature_importance, "feature_importance")

            logging.info(f"âœ… Training run logged: {model_name}")

            return run.info.run_id

    def load_best_model(self, metric: str = "test_accuracy", model_type: str = "sklearn"):
        """
        ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ

        Args:
            metric: ê¸°ì¤€ ë©”íŠ¸ë¦­
            model_type: ëª¨ë¸ íƒ€ì…

        Returns:
            ë¡œë“œëœ ëª¨ë¸
        """
        # ì‹¤í—˜ì˜ ëª¨ë“  runs ê°€ì ¸ì˜¤ê¸°
        runs = mlflow.search_runs(
            experiment_ids=[self.experiment_id],
            order_by=[f"metrics.{metric} DESC"],
            max_results=1
        )

        if runs.empty:
            raise ValueError("No runs found in experiment")

        best_run_id = runs.iloc[0]['run_id']

        logging.info(f"ğŸ“‚ Loading best model (run_id: {best_run_id})")
        logging.info(f"   Best {metric}: {runs.iloc[0][f'metrics.{metric}']:.4f}")

        # ëª¨ë¸ ë¡œë“œ
        model_uri = f"runs:/{best_run_id}/model"

        if model_type == 'xgboost':
            model = mlflow.xgboost.load_model(model_uri)
        elif model_type == 'lightgbm':
            model = mlflow.lightgbm.load_model(model_uri)
        elif model_type == 'catboost':
            model = mlflow.catboost.load_model(model_uri)
        else:
            model = mlflow.sklearn.load_model(model_uri)

        return model

    def compare_runs(self, metric: str = "test_accuracy", top_n: int = 10) -> pd.DataFrame:
        """
        Run ë¹„êµ

        Args:
            metric: ë¹„êµ ë©”íŠ¸ë¦­
            top_n: ìƒìœ„ Nê°œ

        Returns:
            ë¹„êµ DataFrame
        """
        runs = mlflow.search_runs(
            experiment_ids=[self.experiment_id],
            order_by=[f"metrics.{metric} DESC"],
            max_results=top_n
        )

        if runs.empty:
            logging.warning("No runs found")
            return pd.DataFrame()

        # ê´€ë ¨ ì»¬ëŸ¼ë§Œ ì„ íƒ
        cols = ['run_id', 'start_time', 'tags.model_type'] + \
               [col for col in runs.columns if col.startswith('metrics.') or col.startswith('params.')]

        comparison_df = runs[cols]

        logging.info(f"ğŸ“Š Top {len(comparison_df)} runs by {metric}:")
        for idx, row in comparison_df.head(5).iterrows():
            logging.info(f"   {idx+1}. {row.get('tags.model_type', 'unknown')}: {row[f'metrics.{metric}']:.4f}")

        return comparison_df

    def delete_experiment(self):
        """ì‹¤í—˜ ì‚­ì œ (ì£¼ì˜!)"""
        mlflow.delete_experiment(self.experiment_id)
        logging.warning(f"ğŸ—‘ï¸  Experiment deleted: {self.experiment_name}")
