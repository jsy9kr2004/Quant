"""
í€€íŠ¸ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œìš© ML í•™ìŠµ ë°ì´í„° ìƒì„±ê¸°

ì´ ëª¨ë“ˆì€ ì›ì‹œ ê¸ˆìœµ ë°ì´í„°ë¡œë¶€í„° ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
ì™„ì „í•œ ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

1. VIEW íŒŒì¼ì—ì„œ ê¸ˆìœµ ë°ì´í„° ë¡œë“œ (ì¢…ëª© ë¦¬ìŠ¤íŠ¸, ê°€ê²©, ì¬ë¬´ì œí‘œ, ë©”íŠ¸ë¦­)
2. ì§€ì •ëœ ê°„ê²©ìœ¼ë¡œ ë¦¬ë°¸ëŸ°ì‹± ë‚ ì§œ ìƒì„± (ì˜ˆ: ë¶„ê¸°ë³„)
3. ê°€ê²© ë³€ë™ ê³„ì‚° (ML ëª¨ë¸ì˜ íƒ€ê²Ÿ ë³€ìˆ˜)
4. ê° ì¢…ëª©ë³„ë¡œ 12ê°œì›” ë£©ë°± ìœˆë„ìš° ìƒì„±
5. tsfreshë¥¼ ì‚¬ìš©í•œ ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œ (ìê¸°ìƒê´€, FFT, AR ê³„ìˆ˜ ë“±)
6. ì»¤ìŠ¤í…€ ì¬ë¬´ ë¹„ìœ¨ ê³„ì‚° (OverMC_*, adaptiveMC_*)
7. RobustScalerë¥¼ ì‚¬ìš©í•œ íŠ¹ì„± ì •ê·œí™” (ì•„ì›ƒë¼ì´ì–´ ì €í•­ì„±)
8. ë¶„ê¸°ë³„ ML ë°ì´í„°ì…‹ì„ Parquet íŒŒì¼ë¡œ ì €ì¥

ì¶œë ¥ íŒŒì¼:
    - rnorm_fs_{year}_{quarter}.parquet: íƒ€ê²Ÿ ë³€ìˆ˜ ì—†ëŠ” íŠ¹ì„± (ìµœì‹  ì˜ˆì¸¡ìš©)
    - rnorm_ml_{year}_{quarter}.parquet: íƒ€ê²Ÿ ë³€ìˆ˜ í¬í•¨ íŠ¹ì„± (í•™ìŠµ/í…ŒìŠ¤íŠ¸ìš©)

ì‚¬ìš© ì˜ˆì‹œ:
    from config.context_loader import load_config, MainContext
    from training.make_mldata import AIDataMaker

    config = load_config('config/conf.yaml')
    ctx = MainContext(config)
    maker = AIDataMaker(ctx, config)
    # ML data files created in /data/ml_per_year/

ì‘ì„±ì: Quant Trading Team
ë‚ ì§œ: 2025-10-29
"""

import datetime
import logging
import os
from typing import Dict, Any, List, Optional
import numpy as np
import pandas as pd

from tqdm import tqdm
from tsfresh import extract_features
from tsfresh.feature_extraction import EfficientFCParameters, ComprehensiveFCParameters, MinimalFCParameters
from dateutil.relativedelta import relativedelta
from functools import reduce
from config.g_variables import ratio_col_list, meaning_col_list, cal_ev_col_list, sector_map, cal_timefeature_col_list
from config.logger import get_logger
from sklearn.preprocessing import StandardScaler, RobustScaler
from warnings import simplefilter
import warnings

# ë””ë²„ê¹…ì„ ìœ„í•œ pandas ë””ìŠ¤í”Œë ˆì´ ì„¤ì •
pd.options.display.width = 30

# ì„±ëŠ¥ ê²½ê³  ì–µì œ
simplefilter(action="ignore", category=pd.errors.PerformanceWarning)

# íŠ¹ì • ê²½ê³  ì–µì œ (ë°ì´í„° ê²€ì¦ì€ ì½”ë“œì—ì„œ ìˆ˜í–‰ë¨)
warnings.filterwarnings('ignore', category=RuntimeWarning, message='All-NaN slice encountered')
warnings.filterwarnings('ignore', category=FutureWarning, message='.*grouping columns.*')


class AIDataMaker:
    """
    ì¬ë¬´ì œí‘œì™€ ê°€ê²© ë°ì´í„°ë¡œë¶€í„° ML í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ì´ í´ë˜ìŠ¤ëŠ” ì™„ì „í•œ ML ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸ì„ ì¡°ìœ¨í•©ë‹ˆë‹¤:
    - VIEW íŒŒì¼ì—ì„œ ì›ì‹œ ê¸ˆìœµ ë°ì´í„° ë¡œë“œ
    - ë¦¬ë°¸ëŸ°ì‹± ë‚ ì§œ ìƒì„±
    - ê°€ê²© ë³€ë™ ê³„ì‚° (íƒ€ê²Ÿ ë³€ìˆ˜)
    - tsfreshë¡œ ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œ
    - ì»¤ìŠ¤í…€ ì¬ë¬´ ë¹„ìœ¨ ê³„ì‚°
    - RobustScalerë¡œ íŠ¹ì„± ì •ê·œí™”
    - ë¶„ê¸°ë³„ ML ë°ì´í„°ì…‹ì„ Parquet íŒŒì¼ë¡œ ì €ì¥

    ì¶œë ¥ ë°ì´í„°ì…‹ì€ ì£¼ì‹ ì„ íƒì„ ìœ„í•œ ML ëª¨ë¸ í•™ìŠµì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Attributes:
        main_ctx (MainContext): ì„¤ì •ê³¼ ê²½ë¡œë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸
        conf (Dict[str, Any]): YAMLì—ì„œ ë¡œë“œí•œ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        logger (logging.Logger): ì´ í´ë˜ìŠ¤ì˜ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
        rebalance_period (int): ë¦¬ë°¸ëŸ°ì‹± ê°„ê²©(ê°œì›”) (ê¸°ë³¸ê°’: 3)
        symbol_table (pd.DataFrame): ì£¼ì‹ ë©”íƒ€ë°ì´í„° (ì¢…ëª©ì½”ë“œ, ì„¹í„°, IPO ë‚ ì§œ ë“±)
        price_table (pd.DataFrame): ê³¼ê±° ê°€ê²© ë° ê±°ë˜ëŸ‰ ë°ì´í„°
        fs_table (pd.DataFrame): ì¬ë¬´ì œí‘œ (ì†ìµê³„ì‚°ì„œ, ëŒ€ì°¨ëŒ€ì¡°í‘œ, í˜„ê¸ˆíë¦„í‘œ)
        metrics_table (pd.DataFrame): ì¬ë¬´ ë©”íŠ¸ë¦­ (P/E, ROE, ë¶€ì±„ë¹„ìœ¨ ë“±)
        date_table_list (List): ë‚ ì§œ í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸ (ì‚¬ìš© ì•ˆ í•¨, ë ˆê±°ì‹œ)
        trade_date_list (List[pd.Timestamp]): ë¦¬ë°¸ëŸ°ì‹±ì„ ìœ„í•œ ì‹¤ì œ ê±°ë˜ ë‚ ì§œ

    Class Attributes:
        suffixes_dict (Dict[str, List[str]]): tsfresh íŠ¹ì„± ì„ íƒì„ ìœ„í•œ ì ‘ë¯¸ì‚¬ í•„í„°.
            ê° íŠ¹ì„± ìœ í˜•ì— ëŒ€í•´ ì£¼ìš” ì ‘ë¯¸ì‚¬ë§Œ ì„ íƒí•˜ì—¬ íŠ¹ì„± ì°¨ì›ì„ ì¶•ì†Œí•©ë‹ˆë‹¤
            (ì˜ˆ: r=0.0, 0.25, 0.6, 0.9ì—ì„œì˜ standard_deviation).

    ì‚¬ìš© ì˜ˆì‹œ:
        config = load_config('config/conf.yaml')
        ctx = MainContext(config)
        # 2015-2023ë…„ë„ ML ë°ì´í„° ìƒì„±
        maker = AIDataMaker(ctx, config)
        # ì¶œë ¥: ml_per_year/rnorm_ml_2015_Q1.parquet, rnorm_ml_2015_Q2.parquet, ...

    See Also:
        - config.g_variables: íŠ¹ì„± ë¦¬ìŠ¤íŠ¸ ë° ì„¹í„° ë§¤í•‘
        - training.regressor: ìƒì„±ëœ ML ë°ì´í„°ë¥¼ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©
        - tsfresh ë¬¸ì„œ: https://tsfresh.readthedocs.io/
    """

    # ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ tsfresh íŠ¹ì„± ì ‘ë¯¸ì‚¬ í•„í„°
    # ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ íŠ¹ì„± ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ íŠ¹ì • ì ‘ë¯¸ì‚¬ë§Œ ìœ ì§€
    suffixes_dict = {
        "standard_deviation": ["__r_0.0", "__r_0.25", "__r_0.6", "__r_0.9"],  # ì„œë¡œ ë‹¤ë¥¸ ë¡¤ë§ ìœˆë„ìš°
        "quantile": ["__q_0.2", "__q_0.8"],  # 20ë²ˆì§¸ ë° 80ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜
        "autocorrelation": ["__lag_0", "__lag_5", "__lag_9"],  # ì„œë¡œ ë‹¤ë¥¸ ì§€ì—° ê¸°ê°„
        "fft_coefficient": ["__coeff_0", "__coeff_33", "__coeff_99"],  # ì£¼ìš” í‘¸ë¦¬ì— ì„±ë¶„
        "cwt_coefficients": ["__coeff_0", "__coeff_6", "__coeff_12"],  # ì›¨ì´ë¸”ë¦¿ ë³€í™˜ ê³„ìˆ˜
        "symmetry_looking": ["__r_0.0", "__r_0.25", "__r_0.65", "__r_0.9"],  # ì„œë¡œ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ì˜ ëŒ€ì¹­ì„±
        "ar_coefficient": ["__coeff_0", "__coeff_3", "__coeff_6", "__coeff_10"]  # ìê¸°íšŒê·€ ê³„ìˆ˜
    }

    def __init__(self, main_ctx: 'MainContext', conf: Dict[str, Any]) -> None:
        """
        AIDataMakerë¥¼ ì´ˆê¸°í™”í•˜ê³  ì™„ì „í•œ ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìˆœì„œ:
        1. VIEW íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (load_bt_table)
        2. ë¦¬ë°¸ëŸ°ì‹± ë‚ ì§œ ìƒì„± (set_date)
        3. ê°€ê²© ë³€ë™ ê³„ì‚° (process_price_table_wdate)
        4. ì‹œê³„ì—´ íŠ¹ì„±ì„ í¬í•¨í•œ ML ë°ì´í„°ì…‹ ìƒì„± (make_ml_data)

        Args:
            main_ctx: ì„¤ì •ê³¼ ê²½ë¡œë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸
            conf: DATA, ML, BACKTEST ì„¹ì…˜ì„ í¬í•¨í•œ ì„¤ì • ë”•ì…”ë„ˆë¦¬

        Raises:
            FileNotFoundError: VIEW íŒŒì¼ì´ ëˆ„ë½ëœ ê²½ìš°
            ValueError: ìœ íš¨í•œ ê±°ë˜ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        """
        self.main_ctx = main_ctx
        self.conf = conf
        self.logger = get_logger('AIDataMaker')

        # ì„¤ì •ì—ì„œ ë¦¬ë°¸ëŸ°ì‹± ê¸°ê°„ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’: 3ê°œì›” = ë¶„ê¸°ë³„)
        backtest_config = conf.get('BACKTEST', {})
        self.rebalance_period = backtest_config.get('REBALANCE_PERIOD', 3)

        # ë°ì´í„° ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™”
        self.symbol_table = pd.DataFrame()
        self.price_table = pd.DataFrame()
        self.fs_table = pd.DataFrame()
        self.metrics_table = pd.DataFrame()
        self.date_table_list = []
        self.trade_date_list = []

        # ë‚ ì§œ í…Œì´ë¸”ì„ ìœ„í•œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.main_ctx.create_dir(self.main_ctx.root_path + "/DATE_TABLE")

        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        self.load_bt_table(main_ctx.start_year)
        self.set_date()
        self.process_price_table_wdate()
        self.make_ml_data(main_ctx.start_year, main_ctx.end_year)

    def load_bt_table(self, year: int) -> None:
        """
        VIEW íŒŒì¼ì—ì„œ ê¸ˆìœµ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        4ê°€ì§€ ìœ í˜•ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤:
        1. symbol_table: ì£¼ì‹ ë©”íƒ€ë°ì´í„° (ì¢…ëª©ì½”ë“œ, ì„¹í„°, ì—…ì¢…, IPO/ìƒì¥íì§€ ë‚ ì§œ)
        2. price_table: ì¼ë³„ ê°€ê²© ë° ê±°ë˜ëŸ‰ ë°ì´í„°
        3. fs_table: ì¬ë¬´ì œí‘œ (ì†ìµê³„ì‚°ì„œ, ëŒ€ì°¨ëŒ€ì¡°í‘œ, í˜„ê¸ˆíë¦„í‘œ) - 3ë…„ ì „ë¶€í„° ë¡œë“œ
        4. metrics_table: ì¬ë¬´ ë©”íŠ¸ë¦­ (P/E, ROE, ë¶€ì±„ë¹„ìœ¨) - 3ë…„ ì „ë¶€í„° ë¡œë“œ

        3ë…„ ë£©ë°±ì€ ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œì„ ìœ„í•œ ì¶©ë¶„í•œ ê³¼ê±° ë°ì´í„°ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.

        Args:
            year: ë°ì´í„° ë¡œë”© ì‹œì‘ ì—°ë„ (ì‚¬ìš© ì•ˆ í•¨, start_year-3ë¶€í„° end_yearê¹Œì§€ ë¡œë“œ)

        Raises:
            FileNotFoundError: VIEW ë””ë ‰í† ë¦¬ ë˜ëŠ” í•„ìˆ˜ íŒŒì¼ì´ ëˆ„ë½ëœ ê²½ìš°

        Note:
            - ì‹¬ë³¼ í…Œì´ë¸”ì€ symbolë¡œ ì¤‘ë³µ ì œê±°ë¨ (ì²« ë²ˆì§¸ ë°œìƒ ìœ ì§€)
            - ì¬ë¬´ì œí‘œì™€ ë©”íŠ¸ë¦­ì€ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì—°ë„ë³„ë¡œ ë¡œë“œ
            - ëˆ„ë½ëœ íŒŒì¼ì€ ë¡œê·¸ì— ê¸°ë¡ë˜ê³  ê±´ë„ˆëœ€ (ë¶€ë¶„ ë°ì´í„° ë¡œë”© í—ˆìš©)

        TODO:
            - CSV/Parquet íŒŒì¼ì˜ ëŒ€ì•ˆìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì§€ì› ì¶”ê°€
            - ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì„ ìœ„í•œ ì¦ë¶„ ë¡œë”© êµ¬í˜„
        """
        # ì£¼ì‹ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self.symbol_table = pd.read_csv(self.main_ctx.root_path + "/VIEW/symbol_list.csv")
        self.symbol_table = self.symbol_table.drop_duplicates('symbol', keep='first')
        self.symbol_table['ipoDate'] = pd.to_datetime(self.symbol_table['ipoDate'])
        self.symbol_table['delistedDate'] = pd.to_datetime(self.symbol_table['delistedDate'])

        # ê°€ê²© ë°ì´í„° ë¡œë“œ
        self.price_table = pd.read_csv(self.main_ctx.root_path + "/VIEW/price.csv")
        self.price_table['date'] = pd.to_datetime(self.price_table['date'])

        # ì¬ë¬´ì œí‘œ ë¡œë“œ (ì‹œê³„ì—´ íŠ¹ì„±ì„ ìœ„í•´ 3ë…„ ê³¼ê±° ë°ì´í„°)
        self.fs_table = pd.DataFrame()
        for year in range(self.main_ctx.start_year-3, self.main_ctx.end_year+1):
            fs_file = self.main_ctx.root_path + "/VIEW/financial_statement_" + str(year) + ".csv"
            if not os.path.exists(fs_file):
                self.logger.warning(f"Financial statement file not found, skipping: {fs_file}")
                print(f"WARNING: Financial statement file not found for year {year}")
                continue
            tmp_fs = pd.read_csv(fs_file,
                                    parse_dates=['fillingDate_x', 'acceptedDate_x'],
                                    dtype={'reportedCurrency_x': str, 'period_x': str,
                                        'link_x': str, 'finalLink_x': str})
            self.fs_table = pd.concat([tmp_fs, self.fs_table])

        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        if not self.fs_table.empty:
            self.fs_table['date'] = pd.to_datetime(self.fs_table['date'])
            self.fs_table['fillingDate'] = pd.to_datetime(self.fs_table['fillingDate'])
            self.fs_table['acceptedDate'] = pd.to_datetime(self.fs_table['acceptedDate'])

        # ì¬ë¬´ ë©”íŠ¸ë¦­ ë¡œë“œ (3ë…„ ê³¼ê±°)
        self.metrics_table = pd.DataFrame()
        for year in range(self.main_ctx.start_year-3, self.main_ctx.end_year+1):
            metrics_file = self.main_ctx.root_path + "/VIEW/metrics_" + str(year) + ".csv"
            if not os.path.exists(metrics_file):
                self.logger.warning(f"Metrics file not found, skipping: {metrics_file}")
                print(f"WARNING: Metrics file not found for year {year}")
                continue
            tmp_metrics = pd.read_csv(metrics_file,
                                        dtype={'period_x': str, 'period_y': str})
            self.metrics_table = pd.concat([tmp_metrics, self.metrics_table])

        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        if not self.metrics_table.empty:
            self.metrics_table['date'] = pd.to_datetime(self.metrics_table['date'])

    def get_trade_date(self, pdate: pd.Timestamp) -> Optional[pd.Timestamp]:
        """
        ë‹¬ë ¥ ë‚ ì§œë¥¼ ì‹¤ì œ ê±°ë˜ ë‚ ì§œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        ì£¼ì–´ì§„ ë‚ ì§œ ì´ì „ 10ì¼ ì´ë‚´ì˜ ê°€ì¥ ê°€ê¹Œìš´ ê±°ë˜ ë‚ ì§œ(ê°€ê²© ë°ì´í„°ê°€ ìˆëŠ” ë‚ ì§œ)ë¥¼
        ì°¾ìŠµë‹ˆë‹¤. ì´ëŠ” ì£¼ë§, ê³µíœ´ì¼, ì‹œì¥ íœ´ì¥ì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            pdate: ëª©í‘œ ë‹¬ë ¥ ë‚ ì§œ

        Returns:
            ê±°ë˜ ë‚ ì§œë¥¼ ì°¾ìœ¼ë©´ ë°˜í™˜, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ None

        ì‚¬ìš© ì˜ˆì‹œ:
            # pdateê°€ ì¼ìš”ì¼ì´ë©´, ì´ì „ ê¸ˆìš”ì¼ì˜ ê±°ë˜ ë‚ ì§œë¥¼ ë°˜í™˜
            trading_date = maker.get_trade_date(pd.Timestamp('2023-01-01'))
        """
        # ëª©í‘œ ë‚ ì§œ ì´ì „ 10ì¼ ì´ë‚´ì˜ ê±°ë˜ ë‚ ì§œ ê²€ìƒ‰
        post_date = pdate - relativedelta(days=10)
        res = self.price_table.query("date >= @post_date and date <= @pdate")
        if res.empty:
            return None
        else:
            return res.iloc[0].date

    def generate_date_list(self) -> List[datetime.datetime]:
        """
        ë¦¬ë°¸ëŸ°ì‹±ì„ ìœ„í•œ ë‹¬ë ¥ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        start_year-3ë¶€í„° end_yearê¹Œì§€ ê³ ì • ê°„ê²©(rebalance_period)ìœ¼ë¡œ ë‚ ì§œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        3ë…„ ë£©ë°±ì€ ì¶©ë¶„í•œ ê³¼ê±° ë°ì´í„°ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.

        Returns:
            rebalance_period ê°„ê²©ì˜ ë‹¬ë ¥ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸

        ì‚¬ìš© ì˜ˆì‹œ:
            # REBALANCE_PERIOD=3 (ë¶„ê¸°ë³„)ì¸ ê²½ìš°
            dates = maker.generate_date_list()
            # ë°˜í™˜: [2012-01-01, 2012-04-01, 2012-07-01, 2012-10-01, 2013-01-01, ...]
        """
        date_list = []

        # ì„¤ì •ì—ì„œ ì‹œì‘ ì›”/ì¼ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’: 1ì›” 1ì¼)
        backtest_config = self.conf.get('BACKTEST', {})
        start_month = backtest_config.get('START_MONTH', 1)
        start_date = backtest_config.get('START_DATE', 1)

        # ê³¼ê±° ë°ì´í„°ë¥¼ ìœ„í•´ start_yearë³´ë‹¤ 3ë…„ ì „ë¶€í„° ì‹œì‘
        date = datetime.datetime(int(self.main_ctx.start_year)-3, start_month, start_date)
        print(date)

        # ì‚¬ìš© ê°€ëŠ¥í•œ ê°€ê²© ë°ì´í„°ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ í•¨
        recent_date = self.price_table["date"].max()
        end_date = datetime.datetime(self.main_ctx.end_year, 12, 31)
        if end_date > recent_date:
            end_date = recent_date

        # rebalance_period ê°„ê²©ìœ¼ë¡œ ë‚ ì§œ ìƒì„±
        while date <= end_date:
            date_list.append(date)
            date += relativedelta(months=self.rebalance_period)

        return date_list

    def set_trade_date_list(self, date_list: List[datetime.datetime]) -> List[pd.Timestamp]:
        """
        ë‹¬ë ¥ ë‚ ì§œë¥¼ ì‹¤ì œ ê±°ë˜ ë‚ ì§œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        ê° ë‹¬ë ¥ ë‚ ì§œì— ëŒ€í•´ ê°€ê²© ë°ì´í„°ê°€ ìˆëŠ” ê°€ì¥ ê°€ê¹Œìš´ ê±°ë˜ ë‚ ì§œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        ê°€ê²© ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œëŠ” ê±´ë„ˆëœë‹ˆë‹¤.

        Args:
            date_list: ë‹¬ë ¥ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì‹¤ì œ ê±°ë˜ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸

        Raises:
            ValueError: ìœ íš¨í•œ ê±°ë˜ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°

        Note:
            - ê°€ê²© ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œëŠ” ê±´ë„ˆëœ€ (ì˜¤ë¥˜ ì•„ë‹˜)
            - ê±´ë„ˆë›´ ë‚ ì§œì— ëŒ€í•œ ê²½ê³ ë¥¼ ë¡œê·¸ì— ê¸°ë¡
            - ìµœì†Œ í•˜ë‚˜ì˜ ìœ íš¨í•œ ê±°ë˜ ë‚ ì§œê°€ í•„ìš”
        """
        trade_date_list = []
        price_min_date = self.price_table["date"].min()
        price_max_date = self.price_table["date"].max()

        for date in date_list:
            tdate = self.get_trade_date(date)
            if tdate is None:
                # ì´ ë‚ ì§œì— ëŒ€í•œ ê°€ê²© ë°ì´í„°ê°€ ì—†ìŒ - ê±´ë„ˆëœ€
                self.logger.warning(f"âš ï¸  Cannot find tradable date for {date.strftime('%Y-%m-%d')}")
                self.logger.warning(f"   Price data range: {price_min_date.strftime('%Y-%m-%d')} to {price_max_date.strftime('%Y-%m-%d')}")
                self.logger.warning(f"   Skipping dates before price data is available...")
                print(f"âš ï¸  WARNING: Skipping {date.strftime('%Y-%m-%d')} - no price data available")
                continue
            trade_date_list.append(tdate)

        if not trade_date_list:
            error_msg = f"âŒ FATAL: No valid trading dates found! Price data range: {price_min_date} to {price_max_date}"
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        self.logger.info(f"âœ… Found {len(trade_date_list)} valid trading dates")
        return trade_date_list


    def set_date(self) -> None:
        """
        ë¦¬ë°¸ëŸ°ì‹±ì„ ìœ„í•œ ê±°ë˜ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ë‹¬ë ¥ ë‚ ì§œë¥¼ ìƒì„±í•˜ê³  ì‹¤ì œ ê±°ë˜ ë‚ ì§œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        ë°ì´í„° ì¤€ë¹„ ì „ë°˜ì— ì‚¬ìš©ë˜ëŠ” self.trade_date_listë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        date_list = self.generate_date_list()
        self.trade_date_list = self.set_trade_date_list(date_list)


    def process_price_table_wdate(self) -> None:
        """
        ë¦¬ë°¸ëŸ°ì‹± ë‚ ì§œì— ëŒ€í•œ ê°€ê²© ë³€ë™(íƒ€ê²Ÿ ë³€ìˆ˜)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        ì²˜ë¦¬ ë‹¨ê³„:
        1. ê°€ê²© í…Œì´ë¸”ì„ ë¦¬ë°¸ëŸ°ì‹± ë‚ ì§œë§Œìœ¼ë¡œ í•„í„°ë§
        2. ì¢…ëª©ì½”ë“œì™€ ë‚ ì§œë¡œ ì •ë ¬
        3. price_diff ê³„ì‚°: ì´ì „ ë¦¬ë°¸ëŸ°ì‹±ë¶€í„°ì˜ ì ˆëŒ€ ê°€ê²© ë³€ë™
        4. volume_mul_price ê³„ì‚°: ìœ ë™ì„± ì§€í‘œ (ê°€ê²© Ã— ê±°ë˜ëŸ‰)
        5. price_dev ê³„ì‚°: ê°€ê²© ë³€ë™ë¥  (ìˆ˜ìµë¥ )

        íƒ€ê²Ÿ ë³€ìˆ˜:
            price_dev = (price_t - price_{t-1}) / price_{t-1}
            ì´ëŠ” ì´ì „ ë¦¬ë°¸ëŸ°ì‹±ë¶€í„° í˜„ì¬ ë¦¬ë°¸ëŸ°ì‹±ê¹Œì§€ ì£¼ì‹ì„ ë³´ìœ í•˜ì—¬ ë‹¬ì„±í•œ ìˆ˜ìµë¥ ì…ë‹ˆë‹¤.

        ì €ì¥:
            ì°¸ì¡°ìš©ìœ¼ë¡œ VIEW ë””ë ‰í† ë¦¬ì— price_diff.csv

        Note:
            - ê° ì¢…ëª©ì˜ ì²« ë²ˆì§¸ í–‰ì€ price_diffì™€ price_devê°€ NaN (ì´ì „ ë°ì´í„° ì—†ìŒ)
            - ì´ëŸ¬í•œ í–‰ì€ ML ë°ì´í„° ìƒì„± ì¤‘ì— í•„í„°ë§ë¨
        """
        # ë¦¬ë°¸ëŸ°ì‹± ë‚ ì§œë§Œìœ¼ë¡œ í•„í„°ë§
        self.price_table = self.price_table[self.price_table['date'].isin(self.trade_date_list)]

        # ìˆœì°¨ ê³„ì‚°ì„ ìœ„í•´ ì¢…ëª©ì½”ë“œì™€ ë‚ ì§œë¡œ ì •ë ¬
        self.price_table = self.price_table.sort_values(by=['symbol', 'date'])

        # ì´ì „ ë¦¬ë°¸ëŸ°ì‹±ë¶€í„°ì˜ ì ˆëŒ€ ê°€ê²© ë³€ë™ ê³„ì‚°
        self.price_table['price_diff'] = self.price_table.groupby('symbol')['close'].diff()

        # ìœ ë™ì„± ì§€í‘œ ê³„ì‚° (ì €ìœ ë™ì„± ì£¼ì‹ í•„í„°ë§ì— ì‚¬ìš©)
        self.price_table['volume_mul_price'] = self.price_table['close'] * self.price_table['volume']

        # ë°±ë¶„ìœ¨ ìˆ˜ìµë¥  ê³„ì‚° (ML ëª¨ë¸ì˜ íƒ€ê²Ÿ ë³€ìˆ˜)
        # ë¶„ëª¨ë¡œ ì´ì „ ì¢…ê°€ ì‚¬ìš© (í˜„ì¬ ì¢…ê°€ ì•„ë‹˜)
        self.price_table['price_dev'] = self.price_table['price_diff'] / self.price_table.groupby('symbol')['close'].shift(1)

        # ëª…í™•ì„±ì„ ìœ„í•œ ì»¬ëŸ¼ëª… ë³€ê²½
        self.price_table.rename(columns={'close': 'price'}, inplace=True)

        # ì°¸ì¡°ë¥¼ ìœ„í•´ ì €ì¥
        self.price_table.to_csv(self.main_ctx.root_path + "/VIEW/price_diff.csv", index=False)


    def filter_columns_by_suffixes(self, df: pd.DataFrame) -> List[str]:
        """
        ì ‘ë¯¸ì‚¬ë¡œ tsfresh ì»¬ëŸ¼ì„ í•„í„°ë§í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•©ë‹ˆë‹¤.

        tsfreshëŠ” ì…ë ¥ ì»¬ëŸ¼ë‹¹ ìˆ˜ë°± ê°œì˜ íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ”
        suffixes_dictì— ì •ì˜ëœ íŠ¹ì • ì ‘ë¯¸ì‚¬ë¥¼ ê°€ì§„ íŠ¹ì„±ë§Œ ìœ ì§€í•˜ë„ë¡ í•„í„°ë§í•˜ì—¬
        ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ íŠ¹ì„± ìˆ˜ë¥¼ ê·¹ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.

        Args:
            df: tsfresh íŠ¹ì„±ì´ í¬í•¨ëœ DataFrame

        Returns:
            ìœ ì§€í•  ì»¬ëŸ¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        ì‚¬ìš© ì˜ˆì‹œ:
            # ì´ì „: ['revenue_ts_standard_deviation__r_0.0', '__r_0.05', '__r_0.1', ...]
            # ì´í›„: ['revenue_ts_standard_deviation__r_0.0', '__r_0.25', '__r_0.6', '__r_0.9']
            filtered_cols = maker.filter_columns_by_suffixes(features_df)
        """
        filtered_cols = []
        for col in df.columns:
            include_col = True

            # ì»¬ëŸ¼ì´ suffixes_dictì˜ í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            for keyword, suffixes in self.suffixes_dict.items():
                if keyword in col:
                    # í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ë©´, ì»¬ëŸ¼ì´ ìœ íš¨í•œ ì ‘ë¯¸ì‚¬ë¥¼ ê°€ì§„ ê²½ìš°ë§Œ í¬í•¨
                    if any(suffix in col for suffix in suffixes):
                        break  # ìœ íš¨í•œ ì ‘ë¯¸ì‚¬ ë°œê²¬, ì»¬ëŸ¼ í¬í•¨
                    else:
                        include_col = False  # í‚¤ì›Œë“œëŠ” ìˆì§€ë§Œ ìœ íš¨í•œ ì ‘ë¯¸ì‚¬ ì—†ìŒ, ì œì™¸
                        break

            if include_col:
                filtered_cols.append(col)

        return filtered_cols

    def filter_dates(self, df: pd.DataFrame, target_col_name: str, start_year: int, end_year: int) -> pd.DataFrame:
        """
        ë‚ ì§œ ë²”ìœ„ë¡œ DataFrameì„ í•„í„°ë§í•©ë‹ˆë‹¤.

        end_yearì˜ ëª¨ë“  Q4 ë°ì´í„°ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼
        [start_year-01-01, end_year+1-03-01]ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤.

        Args:
            df: í•„í„°ë§í•  DataFrame
            target_col_name: í•„í„°ë§í•  ë‚ ì§œ ì»¬ëŸ¼ ì´ë¦„
            start_year: ì‹œì‘ ì—°ë„ (í¬í•¨)
            end_year: ì¢…ë£Œ ì—°ë„ (í¬í•¨, ë‹¤ìŒ í•´ 3ì›”ê¹Œì§€ í™•ì¥)

        Returns:
            í•„í„°ë§ëœ DataFrame
        """
        df[target_col_name] = pd.to_datetime(df[target_col_name])
        start_date = pd.Timestamp(year=start_year, month=1, day=1)
        end_date = pd.Timestamp(year=end_year+1, month=3, day=1)  # Q4 ì‹ ê³ ë¥¼ í¬í•¨í•˜ê¸° ìœ„í•´ í™•ì¥
        filtered_df = df[(df[target_col_name] >= start_date) & (df[target_col_name] <= end_date)]
        return filtered_df

    def reorder_columns(self, df: pd.DataFrame, keywords: List[str] = ['symbol', 'date']) -> pd.DataFrame:
        """
        ì£¼ìš” ì»¬ëŸ¼ì„ ì•ìœ¼ë¡œ ì´ë™ì‹œí‚¤ê¸° ìœ„í•´ DataFrame ì»¬ëŸ¼ì„ ì¬ì •ë ¬í•©ë‹ˆë‹¤.

        Args:
            df: ì¬ì •ë ¬í•  DataFrame
            keywords: ì•ìœ¼ë¡œ ì´ë™ì‹œí‚¬ ì»¬ëŸ¼ í‚¤ì›Œë“œ

        Returns:
            ì¬ì •ë ¬ëœ ì»¬ëŸ¼ì„ ê°€ì§„ DataFrame
        """
        key_cols = [col for col in df.columns if any(key.lower() in col.lower() for key in keywords)]
        other_cols = [col for col in df.columns if col not in key_cols]
        new_order = key_cols + other_cols
        return df[new_order]

    def efficient_merge_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        DataFrame ì¡°ì¸ì—ì„œ ë°œìƒí•œ _x ë° _y ì ‘ë¯¸ì‚¬ ì»¬ëŸ¼ì„ ë³‘í•©í•©ë‹ˆë‹¤.

        DataFrame ë³‘í•© í›„ ì¤‘ë³µ ì»¬ëŸ¼ì€ _x ë° _y ì ‘ë¯¸ì‚¬ë¥¼ ê°–ê²Œ ë©ë‹ˆë‹¤.
        ì´ ë©”ì„œë“œëŠ” ì–´ëŠ ì»¬ëŸ¼ì—ì„œë“  nullì´ ì•„ë‹Œ ê°’ì„ ì„ í˜¸í•˜ì—¬ ë‹¤ì‹œ ë³‘í•©í•©ë‹ˆë‹¤.

        Args:
            df: _x ë° _y ì ‘ë¯¸ì‚¬ ì»¬ëŸ¼ì´ ìˆëŠ” DataFrame

        Returns:
            ë³‘í•©ëœ ì»¬ëŸ¼ì„ ê°€ì§„ DataFrame

        ì‚¬ìš© ì˜ˆì‹œ:
            # ì´ì „: revenue_x, revenue_y
            # ì´í›„: revenue (revenue_x.combine_first(revenue_y)ë¡œ ê²°í•©)
        """
        # ê³ ìœ í•œ ê¸°ë³¸ ì»¬ëŸ¼ ì´ë¦„ ì¶”ì¶œ
        base_cols = set(col.rstrip('_x').rstrip('_y') for col in df.columns)

        result_df = pd.DataFrame(index=df.index)
        for base in base_cols:
            # ì´ ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì»¬ëŸ¼ ì°¾ê¸°
            cols = [col for col in df.columns if col.startswith(base)]
            # combine_firstë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘í•© (ì²« ë²ˆì§¸ nullì´ ì•„ë‹Œ ê°’ ì„ í˜¸)
            merged_col = reduce(lambda x, y: df[y].combine_first(x), cols, pd.Series([np.nan]*len(df), index=df.index))
            result_df[base] = merged_col

        result_df = self.reorder_columns(result_df)
        return result_df


    def make_ml_data(self, start_year: int, end_year: int) -> None:
        """
        ì‹œê³„ì—´ íŠ¹ì„±ì„ í¬í•¨í•œ ML í•™ìŠµ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

        ì§€ì •ëœ ì—°ë„ ë²”ìœ„ì— ëŒ€í•´ ë¶„ê¸°ë³„ ML ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.

        ê° ì—°ë„ë³„ ì²˜ë¦¬ ê³¼ì •:
        1. ê³ ìœ ë™ì„± ì£¼ì‹ìœ¼ë¡œ í•„í„°ë§ (ê±°ë˜ëŸ‰ ê¸°ì¤€ ìƒìœ„ 50%)
        2. ì¢…ëª©, ê°€ê²©, ì¬ë¬´ì œí‘œ, ë©”íŠ¸ë¦­ ë³‘í•©
        3. ê° ë¶„ê¸°(Q1, Q2, Q3, Q4)ë§ˆë‹¤:
           a. 12ê°œì›” ë£©ë°± ìœˆë„ìš° ìƒì„±
           b. tsfresh ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œ
           c. ì»¤ìŠ¤í…€ ì¬ë¬´ ë¹„ìœ¨ ê³„ì‚° (OverMC_*, adaptiveMC_*)
           d. RobustScalerë¡œ ì •ê·œí™”
           e. íƒ€ê²Ÿ ë³€ìˆ˜ ê³„ì‚° (price_dev, price_dev_subavg)
           f. Parquet íŒŒì¼ë¡œ ì €ì¥

        Args:
            start_year: ì²˜ë¦¬í•  ì²« ë²ˆì§¸ ì—°ë„
            end_year: ì²˜ë¦¬í•  ë§ˆì§€ë§‰ ì—°ë„

        ì¶œë ¥ íŒŒì¼:
            ê° ì—°ë„ì™€ ë¶„ê¸°ë³„ë¡œ:
            - rnorm_fs_{year}_{quarter}.parquet: íŠ¹ì„±ë§Œ (ìµœì‹  ì˜ˆì¸¡ìš©)
            - rnorm_ml_{year}_{quarter}.parquet: íŠ¹ì„± + íƒ€ê²Ÿ (í•™ìŠµìš©)

        ì‚¬ìš© ì˜ˆì‹œ:
            maker.make_ml_data(2015, 2023)
            # ìƒì„±: rnorm_ml_2015_Q1.parquet, rnorm_ml_2015_Q2.parquet, ...

        Notes:
            - 12ê°œ ì´ìƒì˜ ë¶„ê¸° ë°ì´í„°ê°€ ìˆëŠ” ì£¼ì‹ë§Œ ì²˜ë¦¬
            - ì €ìœ ë™ì„± ì£¼ì‹ í•„í„°ë§ (ê±°ë˜ëŸ‰ í•˜ìœ„ 50%)
            - ë°ì´í„°ê°€ ë¶ˆì¶©ë¶„í•œ ë¶„ê¸°ëŠ” ê±´ë„ˆëœ€ (ê²½ê³  ë¡œê·¸)
            - ì•„ì›ƒë¼ì´ì–´ ì €í•­ì„± ì •ê·œí™”ë¥¼ ìœ„í•´ RobustScaler ì‚¬ìš©
            - ì„¹í„° ì¡°ì • ìˆ˜ìµë¥  ê³„ì‚° (sec_price_dev_subavg)

        TODO:
            - ë£©ë°± ìœˆë„ìš° ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ ì˜µì…˜ ì¶”ê°€ (í˜„ì¬ 12ë¡œ ê³ ì •)
            - ìœ ë™ì„± ì„ê³„ê°’ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ ì˜µì…˜ ì¶”ê°€ (í˜„ì¬ 50%)
            - ì»¤ìŠ¤í…€ íŠ¹ì„± ì¶”ì¶œ íŒŒë¼ë¯¸í„° ì§€ì› ì¶”ê°€
        """
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        ml_dir = os.path.join(self.main_ctx.root_path, "ml_per_year")
        self.main_ctx.create_dir(ml_dir)

        for cur_year in range(start_year, end_year+1):
            # ì¢…ëª© í…Œì´ë¸”ë¡œ ì‹œì‘
            table_for_ai = self.symbol_table.copy()

            # ê°€ê²© ë°ì´í„°ë¥¼ í˜„ì¬ ì—°ë„ Â± 4ë…„ìœ¼ë¡œ í•„í„°ë§ (ì‹œê³„ì—´ íŠ¹ì„±ìš©)
            cur_price_table = self.price_table.copy()
            cur_price_table = self.filter_dates(cur_price_table, 'date', cur_year-4, cur_year)

            # ê³ ìœ ë™ì„± ì£¼ì‹ìœ¼ë¡œ í•„í„°ë§ (í‰ê·  ê±°ë˜ ê¸ˆì•¡ ê¸°ì¤€ ìƒìœ„ 50%)
            symbol_means = cur_price_table.groupby('symbol')['volume_mul_price'].mean().reset_index()
            top_symbols = symbol_means.nlargest(int(len(symbol_means) * 0.50), 'volume_mul_price')
            cur_price_table = cur_price_table[cur_price_table['symbol'].isin(top_symbols['symbol'])]

            # ì¢…ëª© í…Œì´ë¸”ê³¼ ë³‘í•©
            table_for_ai = pd.merge(table_for_ai, cur_price_table, how='inner', on='symbol')
            table_for_ai.rename(columns={'date': 'rebalance_date'}, inplace=True)

            # ì¬ë¬´ì œí‘œ ì¤€ë¹„
            fs = self.fs_table.copy()
            fs = fs[fs['symbol'].isin(top_symbols['symbol'])]
            fs = self.filter_dates(fs, 'fillingDate', cur_year-4, cur_year)
            fs = fs.drop_duplicates(['symbol', 'date'], keep='first')

            # ë©”íŠ¸ë¦­ ì¤€ë¹„
            metrics = self.metrics_table.copy()
            metrics = metrics[metrics['symbol'].isin(top_symbols['symbol'])]
            metrics = metrics.drop_duplicates(['symbol', 'date'], keep='first')

            # ì¬ë¬´ì œí‘œì™€ ë©”íŠ¸ë¦­ ë³‘í•©
            common_columns = fs.columns.intersection(metrics.columns)
            fs = fs.drop(columns=common_columns.difference(['symbol', 'date']))
            fs_metrics = pd.merge(fs, metrics, how='inner', on=['symbol', 'date'])
            fs_metrics = fs_metrics.drop_duplicates(['symbol', 'date'], keep='first')

            # ë‚ ì§œ ì¤€ë¹„
            fs_metrics['date'] = pd.to_datetime(fs_metrics['date'])
            fs_metrics.rename(columns={'date': 'report_date'}, inplace=True)
            fs_metrics['fillingDate'] = pd.to_datetime(fs_metrics['fillingDate'])

            # ê° ì‹ ê³  ë‚ ì§œë¥¼ ë‹¤ìŒ ë¦¬ë°¸ëŸ°ì‹± ë‚ ì§œì— ë§¤í•‘
            # ì´ëŠ” ê° ë¦¬ë°¸ëŸ°ì‹±ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ë§Œ ì‚¬ìš©í•˜ë„ë¡ ë³´ì¥
            date_index = np.sort(pd.DatetimeIndex(self.trade_date_list.copy()))
            indices = np.searchsorted(date_index, fs_metrics['fillingDate'], side='right')
            fs_metrics['rebalance_date'] = [date_index[i] if i < len(date_index) else pd.NaT for i in indices]

            # ì‹œê°„ ìˆœì„œë¥¼ ìœ„í•œ year_period ìƒì„±
            # Q1=.2, Q2=.4, Q3=.6, Q4=.8ë¡œ ì†Œìˆ˜ì  ì •ë ¬ ê°€ëŠ¥
            fs_metrics = fs_metrics.dropna(subset=['calendarYear'])
            period_map = {'Q1': 0.2, 'Q2': 0.4, 'Q3': 0.6, 'Q4': 0.8}
            fs_metrics['year_period'] = fs_metrics['calendarYear'] + fs_metrics['period'].map(period_map)
            fs_metrics = fs_metrics.sort_values(by=['symbol', 'year_period'])

            # tsfreshë¥¼ ìœ„í•œ ì‹œê°„ ì¸ë±ìŠ¤ í• ë‹¹ (12ë¶„ê¸° ìœˆë„ìš°ì˜ ê²½ìš° 0, 1, 2, ..., 11)
            def assign_time(group):
                group = group.sort_values(by='year_period').reset_index(drop=True)
                group['time_for_sort'] = range(len(group))
                return group

            fs_metrics = fs_metrics.groupby('symbol', group_keys=False).apply(assign_time).reset_index(drop=True)

            # ì»¤ìŠ¤í…€ ë¹„ìœ¨ ê³„ì‚°: OverMC_* (ë©”íŠ¸ë¦­ / ì‹œê°€ì´ì•¡)
            for col in meaning_col_list:
                if col not in fs_metrics.columns:
                    continue
                new_col_name = 'OverMC_' + col
                fs_metrics[new_col_name] = np.where(fs_metrics['marketCap'] > 0,
                                                    fs_metrics[col]/fs_metrics['marketCap'], np.nan)

            # ì»¤ìŠ¤í…€ ë¹„ìœ¨ ê³„ì‚°: adaptiveMC_* (EV / ë©”íŠ¸ë¦­)
            # EV (ê¸°ì—…ê°€ì¹˜) = ì‹œê°€ì´ì•¡ + ìˆœë¶€ì±„
            fs_metrics["adaptiveMC_ev"] = fs_metrics['marketCap'] + fs_metrics["netDebt"]
            for col in cal_ev_col_list:
                new_col_name = 'adaptiveMC_' + col
                fs_metrics[new_col_name] = np.where(fs_metrics[col] > 0,
                                                    fs_metrics['adaptiveMC_ev']/fs_metrics[col], np.nan)

            # ë””ë²„ê¹…ì„ ìœ„í•œ ìŠ¤ëƒ…ìƒ· ì €ì¥
            print("*** fs_metrics w/ rebalance_date")
            print(fs_metrics)
            fs_metrics.head(1000).to_csv(self.main_ctx.root_path + f"/fs_metric_wdate_{str(cur_year)}.csv", index=False)

            # ê° ë¶„ê¸° ì²˜ë¦¬
            for quarter_str, quarter in [('Q1', 0.2), ('Q2', 0.4), ('Q3', 0.6), ('Q4', 0.8)]:
                base_year_period = cur_year + quarter

                # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
                file_path = os.path.join(self.main_ctx.root_path, "ml_per_year", f"rnorm_fs_{str(cur_year)}_{quarter_str}.parquet")
                file2_path = os.path.join(self.main_ctx.root_path, "ml_per_year", f"rnorm_ml_{str(cur_year)}_{quarter_str}.parquet")

                # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆë›°ê¸°
                if os.path.isfile(file2_path):
                    print(f"*** there is parquet file {str(cur_year)}_{quarter_str}")
                    continue

                print(base_year_period)

                # í˜„ì¬ ë¶„ê¸°ê¹Œì§€ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                filtered_data = fs_metrics[fs_metrics['year_period'] <= float(base_year_period)]

                # 12ë¶„ê¸° ë£©ë°± ìœˆë„ìš° ìƒì„±
                def get_last_12_rows(group):
                    return group.tail(12)

                window_data = filtered_data.groupby('symbol', group_keys=False).apply(get_last_12_rows).reset_index(drop=True)
                print(window_data)

                # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if window_data.empty:
                    self.logger.warning(f"No data available for {cur_year}_{quarter_str}. Skipping...")
                    print(f"âš ï¸  WARNING: No data for {cur_year}_{quarter_str} - skipping file generation")
                    continue

                # 12ë¶„ê¸° ë¯¸ë§Œì˜ ë°ì´í„°ë¥¼ ê°€ì§„ ì¢…ëª© í•„í„°ë§
                symbol_counts = window_data['symbol'].value_counts()
                symbols_to_remove = symbol_counts[symbol_counts < 12].index
                window_data = window_data[~window_data['symbol'].isin(symbols_to_remove)]

                if window_data.empty:
                    self.logger.warning(f"No symbols with sufficient data (12+ rows) for {cur_year}_{quarter_str}. Skipping...")
                    print(f"âš ï¸  WARNING: No symbols with 12+ data points for {cur_year}_{quarter_str} - skipping")
                    continue

                # tsfreshë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œ
                df_for_extract_feature = pd.DataFrame()

                # ë””ë²„ê¹…: ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œ ì‹œì‘
                self.logger.info(f"ğŸ” [{base_year_period}] Starting time series feature extraction")
                self.logger.info(f"   Total columns to process: {len(cal_timefeature_col_list)}")
                self.logger.info(f"   Window data shape: {window_data.shape}")
                self.logger.info(f"   Unique symbols in window: {window_data['symbol'].nunique()}")

                filtered_col_count = 0
                accepted_col_count = 0
                filter_reasons = {'not_in_columns': 0, 'has_nan': 0, 'has_infinite': 0}

                for target_col in cal_timefeature_col_list:
                    # ìœ íš¨í•œ (NaNì´ ì•„ë‹Œ, ìœ í•œí•œ) ë°ì´í„°ê°€ ìˆëŠ” ì»¬ëŸ¼ë§Œ ì²˜ë¦¬
                    if target_col not in window_data.columns:
                        filtered_col_count += 1
                        filter_reasons['not_in_columns'] += 1
                        continue

                    if window_data[target_col].isna().any():
                        filtered_col_count += 1
                        filter_reasons['has_nan'] += 1
                        continue

                    if not np.isfinite(window_data[target_col]).all():
                        filtered_col_count += 1
                        filter_reasons['has_infinite'] += 1
                        continue

                    # tsfresh í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„
                    temp_df = pd.DataFrame({
                        'id': window_data['symbol'],
                        'kind': target_col,
                        'time': window_data['time_for_sort'],
                        'value': window_data[target_col].values,
                        'year_period': window_data['year_period']
                    })
                    df_for_extract_feature = pd.concat([df_for_extract_feature, temp_df])
                    accepted_col_count += 1

                # ë””ë²„ê¹…: í•„í„°ë§ ê²°ê³¼
                self.logger.info(f"   Columns accepted: {accepted_col_count}/{len(cal_timefeature_col_list)}")
                self.logger.info(f"   Columns filtered: {filtered_col_count} (not_in_data={filter_reasons['not_in_columns']}, has_nan={filter_reasons['has_nan']}, has_infinite={filter_reasons['has_infinite']})")
                self.logger.info(f"   df_for_extract_feature shape: {df_for_extract_feature.shape}")

                if not df_for_extract_feature.empty:
                    # tsfreshë¡œ íŠ¹ì„± ì¶”ì¶œ
                    self.logger.info(f"ğŸ”„ [{base_year_period}] Running tsfresh feature extraction...")
                    features = extract_features(df_for_extract_feature,
                                               column_id='id',
                                               column_kind='kind',
                                               column_sort='time',
                                               column_value='value',
                                               default_fc_parameters=EfficientFCParameters())

                    self.logger.info(f"   Extracted features shape: {features.shape}")
                    self.logger.info(f"   Unique symbols in features: {len(features.index)}")

                    # '_ts_' ë§ˆì»¤ë¥¼ í¬í•¨í•˜ë„ë¡ ì»¬ëŸ¼ëª… ë³€ê²½
                    features = features.rename(columns=lambda x: f"{x.partition('__')[0]}_ts_{x.partition('__')[2]}")

                    # ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•´ ì ‘ë¯¸ì‚¬ë¡œ íŠ¹ì„± í•„í„°ë§
                    self.logger.info(f"ğŸ”„ [{base_year_period}] Filtering columns by suffixes...")
                    self.logger.info(f"   Before suffix filtering: {features.shape[1]} columns")
                    filtered_columns = self.filter_columns_by_suffixes(features)
                    self.logger.info(f"   After suffix filtering: {len(filtered_columns)} columns")

                    df_w_time_feature = features[filtered_columns].copy()
                    df_w_time_feature['symbol'] = features.index

                    # í˜„ì¬ ë¶„ê¸°ë§Œìœ¼ë¡œ í•„í„°ë§
                    self.logger.info(f"ğŸ”„ [{base_year_period}] Merging with window_data...")
                    window_data_before = window_data.copy()

                    # ë””ë²„ê¹…: window_dataì˜ year_period ê°’ë“¤ í™•ì¸
                    unique_periods = sorted(window_data['year_period'].unique())
                    self.logger.info(f"   window_data BEFORE filter - year_period values: {unique_periods[:20]}")
                    self.logger.info(f"   Total unique year_periods: {len(unique_periods)}")
                    self.logger.info(f"   Looking for year_period: {float(base_year_period)}")
                    self.logger.info(f"   Is {base_year_period} in window_data? {float(base_year_period) in window_data['year_period'].values}")

                    window_data = window_data[window_data['year_period'] == float(base_year_period)]
                    self.logger.info(f"   window_data after year_period filter: {window_data.shape[0]} rows, {window_data['symbol'].nunique() if not window_data.empty else 0} symbols")
                    self.logger.info(f"   df_w_time_feature before merge: {df_w_time_feature.shape[0]} rows")

                    df_w_time_feature = pd.merge(window_data, df_w_time_feature, how='inner', on='symbol')
                    self.logger.info(f"   After merge: {df_w_time_feature.shape}")

                    # ì ˆëŒ€ê°’ ì»¬ëŸ¼ ì œê±° (MLì— ìœ ìš©í•˜ì§€ ì•ŠìŒ, ë¹„ìœ¨ë§Œ ì¤‘ìš”)
                    abs_col_list = list(set(meaning_col_list) - set(ratio_col_list))
                    self.logger.info(f"ğŸ”„ [{base_year_period}] Removing absolute value columns...")
                    self.logger.info(f"   Absolute columns to remove: {len(abs_col_list)}")
                    cols_before_abs_removal = df_w_time_feature.shape[1]

                    for col in abs_col_list:
                        df_w_time_feature = df_w_time_feature.drop([col], axis=1, errors='ignore')

                    self.logger.info(f"   Columns before removal: {cols_before_abs_removal}, after: {df_w_time_feature.shape[1]}")

                    # ì •ê·œí™”í•˜ì§€ ì•Šì„ ì»¬ëŸ¼ ë¶„ë¦¬
                    excluded_columns = ['symbol', 'rebalance_date', 'report_date', 'fillingDate_x', 'year_period']
                    excluded_df = df_w_time_feature[excluded_columns]

                    # ì •ê·œí™”í•  ì»¬ëŸ¼ ì„ íƒ
                    self.logger.info(f"ğŸ”„ [{base_year_period}] Selecting columns for normalization...")
                    self.logger.info(f"   Total columns in df_w_time_feature: {len(df_w_time_feature.columns)}")

                    # ê° ì¡°ê±´ë³„ë¡œ ë§¤ì¹­ë˜ëŠ” ì»¬ëŸ¼ ë¶„ì„
                    ts_cols = [col for col in df_w_time_feature.columns if '_ts_' in col]
                    ratio_cols = [col for col in df_w_time_feature.columns if col in ratio_col_list]
                    overmc_cols = [col for col in df_w_time_feature.columns if col.startswith('OverMC_')]
                    adaptive_cols = [col for col in df_w_time_feature.columns if col.startswith('adaptiveMC_')]

                    self.logger.info(f"   Columns by type:")
                    self.logger.info(f"     - Time series (_ts_): {len(ts_cols)}")
                    self.logger.info(f"     - Ratio columns: {len(ratio_cols)}")
                    self.logger.info(f"     - OverMC_ columns: {len(overmc_cols)}")
                    self.logger.info(f"     - adaptiveMC_ columns: {len(adaptive_cols)}")

                    filtered_columns = [
                        col for col in df_w_time_feature.columns
                        if ('_ts_' in col) or  # ì‹œê³„ì—´ íŠ¹ì„±
                        (col in ratio_col_list) or  # ì¬ë¬´ ë¹„ìœ¨
                        col.startswith('OverMC_') or  # ì»¤ìŠ¤í…€ ë¹„ìœ¨
                        col.startswith('adaptiveMC_')  # ì»¤ìŠ¤í…€ EV ë¹„ìœ¨
                    ]
                    filtered_df = df_w_time_feature[filtered_columns]

                    self.logger.info(f"   Total columns selected for scaling: {len(filtered_columns)}")
                    self.logger.info(f"   Filtered df shape: {filtered_df.shape}")

                    # ë¹ˆ ë°ì´í„° ì²´í¬
                    if filtered_df.empty or len(filtered_df) == 0:
                        self.logger.warning(f"âŒ [{base_year_period}] No data to scale - SKIPPING")
                        self.logger.warning(f"   Available columns in df_w_time_feature: {len(df_w_time_feature.columns)}")
                        self.logger.warning(f"   Columns after filtering: {len(filtered_columns)}")
                        self.logger.warning(f"   Rows in filtered_df: {len(filtered_df)}")

                        # ìƒ˜í”Œ ì»¬ëŸ¼ ì´ë¦„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                        sample_cols = list(df_w_time_feature.columns[:20])
                        self.logger.warning(f"   Sample column names (first 20): {sample_cols}")

                        self.logger.warning(f"   REASON: No columns matched the scaling criteria")
                        continue

                    # RobustScalerë¡œ ì •ê·œí™” (ì•„ì›ƒë¼ì´ì–´ì— ê°•í•¨)
                    self.logger.info(f"âœ… [{base_year_period}] Scaling {filtered_df.shape[1]} columns for {filtered_df.shape[0]} symbols")
                    scaler = RobustScaler()
                    scaled_data = scaler.fit_transform(filtered_df)
                    scaled_df = pd.DataFrame(scaled_data, columns=filtered_df.columns)

                    # ì •ê·œí™”ëœ ì»¬ëŸ¼ê³¼ ì œì™¸ëœ ì»¬ëŸ¼ ê²°í•©
                    scaled_df = pd.concat([excluded_df, scaled_df], axis=1)

                    # íƒ€ê²Ÿ ë³€ìˆ˜ ì—†ì´ íŠ¹ì„± ì €ì¥ (ìµœì‹  ì˜ˆì¸¡ìš©)
                    symbol_industry = table_for_ai[['symbol', 'industry', 'volume_mul_price']]
                    symbol_industry = symbol_industry.drop_duplicates('symbol', keep='first')
                    fs_df = pd.merge(symbol_industry, scaled_df, how='inner', on=['symbol'])
                    fs_df["sector"] = fs_df["industry"].map(sector_map)
                    fs_df.to_parquet(file_path, engine='pyarrow', compression='snappy', index=False)

                    # í•™ìŠµ ë°ì´í„°ë¥¼ ìœ„í•œ íƒ€ê²Ÿ ë³€ìˆ˜ ì¶”ê°€
                    cur_table_for_ai = pd.merge(table_for_ai, scaled_df, how='inner', on=['symbol','rebalance_date'])
                    cur_table_for_ai["sector"] = cur_table_for_ai["industry"].map(sector_map)

                    # ì‹œì¥ ì¡°ì • ìˆ˜ìµë¥  ê³„ì‚°
                    cur_table_for_ai['price_dev_subavg'] = cur_table_for_ai['price_dev'] - cur_table_for_ai['price_dev'].mean()

                    # ì„¹í„° ì¡°ì • ìˆ˜ìµë¥  ê³„ì‚°
                    sector_list = list(cur_table_for_ai['sector'].unique())
                    sector_list = [x for x in sector_list if str(x) != 'nan']
                    for sec in sector_list:
                        sec_mask = cur_table_for_ai['sector'] == sec
                        sec_mean = cur_table_for_ai.loc[sec_mask, 'price_dev'].mean()
                        cur_table_for_ai.loc[sec_mask, 'sec_price_dev_subavg'] = cur_table_for_ai.loc[sec_mask, 'price_dev'] - sec_mean

                    # íƒ€ê²Ÿ ë³€ìˆ˜ê°€ í¬í•¨ëœ ì™„ì „í•œ ë°ì´í„°ì…‹ ì €ì¥
                    cur_table_for_ai.to_parquet(file2_path, engine='pyarrow', compression='snappy', index=False)
                    self.logger.info(f"âœ… Saved ML data: {os.path.basename(file2_path)}")
                else:
                    # df_for_extract_featureê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
                    self.logger.warning(f"âŒ [{base_year_period}] No features to extract - SKIPPING")
                    self.logger.warning(f"   REASON: All time series columns were filtered out")
                    self.logger.warning(f"   Total columns checked: {len(cal_timefeature_col_list)}")
                    self.logger.warning(f"   Columns accepted: {accepted_col_count}")
                    self.logger.warning(f"   Filter breakdown:")
                    self.logger.warning(f"     - Not in window_data: {filter_reasons['not_in_columns']}")
                    self.logger.warning(f"     - Contains NaN: {filter_reasons['has_nan']}")
                    self.logger.warning(f"     - Contains infinite: {filter_reasons['has_infinite']}")

                    # ìƒ˜í”Œ ëˆ„ë½ ì»¬ëŸ¼ ì¶œë ¥
                    if filter_reasons['not_in_columns'] > 0:
                        missing_cols = [col for col in cal_timefeature_col_list if col not in window_data.columns]
                        sample_missing = missing_cols[:10]
                        self.logger.warning(f"   Sample missing columns (first 10): {sample_missing}")
                    continue
