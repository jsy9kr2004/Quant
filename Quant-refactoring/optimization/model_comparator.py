"""
Model Comparator

ì—¬ëŸ¬ ëª¨ë¸ ë²„ì „ì„ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ ê°œì„  ì—¬ë¶€ë¥¼ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import pandas as pd
import numpy as np
import logging
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import matplotlib.pyplot as plt
from scipy import stats


class ModelComparator:
    """
    ëª¨ë¸ ë²„ì „ ë¹„êµ ì‹œìŠ¤í…œ

    - ë™ì¼í•œ í…ŒìŠ¤íŠ¸ ì…‹ì—ì„œ ì—¬ëŸ¬ ëª¨ë¸ í‰ê°€
    - í†µê³„ì  ìœ ì˜ì„± ê²€ì •
    - ì„±ëŠ¥ ê°œì„  ì—¬ë¶€ íŒë‹¨
    - ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥
    """

    def __init__(self, experiment_name: str = "model_comparison"):
        """
        Args:
            experiment_name: ì‹¤í—˜ ì´ë¦„
        """
        self.experiment_name = experiment_name
        self.models = {}
        self.results = []
        self.comparison_data = None

    def add_model(self,
                 model_name: str,
                 model_instance: Any,
                 description: str = "",
                 hyperparameters: Optional[Dict] = None):
        """
        ë¹„êµí•  ëª¨ë¸ ì¶”ê°€

        Args:
            model_name: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "XGBoost_v1", "XGBoost_v2")
            model_instance: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            description: ëª¨ë¸ ì„¤ëª…
            hyperparameters: í•˜ì´í¼íŒŒë¼ë¯¸í„°
        """
        self.models[model_name] = {
            'instance': model_instance,
            'description': description,
            'hyperparameters': hyperparameters or {},
            'added_time': datetime.now()
        }

        logging.info(f"âœ… ëª¨ë¸ ì¶”ê°€: {model_name}")
        if description:
            logging.info(f"   ì„¤ëª…: {description}")

    def compare_models(self,
                      X_train: pd.DataFrame,
                      y_train: pd.Series,
                      X_test: pd.DataFrame,
                      y_test: pd.Series,
                      cv_splits: int = 5) -> pd.DataFrame:
        """
        ëª¨ë“  ëª¨ë¸ ë¹„êµ

        Args:
            X_train: í•™ìŠµ ë°ì´í„°
            y_train: í•™ìŠµ ë ˆì´ë¸”
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            y_test: í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”
            cv_splits: Cross-validation fold ìˆ˜

        Returns:
            ë¹„êµ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
        """
        if not self.models:
            logging.error("ë¹„êµí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            return pd.DataFrame()

        logging.info(f"\n{'='*60}")
        logging.info(f"ëª¨ë¸ ë¹„êµ ì‹œì‘: {len(self.models)}ê°œ ëª¨ë¸")
        logging.info(f"{'='*60}\n")

        self.results = []

        for model_name, model_info in self.models.items():
            logging.info(f"\n{'='*60}")
            logging.info(f"í‰ê°€ ì¤‘: {model_name}")
            logging.info(f"{'='*60}")

            try:
                model = model_info['instance']

                # 1. Cross-Validation ì„±ëŠ¥
                logging.info("1ï¸âƒ£ Cross-Validation...")
                cv_scores, cv_fold_scores = model.cross_validate(
                    X_train, y_train, cv_splits=cv_splits, verbose=False
                )

                # 2. í•™ìŠµ
                logging.info("2ï¸âƒ£ ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ...")
                model.fit(X_train, y_train, verbose=0)

                # 3. í…ŒìŠ¤íŠ¸ í‰ê°€
                logging.info("3ï¸âƒ£ í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€...")
                test_metrics = model.evaluate(X_test, y_test)

                # 4. ì˜ˆì¸¡ ê°’ ì €ì¥ (í†µê³„ ê²€ì •ìš©)
                y_pred = model.predict(X_test)

                result = {
                    'model_name': model_name,
                    'description': model_info['description'],
                    'train_samples': len(X_train),
                    'test_samples': len(X_test),
                    'y_pred': y_pred,  # í†µê³„ ê²€ì •ìš©
                    **test_metrics,
                    **{f'cv_{k}': v for k, v in cv_scores.items()}
                }

                self.results.append(result)

                logging.info(f"\nğŸ“Š {model_name} ê²°ê³¼:")
                logging.info(f"  Test Accuracy: {test_metrics.get('accuracy', 0):.4f}")
                logging.info(f"  CV Accuracy: {cv_scores.get('accuracy_mean', 0):.4f} Â± {cv_scores.get('accuracy_std', 0):.4f}")

            except Exception as e:
                logging.error(f"âŒ {model_name} í‰ê°€ ì‹¤íŒ¨: {str(e)}")
                import traceback
                traceback.print_exc()
                continue

        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        self.comparison_data = self._create_comparison_df()

        # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        self._perform_statistical_tests(y_test)

        # ìš”ì•½
        self._print_summary()

        return self.comparison_data

    def _create_comparison_df(self) -> pd.DataFrame:
        """ë¹„êµ ë°ì´í„°í”„ë ˆì„ ìƒì„±"""
        if not self.results:
            return pd.DataFrame()

        # y_predëŠ” ì œì™¸í•˜ê³  ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df_data = []
        for result in self.results:
            row = {k: v for k, v in result.items() if k != 'y_pred'}
            df_data.append(row)

        df = pd.DataFrame(df_data)

        # ì •ë ¬ (í…ŒìŠ¤íŠ¸ accuracy ê¸°ì¤€)
        if 'accuracy' in df.columns:
            df = df.sort_values('accuracy', ascending=False)

        return df

    def _perform_statistical_tests(self, y_test: pd.Series):
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
        if len(self.results) < 2:
            return

        logging.info(f"\n{'='*60}")
        logging.info("í†µê³„ì  ìœ ì˜ì„± ê²€ì •")
        logging.info(f"{'='*60}\n")

        # ëª¨ë“  ëª¨ë¸ ìŒì— ëŒ€í•´ McNemar's test ìˆ˜í–‰ (ë¶„ë¥˜ ë¬¸ì œ)
        for i in range(len(self.results)):
            for j in range(i + 1, len(self.results)):
                model1 = self.results[i]
                model2 = self.results[j]

                try:
                    # McNemar's test
                    y_pred1 = model1['y_pred']
                    y_pred2 = model2['y_pred']

                    # Confusion matrix
                    both_correct = ((y_pred1 == y_test) & (y_pred2 == y_test)).sum()
                    both_wrong = ((y_pred1 != y_test) & (y_pred2 != y_test)).sum()
                    model1_correct = ((y_pred1 == y_test) & (y_pred2 != y_test)).sum()
                    model2_correct = ((y_pred1 != y_test) & (y_pred2 == y_test)).sum()

                    # McNemar's test (continuity correction ì ìš©)
                    if model1_correct + model2_correct > 0:
                        statistic = (abs(model1_correct - model2_correct) - 1) ** 2 / (model1_correct + model2_correct)
                        p_value = 1 - stats.chi2.cdf(statistic, 1)

                        logging.info(f"\n{model1['model_name']} vs {model2['model_name']}:")
                        logging.info(f"  McNemar's test statistic: {statistic:.4f}")
                        logging.info(f"  p-value: {p_value:.4f}")

                        if p_value < 0.05:
                            better_model = model1['model_name'] if model1_correct > model2_correct else model2['model_name']
                            logging.info(f"  âœ… ìœ ì˜ë¯¸í•œ ì°¨ì´ ìˆìŒ (p < 0.05)")
                            logging.info(f"  ğŸ† {better_model}ì´(ê°€) í†µê³„ì ìœ¼ë¡œ ë” ìš°ìˆ˜")
                        else:
                            logging.info(f"  âŒ ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ìŒ (p >= 0.05)")

                except Exception as e:
                    logging.debug(f"í†µê³„ ê²€ì • ì‹¤íŒ¨: {str(e)}")
                    continue

    def _print_summary(self):
        """ìš”ì•½ ì¶œë ¥"""
        if self.comparison_data is None or self.comparison_data.empty:
            return

        logging.info(f"\n{'='*60}")
        logging.info("ëª¨ë¸ ë¹„êµ ìš”ì•½")
        logging.info(f"{'='*60}\n")

        # ì£¼ìš” ë©”íŠ¸ë¦­ë§Œ ì¶œë ¥
        display_cols = ['model_name', 'description']

        if 'accuracy' in self.comparison_data.columns:
            display_cols.extend(['accuracy', 'cv_accuracy_mean', 'cv_accuracy_std'])
        if 'f1' in self.comparison_data.columns:
            display_cols.extend(['f1'])
        if 'precision' in self.comparison_data.columns:
            display_cols.extend(['precision', 'recall'])

        display_cols = [col for col in display_cols if col in self.comparison_data.columns]

        logging.info(self.comparison_data[display_cols].to_string())

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        if 'accuracy' in self.comparison_data.columns:
            best_model = self.comparison_data.iloc[0]
            logging.info(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model_name']}")
            logging.info(f"   Test Accuracy: {best_model['accuracy']:.4f}")

    def plot_comparison(self, save_path: Optional[str] = None):
        """ëª¨ë¸ ë¹„êµ ì‹œê°í™”"""
        if self.comparison_data is None or self.comparison_data.empty:
            logging.warning("ë¹„êµ ë°ì´í„°ê°€ ì—†ì–´ ì‹œê°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = self.comparison_data

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(f'ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ: {self.experiment_name}', fontsize=16, fontweight='bold')

        model_names = df['model_name'].values
        x_pos = np.arange(len(model_names))

        # 1. Test Accuracy
        if 'accuracy' in df.columns:
            axes[0, 0].bar(x_pos, df['accuracy'])
            axes[0, 0].set_title('Test Accuracy')
            axes[0, 0].set_xticks(x_pos)
            axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')
            axes[0, 0].set_ylabel('Accuracy')
            axes[0, 0].grid(axis='y', alpha=0.3)

        # 2. CV Accuracy (with error bars)
        if 'cv_accuracy_mean' in df.columns and 'cv_accuracy_std' in df.columns:
            axes[0, 1].bar(x_pos, df['cv_accuracy_mean'], yerr=df['cv_accuracy_std'], capsize=5)
            axes[0, 1].set_title('CV Accuracy (mean Â± std)')
            axes[0, 1].set_xticks(x_pos)
            axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')
            axes[0, 1].set_ylabel('Accuracy')
            axes[0, 1].grid(axis='y', alpha=0.3)

        # 3. Precision & Recall
        if 'precision' in df.columns and 'recall' in df.columns:
            width = 0.35
            axes[1, 0].bar(x_pos - width/2, df['precision'], width, label='Precision')
            axes[1, 0].bar(x_pos + width/2, df['recall'], width, label='Recall')
            axes[1, 0].set_title('Precision & Recall')
            axes[1, 0].set_xticks(x_pos)
            axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].legend()
            axes[1, 0].grid(axis='y', alpha=0.3)

        # 4. F1 Score
        if 'f1' in df.columns:
            axes[1, 1].bar(x_pos, df['f1'])
            axes[1, 1].set_title('F1 Score')
            axes[1, 1].set_xticks(x_pos)
            axes[1, 1].set_xticklabels(model_names, rotation=45, ha='right')
            axes[1, 1].set_ylabel('F1')
            axes[1, 1].grid(axis='y', alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logging.info(f"ğŸ“Š ì°¨íŠ¸ ì €ì¥: {save_path}")

        plt.show()

    def save_results(self, output_dir: str = './results/model_comparison'):
        """ê²°ê³¼ ì €ì¥"""
        if self.comparison_data is None or self.comparison_data.empty:
            logging.warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # 1. CSV ì €ì¥
        csv_file = output_path / f'{self.experiment_name}_{timestamp}.csv'
        # y_pred ì œì™¸í•˜ê³  ì €ì¥
        df_to_save = self.comparison_data.copy()
        if 'y_pred' in df_to_save.columns:
            df_to_save = df_to_save.drop('y_pred', axis=1)
        df_to_save.to_csv(csv_file, index=False)
        logging.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {csv_file}")

        # 2. ë©”íƒ€ë°ì´í„° ì €ì¥ (JSON)
        metadata = {
            'experiment_name': self.experiment_name,
            'timestamp': timestamp,
            'num_models': len(self.models),
            'models': {}
        }

        for model_name, model_info in self.models.items():
            metadata['models'][model_name] = {
                'description': model_info['description'],
                'hyperparameters': model_info['hyperparameters'],
                'added_time': model_info['added_time'].strftime('%Y-%m-%d %H:%M:%S')
            }

        json_file = output_path / f'{self.experiment_name}_{timestamp}_metadata.json'
        with open(json_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        logging.info(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {json_file}")

        # 3. ìš”ì•½ í…ìŠ¤íŠ¸ ì €ì¥
        summary_file = output_path / f'{self.experiment_name}_{timestamp}_summary.txt'
        with open(summary_file, 'w') as f:
            f.write(f"ì‹¤í—˜ëª…: {self.experiment_name}\n")
            f.write(f"ë‚ ì§œ: {timestamp}\n")
            f.write(f"ë¹„êµ ëª¨ë¸ ìˆ˜: {len(self.models)}\n\n")

            f.write("="*60 + "\n")
            f.write("ì„±ëŠ¥ ë¹„êµ\n")
            f.write("="*60 + "\n")
            f.write(df_to_save.to_string())
            f.write("\n\n")

            if 'accuracy' in df_to_save.columns:
                best_model = df_to_save.iloc[0]
                f.write("="*60 + "\n")
                f.write("ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n")
                f.write("="*60 + "\n")
                f.write(f"ëª¨ë¸ëª…: {best_model['model_name']}\n")
                f.write(f"ì„¤ëª…: {best_model['description']}\n")
                f.write(f"Test Accuracy: {best_model['accuracy']:.4f}\n")

        logging.info(f"ğŸ’¾ ìš”ì•½ ì €ì¥: {summary_file}")

    def get_best_model(self) -> Optional[Dict]:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if self.comparison_data is None or self.comparison_data.empty:
            return None

        best_row = self.comparison_data.iloc[0]
        return {
            'model_name': best_row['model_name'],
            'description': best_row.get('description', ''),
            'metrics': best_row.to_dict()
        }

    def is_improved(self,
                   new_model_name: str,
                   baseline_model_name: str,
                   metric: str = 'accuracy',
                   threshold: float = 0.01) -> bool:
        """
        ìƒˆ ëª¨ë¸ì´ baseline ëŒ€ë¹„ ê°œì„ ë˜ì—ˆëŠ”ì§€ í™•ì¸

        Args:
            new_model_name: ìƒˆ ëª¨ë¸ ì´ë¦„
            baseline_model_name: ê¸°ì¤€ ëª¨ë¸ ì´ë¦„
            metric: ë¹„êµ ë©”íŠ¸ë¦­
            threshold: ê°œì„  ì„ê³„ê°’ (1% = 0.01)

        Returns:
            ê°œì„  ì—¬ë¶€
        """
        if self.comparison_data is None or self.comparison_data.empty:
            return False

        new_model = self.comparison_data[self.comparison_data['model_name'] == new_model_name]
        baseline = self.comparison_data[self.comparison_data['model_name'] == baseline_model_name]

        if new_model.empty or baseline.empty:
            logging.warning(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {new_model_name} or {baseline_model_name}")
            return False

        new_score = new_model[metric].values[0]
        baseline_score = baseline[metric].values[0]

        improvement = new_score - baseline_score
        improvement_pct = improvement / baseline_score * 100 if baseline_score > 0 else 0

        logging.info(f"\n{'='*60}")
        logging.info(f"ëª¨ë¸ ê°œì„  ì—¬ë¶€ í™•ì¸")
        logging.info(f"{'='*60}")
        logging.info(f"Baseline: {baseline_model_name} ({metric}={baseline_score:.4f})")
        logging.info(f"New: {new_model_name} ({metric}={new_score:.4f})")
        logging.info(f"ê°œì„ : {improvement:+.4f} ({improvement_pct:+.2f}%)")
        logging.info(f"ì„ê³„ê°’: {threshold:.4f} ({threshold*100:.2f}%)")

        if improvement > threshold:
            logging.info(f"âœ… ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!")
            return True
        else:
            logging.info(f"âŒ ê°œì„ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
