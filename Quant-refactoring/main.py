"""
Quant Trading System - Main Pipeline (Refactored)

í€€íŠ¸ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œì˜ ë©”ì¸ ì§„ì…ì ì…ë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ë°±í…ŒìŠ¤íŒ…ê¹Œì§€
ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì¡°ìœ¨í•©ë‹ˆë‹¤:

1. ì„¤ì • ë¡œë”©: YAML ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤
2. ë°ì´í„° ìˆ˜ì§‘: FMP APIì—ì„œ ê¸ˆìœµ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì„ íƒì‚¬í•­)
3. ML íŒŒì´í”„ë¼ì¸: í•™ìŠµ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ê³ , ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë©°, ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤
4. ë°±í…ŒìŠ¤íŒ…: ê³¼ê±° ë°ì´í„°ë¡œ íŠ¸ë ˆì´ë”© ì „ëµì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤

ì‹œìŠ¤í…œ ê¸°ëŠ¥:
- Parquet ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€ë¡œ íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬
- ë‹¤ì–‘í•œ ML ëª¨ë¸ ì§€ì› (XGBoost, LightGBM, CatBoost)
- MLflow ì¶”ì ì´ ê°€ëŠ¥í•œ ì•™ìƒë¸” ì „ëµ
- ì»¤ìŠ¤í…€ ìŠ¤ì½”ì–´ë§ ì „ëµì„ ì‚¬ìš©í•œ ìœ ì—°í•œ ë°±í…ŒìŠ¤íŒ…

ì‚¬ìš©ë²•:
    python main.py

ì„¤ì •:
    config/conf.yaml íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ:
    - DATA.GET_FMP: ë°ì´í„° ìˆ˜ì§‘ í™œì„±í™”/ë¹„í™œì„±í™”
    - ML.RUN_REGRESSION: ML í•™ìŠµ í™œì„±í™”/ë¹„í™œì„±í™”
    - BACKTEST.RUN_BACKTEST: ë°±í…ŒìŠ¤íŒ… í™œì„±í™”/ë¹„í™œì„±í™”

ì‘ì„±ì: Quant Trading Team
ë‚ ì§œ: 2025-10-29
"""

import logging
import os
import sys
from typing import Dict, Any, Optional, List
import yaml
import pandas as pd
from pathlib import Path

# Add current directory to path for module imports
sys.path.insert(0, str(Path(__file__).parent))

from config.context_loader import load_config, MainContext
from config.logger import get_logger
from storage import ParquetStorage
from data_collector.fmp import FMP
from training.make_mldata import AIDataMaker
from models import XGBoostModel, LightGBMModel, CatBoostModel, StackingEnsemble
from training import OptunaOptimizer, MLflowTracker
from backtest import Backtest, PlanHandler


class RegressorIntegrated:
    """
    ë ˆê±°ì‹œì™€ ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ê²°í•©í•œ í†µí•© ML í•™ìŠµ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

    ì´ í´ë˜ìŠ¤ëŠ” ë ˆê±°ì‹œ regressor êµ¬í˜„ê³¼ ìƒˆë¡œìš´ ëª¨ë“ˆí˜• ëª¨ë¸ êµ¬ì¡° ì‚¬ì´ì˜
    ë¸Œë¦¿ì§€ ì—­í• ì„ í•©ë‹ˆë‹¤. MLflow ì¶”ì  ë° ëª¨ë“ˆí˜• ëª¨ë¸ê³¼ ê°™ì€ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„
    ì‚¬ìš©í•˜ë©´ì„œ í•˜ìœ„ í˜¸í™˜ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

    Attributes:
        conf (Dict[str, Any]): YAMLì—ì„œ ë¡œë“œí•œ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        use_new_models (bool): ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì‚¬ìš© ì—¬ë¶€
        legacy_regressor (Optional[Regressor]): í´ë°±ìš© ë ˆê±°ì‹œ regressor ì¸ìŠ¤í„´ìŠ¤

    Example:
        >>> config = load_config('config/conf.yaml')
        >>> regressor = RegressorIntegrated(config, use_new_models=True)
        >>> regressor.dataload()
        >>> regressor.train()
        >>> regressor.evaluation()

    TODO:
        - ë ˆê±°ì‹œ ì˜ì¡´ì„± ì—†ì´ ë„¤ì´í‹°ë¸Œ ë°ì´í„° ë¡œë”© êµ¬í˜„
        - ì»¤ìŠ¤í…€ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì› ì¶”ê°€
        - í•™ìŠµ íŒŒì´í”„ë¼ì¸ì— êµì°¨ ê²€ì¦ êµ¬í˜„
    """

    def __init__(self, conf: Dict[str, Any], use_new_models: bool = True) -> None:
        """
        í†µí•© regressorë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            conf: DATA, ML, BACKTEST ì„¤ì •ì„ í¬í•¨í•œ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            use_new_models: Trueì¸ ê²½ìš° MLflow ì¶”ì ì´ ê°€ëŠ¥í•œ ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì‚¬ìš©.
                          Falseì¸ ê²½ìš° ë ˆê±°ì‹œ regressor êµ¬í˜„ìœ¼ë¡œ í´ë°±.

        Raises:
            ImportError: ë ˆê±°ì‹œ regressorê°€ ìš”ì²­ë˜ì—ˆì§€ë§Œ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
        """
        self.conf = conf
        self.use_new_models = use_new_models

        # Import legacy Regressor as fallback
        try:
            from training.regressor import Regressor
            self.legacy_regressor: Optional['Regressor'] = Regressor(conf)
        except ImportError:
            logger = get_logger('RegressorIntegrated')
            logger.warning("Legacy regressor not found, using new models only")
            self.legacy_regressor = None

    def dataload(self) -> None:
        """
        Parquet íŒŒì¼ì—ì„œ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        ì„¤ì •ì—ì„œ ì§€ì •í•œ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ê¸°ê°„ì— ëŒ€í•œ ë¶„ê¸°ë³„ ML ë°ì´í„° íŒŒì¼
        (rnorm_ml_{year}_{quarter}.parquet)ì„ ë¡œë“œí•©ë‹ˆë‹¤.

        ë°ì´í„° ì²˜ë¦¬ ê³¼ì •:
        - ê²°ì¸¡ì¹˜ê°€ 80% ì´ìƒì¸ ì—´ í•„í„°ë§
        - 95% ì´ìƒì˜ í–‰ì´ ë™ì¼í•œ ê°’ì„ ê°€ì§„ ì—´ í•„í„°ë§
        - ê²°ì¸¡ì¹˜ê°€ 60% ì´ìƒì¸ í–‰ í•„í„°ë§
        - ì£¼ì‹ì„ ì„¹í„°ì— ë§¤í•‘
        - íŠ¹ì„±(X)ê³¼ íƒ€ê²Ÿ(y) ë¶„ë¦¬

        Raises:
            ValueError: í•™ìŠµ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
            FileNotFoundError: í•„ìˆ˜ ë°ì´í„° íŒŒì¼ì´ ëˆ„ë½ëœ ê²½ìš°

        TODO:
            - ë ˆê±°ì‹œ ì˜ì¡´ì„± ì—†ì´ ë„¤ì´í‹°ë¸Œ ë°ì´í„° ë¡œë”© êµ¬í˜„
            - ë°ì´í„° ê²€ì¦ ë° í’ˆì§ˆ ì²´í¬ ì¶”ê°€
        """
        if self.legacy_regressor:
            self.legacy_regressor.dataload()
        else:
            logger = get_logger('RegressorIntegrated')
            logger.warning("Using new data loading method")
            # TODO: Implement new data loading method
            # Should load from /data/ml_per_year/rnorm_ml_*.parquet

    def train(self) -> None:
        """
        ì„¤ì •ëœ ì „ëµìœ¼ë¡œ ML ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

        í•™ìŠµ í”„ë¡œì„¸ìŠ¤:
        1. use_new_models=Trueì´ê³  MLflowê°€ í™œì„±í™”ëœ ê²½ìš°:
           - ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì‚¬ìš©
           - MLflowë¡œ ì‹¤í—˜ ì¶”ì 
           - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì§€ì›
        2. ê·¸ ì™¸ì˜ ê²½ìš°:
           - ë ˆê±°ì‹œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í´ë°±
           - XGBoost + LightGBM ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ

        í•™ìŠµë˜ëŠ” ëª¨ë¸:
        - 3x XGBoost Classifiers (depth 8, 9, 10)
        - 1x LightGBM Classifier
        - 2x XGBoost Regressors (depth 8, 10)

        Raises:
            ValueError: ì‚¬ìš© ê°€ëŠ¥í•œ í•™ìŠµ ë°©ë²•ì´ ì—†ëŠ” ê²½ìš°
            RuntimeError: ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨ ì‹œ
        """
        if self.use_new_models and self.conf.get('ML', {}).get('USE_MLFLOW'):
            self._train_with_new_models()
        elif self.legacy_regressor:
            self.legacy_regressor.train()
        else:
            logger = get_logger('RegressorIntegrated')
            logger.error("No training method available")

    def _train_with_new_models(self) -> None:
        """
        MLflow ì¶”ì ì´ ê°€ëŠ¥í•œ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

        ê¸°ëŠ¥:
        - MLflow ì‹¤í—˜ ì¶”ì 
        - ëª¨ë“ˆí˜• ëª¨ë¸ ì•„í‚¤í…ì²˜
        - ì„¤ì • ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        - ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì§€ì›

        TODO:
            - ì™„ì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
            - ì»¤ìŠ¤í…€ ëª¨ë¸ ì„¤ì • ì§€ì› ì¶”ê°€
            - HPOë¥¼ ìœ„í•œ OptunaOptimizer í†µí•©
        """
        logger = get_logger('RegressorIntegrated')
        logger.info("Training with new model structure + MLflow")

        ml_config = self.conf.get('ML', {})

        # Initialize MLflow tracker
        if ml_config.get('USE_MLFLOW'):
            tracker = MLflowTracker(
                experiment_name=ml_config.get('MLFLOW_EXPERIMENT', 'quant_trading')
            )

        # TODO: Load training data from legacy regressor or implement native loading
        # X_train = self.legacy_regressor.x_train
        # y_train = self.legacy_regressor.y_train

        # TODO: Build and train models
        # - Create base models (XGBoost, LightGBM, CatBoost)
        # - Train each model with early stopping
        # - Create stacking ensemble
        # - Log to MLflow

        logger.info("New model training completed (placeholder)")

    def evaluation(self) -> None:
        """
        í•™ìŠµëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ í‰ê°€í•©ë‹ˆë‹¤.

        í‰ê°€ í”„ë¡œì„¸ìŠ¤:
        1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (TEST_START_YEARë¶€í„° TEST_END_YEARê¹Œì§€ì˜ ë¶„ê¸°ë³„ íŒŒì¼)
        2. ê° ë¶„ê¸°ë§ˆë‹¤:
           - ë¶„ë¥˜ ëª¨ë¸ ì‹¤í–‰ (ìƒìŠ¹/í•˜ë½ ì˜ˆì¸¡)
           - ì„ê³„ê°’ í•„í„°ë§ ì ìš© (ìƒìœ„ 8%)
           - íšŒê·€ ëª¨ë¸ ì‹¤í–‰ (ë³€ë™ í¬ê¸° ì˜ˆì¸¡)
           - ì•™ìƒë¸” íˆ¬í‘œë¡œ ì˜ˆì¸¡ ê²°í•©
        3. ë©”íŠ¸ë¦­ ê³„ì‚°:
           - ë¶„ë¥˜: Accuracy, Precision, Recall, F1
           - íšŒê·€: RMSE, MAE
           - Top-K ì„±ëŠ¥: ìƒìœ„ 3, 8, 16 ì¢…ëª©ì˜ í‰ê·  ìˆ˜ìµë¥ 
        4. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥

        ì¶œë ¥ íŒŒì¼:
        - prediction_ai_{year}_{quarter}.csv: ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼
        - prediction_*_top0-3.csv: ìƒìœ„ 3ê°œ ì¢…ëª© ì˜ˆì¸¡
        - pred_df_topk.csv: ìš”ì•½ í†µê³„

        Raises:
            FileNotFoundError: í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ëˆ„ë½ëœ ê²½ìš°
        """
        if self.legacy_regressor:
            self.legacy_regressor.evaluation()

    def latest_prediction(self) -> None:
        """
        ê°€ì¥ ìµœê·¼ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤.

        í”„ë¡œì„¸ìŠ¤:
        1. ìµœì‹  ë¶„ê¸°ë³„ ë°ì´í„° ë¡œë“œ (ê°€ì¥ ìµœê·¼ year_period)
        2. í•™ìŠµëœ ëª¨ë“  ëª¨ë¸ ì‹¤í–‰
        3. ì•™ìƒë¸” ì „ëµ ì ìš©
        4. ì˜ˆì¸¡ ìˆ˜ìµë¥ ë¡œ ì¢…ëª© ìˆœìœ„ ë§¤ê¹€
        5. ê° ëª¨ë¸ ì¡°í•©ì— ëŒ€í•´ ìƒìœ„ Kê°œ ì¢…ëª© ì„ íƒ

        ì¶œë ¥ íŒŒì¼:
        - latest_prediction.csv: ì ìˆ˜ê°€ í¬í•¨ëœ ëª¨ë“  ì˜ˆì¸¡
        - latest_prediction_{model}_top0-3.csv: ëª¨ë¸ë³„ ìƒìœ„ 3ê°œ ì¢…ëª©
        - latest_prediction_{model}_top0-7.csv: ëª¨ë¸ë³„ ìƒìœ„ 8ê°œ ì¢…ëª©
        - latest_prediction_{model}_top0-15.csv: ëª¨ë¸ë³„ ìƒìœ„ 16ê°œ ì¢…ëª©

        ì´ íŒŒì¼ë“¤ì€ ì‹¤ì œ íŠ¸ë ˆì´ë”© ì˜ì‚¬ê²°ì •ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        Raises:
            FileNotFoundError: ìµœì‹  ë°ì´í„° íŒŒì¼ì´ ëˆ„ë½ëœ ê²½ìš°
        """
        if self.legacy_regressor:
            self.legacy_regressor.latest_prediction()


def get_config_path() -> str:
    """
    ì„¤ì • íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤.

    ë‹¤ìŒ ìœ„ì¹˜ì—ì„œ config/conf.yamlì„ ê²€ìƒ‰í•©ë‹ˆë‹¤:
    1. ./config/conf.yaml (í˜„ì¬ ë””ë ‰í† ë¦¬)
    2. ../config/conf.yaml (ë¶€ëª¨ ë””ë ‰í† ë¦¬)
    3. Quant-refactoring/config/conf.yaml (í”„ë¡œì íŠ¸ ë£¨íŠ¸)

    Returns:
        str: ì„¤ì • íŒŒì¼ ê²½ë¡œ

    Raises:
        FileNotFoundError: ëª¨ë“  ìœ„ì¹˜ì—ì„œ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°

    Example:
        >>> config_path = get_config_path()
        >>> print(config_path)
        'config/conf.yaml'
    """
    possible_paths = [
        'config/conf.yaml',
        '../config/conf.yaml',
        'Quant-refactoring/config/conf.yaml'
    ]

    for path in possible_paths:
        if os.path.exists(path):
            return path

    raise FileNotFoundError(
        "Config file not found. Please create config/conf.yaml\n"
        "Use config/conf.yaml.template as reference"
    )


def conf_check(config: Dict[str, Any]) -> None:
    """
    ì„¤ì • íŒŒì¼ì„ ê²€ì¦í•©ë‹ˆë‹¤.

    ê²€ì¦ í•­ëª©:
    1. REPORT_LISTê°€ ìœ íš¨í•œ ë¦¬í¬íŠ¸ ìœ í˜•ë§Œ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
    2. DATA ì„¹ì…˜ì— ROOT_PATHê°€ ì§€ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    3. í•„ìˆ˜ ì„¹ì…˜(DATA, ML, BACKTEST) ì¡´ì¬ ì—¬ë¶€ í™•ì¸

    Args:
        config: YAMLì—ì„œ ë¡œë“œí•œ ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Raises:
        SystemExit: ê²€ì¦ ì‹¤íŒ¨ ì‹œ (ì¢…ë£Œ ì½”ë“œ 1)

    Example:
        >>> config = load_config('config/conf.yaml')
        >>> conf_check(config)
        INFO: âœ… Configuration validated
    """
    logger = get_logger('config_check')

    # Validate REPORT_LIST
    valid_reports = ["EVAL", "RANK", "AI", "AVG"]
    report_list = config.get('BACKTEST', {}).get('REPORT_LIST', [])

    for rep_type in report_list:
        if rep_type not in valid_reports:
            logger.critical(f"Invalid REPORT_LIST: {rep_type}. Valid: {valid_reports}")
            sys.exit(1)

    # Validate required settings
    data_config = config.get('DATA', {})
    if not data_config.get('ROOT_PATH'):
        logger.critical("ROOT_PATH not set in config")
        sys.exit(1)

    logger.info("âœ… Configuration validated")


def main() -> None:
    """
    ì „ì²´ í€€íŠ¸ íŠ¸ë ˆì´ë”© ì›Œí¬í”Œë¡œìš°ë¥¼ ì¡°ìœ¨í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

    íŒŒì´í”„ë¼ì¸ ë‹¨ê³„:
    ===============

    1. ì„¤ì • ë¡œë”© ë° ê²€ì¦
       - config/conf.yaml ë¡œë“œ
       - ëª¨ë“  ì„¤ì • ê²€ì¦
       - ë¡œê¹… ë° ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”

    2. ë°ì´í„° ìˆ˜ì§‘ (ì„ íƒì‚¬í•­: GET_FMP=Y)
       - FMP APIì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
       - CSV íŒŒì¼ë¡œ ì €ì¥
       - Parquet í˜•ì‹ìœ¼ë¡œ ë³€í™˜
       - í†µí•© ë·° êµ¬ì¶•

    3. ML íŒŒì´í”„ë¼ì¸ (ì„ íƒì‚¬í•­: RUN_REGRESSION=Y)
       a. ë°ì´í„° ì¤€ë¹„:
          - VIEW íŒŒì¼ ë¡œë“œ
          - ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œ (tsfresh)
          - ì¬ë¬´ ë¹„ìœ¨ ê³„ì‚°
          - RobustScalerë¡œ ì •ê·œí™”
          - ë¶„ê¸°ë³„ ML ë°ì´í„°ì…‹ ì €ì¥
       b. ëª¨ë¸ í•™ìŠµ:
          - í•™ìŠµ ë°ì´í„° ë¡œë“œ (2015-2021)
          - ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (ìƒìŠ¹/í•˜ë½)
          - íšŒê·€ ëª¨ë¸ í•™ìŠµ (ë³€ë™ í¬ê¸°)
          - ëª¨ë¸ì„ /data/MODELS/ì— ì €ì¥
       c. í‰ê°€:
          - 2022-2023 ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
          - ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
          - ìƒìœ„ Kê°œ ì˜ˆì¸¡ ì €ì¥
       d. ìµœì‹  ì˜ˆì¸¡:
          - ê°€ì¥ ìµœê·¼ ë°ì´í„°ë¡œ ì˜ˆì¸¡
          - ì˜ˆìƒ ìˆ˜ìµë¥ ë¡œ ì¢…ëª© ìˆœìœ„ ë§¤ê¹€
          - íŠ¸ë ˆì´ë”© ì‹œê·¸ë„ ìƒì„±

    4. ë°±í…ŒìŠ¤íŒ… (ì„ íƒì‚¬í•­: RUN_BACKTEST=Y)
       - ì»¤ìŠ¤í…€ ìŠ¤ì½”ì–´ë§ ì „ëµ ë¡œë“œ (plan.csv)
       - ê³¼ê±° ë°ì´í„°ë¡œ íŠ¸ë ˆì´ë”© ì‹œë®¬ë ˆì´ì…˜
       - ìˆ˜ìµë¥ , ìƒ¤í”„ ë¹„ìœ¨, ë‚™í­ ê³„ì‚°
       - ë°±í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±

    ì„¤ì • ì˜µì…˜:
    =====================

    config/conf.yaml:
        DATA:
            GET_FMP: Y/N - APIì—ì„œ ìƒˆ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            ROOT_PATH: /path/to/data - ë°ì´í„° ë””ë ‰í† ë¦¬
            START_YEAR: 2015 - ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘
            END_YEAR: 2023 - ë°ì´í„° ìˆ˜ì§‘ ì¢…ë£Œ
            STORAGE_TYPE: PARQUET - ìŠ¤í† ë¦¬ì§€ í˜•ì‹

        ML:
            RUN_REGRESSION: Y/N - ML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            USE_NEW_MODELS: Y/N - ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì‚¬ìš©
            USE_MLFLOW: Y/N - MLflow ì¶”ì  í™œì„±í™”
            TRAIN_START_YEAR: 2015 - í•™ìŠµ ê¸°ê°„ ì‹œì‘
            TRAIN_END_YEAR: 2021 - í•™ìŠµ ê¸°ê°„ ì¢…ë£Œ
            TEST_START_YEAR: 2022 - í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì‹œì‘
            TEST_END_YEAR: 2023 - í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì¢…ë£Œ
            EXIT_AFTER_ML: Y/N - ML í›„ ì¢…ë£Œ (ë°±í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°)

        BACKTEST:
            RUN_BACKTEST: Y/N - ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
            REBALANCE_PERIOD: 3 - ë¦¬ë°¸ëŸ°ì‹± ê°„ê²©(ê°œì›”)
            TOP_K_NUM: 100 - ì„ íƒí•  ì¢…ëª© ìˆ˜
            ABSOLUTE_SCORE: 500 - ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’
            REPORT_LIST: [EVAL, RANK, AVG] - ë¦¬í¬íŠ¸ ìœ í˜•

    Raises:
        FileNotFoundError: ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        ValueError: ì„¤ì •ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
        SystemExit: ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ ì‹œ (ì¢…ë£Œ ì½”ë“œ 1)
        KeyboardInterrupt: ì‚¬ìš©ì ì¤‘ë‹¨ ì‹œ (ì¢…ë£Œ ì½”ë“œ 0)

    Example:
        $ python main.py
        ================================================================================
        Quant Trading System - Refactored Version
        ================================================================================
        INFO: âœ… Configuration validated
        INFO: ================================================================================
        INFO: Step 1: FMP Data Collection
        INFO: ================================================================================
        ...

    See Also:
        - WORKFLOW_GUIDE.md: ìƒì„¸í•œ ì‹œìŠ¤í…œ ë¬¸ì„œ
        - README.md: ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
        - config/conf.yaml.template: ì„¤ì • í…œí”Œë¦¿
    """
    print("="*80)
    print("Quant Trading System - Refactored Version")
    print("="*80)

    # 1. Load and validate configuration
    try:
        config_path = get_config_path()
        config = load_config(config_path)
    except FileNotFoundError as e:
        print(f"\nâŒ {e}")
        print("\nPlease create config/conf.yaml with your settings.")
        print("See config/conf.yaml.template for reference.")
        sys.exit(1)

    # 2. Initialize context (automatically sets up logging)
    main_ctx = MainContext(config)
    logger = get_logger('main')

    conf_check(config)

    # 3. Create output directories
    main_ctx.create_dir("./reports")

    # 4. Configure data storage
    data_config = config.get('DATA', {})
    storage_type = data_config.get('STORAGE_TYPE', 'PARQUET')

    # 5. FMP Data Collection (Optional)
    if data_config.get('GET_FMP') == 'Y':
        logger.info("="*80)
        logger.info("Step 1: FMP Data Collection")
        logger.info("="*80)

        # Check if we should use legacy FMP or refactored FMP
        use_legacy = data_config.get('USE_LEGACY_FMP', 'N') == 'Y'

        try:
            if use_legacy:
                logger.info("ğŸ”„ Using LEGACY FMP (root fmp.py)")
                # Import legacy FMP and MainCtx from parent directory
                sys.path.insert(0, str(Path(__file__).parent.parent))
                from fmp import FMP as LegacyFMP
                from main import MainCtx as LegacyMainCtx

                # Legacy FMP uses old config format
                legacy_config = {
                    'START_YEAR': data_config.get('START_YEAR'),
                    'END_YEAR': data_config.get('END_YEAR'),
                    'ROOT_PATH': data_config.get('ROOT_PATH'),
                    'FMP_URL': data_config.get('FMP_URL'),
                    'API_KEY': data_config.get('API_KEY'),
                    'EX_SYMBOL': data_config.get('EX_SYMBOL'),
                    'TARGET_STOCK_LIST': data_config.get('TARGET_STOCK_LIST'),
                    'TARGET_API_LIST': data_config.get('TARGET_API_LIST'),
                    'LOG_LVL': config.get('LOG_LVL', 20),
                }

                # Create legacy MainCtx object
                legacy_ctx = LegacyMainCtx(legacy_config)
                fmp = LegacyFMP(legacy_config, legacy_ctx)
                fmp.get_new()

                logger.info("âœ… Legacy FMP data collection completed")
            else:
                logger.info("âœ¨ Using REFACTORED FMP (data_collector/fmp.py)")
                from data_collector.fmp import FMP
                fmp = FMP(main_ctx)
                fmp.collect()

            # Convert to Parquet format for efficient storage and retrieval
            if storage_type == 'PARQUET':
                logger.info("Converting to Parquet format...")

                # Initialize Parquet storage with auto-validation
                storage = ParquetStorage(
                    root_path=main_ctx.root_path,
                    auto_validate=True
                )

                # Use legacy converter to process CSV files
                from storage.parquet_converter import Parquet
                df_engine = Parquet(main_ctx)
                df_engine.insert_csv()
                df_engine.rebuild_table_view()

                logger.info("âœ… Data saved in Parquet format")

            elif storage_type == 'DB':
                logger.warning("Database storage not recommended. Use PARQUET instead.")
                from database import Database
                db = Database(main_ctx)
                db.insert_csv()
                db.rebuild_table_view()

        except Exception as e:
            logger.error(f"FMP data collection failed: {e}")
            logger.info("Continuing with existing data...")

    # 6. ML Pipeline (Optional)
    ml_config = config.get('ML', {})
    if ml_config.get('RUN_REGRESSION') == 'Y':
        logger.info("="*80)
        logger.info("Step 2: ML Pipeline")
        logger.info("="*80)

        # 6.1 Prepare ML training data
        logger.info("Preparing ML training data...")
        AIDataMaker(main_ctx, config)

        # 6.2 Train models
        logger.info("Training models...")
        regressor = RegressorIntegrated(
            config,
            use_new_models=ml_config.get('USE_NEW_MODELS', False)
        )
        regressor.dataload()
        regressor.train()
        regressor.evaluation()
        regressor.latest_prediction()

        logger.info("âœ… ML pipeline completed")

        # Exit after ML if configured (allows running ML without backtest)
        if ml_config.get('EXIT_AFTER_ML', True):
            logger.info("Exiting after ML (set EXIT_AFTER_ML=N to continue to backtest)")
            sys.exit(0)

    # 7. Backtesting (Optional)
    backtest_config = config.get('BACKTEST', {})
    if backtest_config.get('RUN_BACKTEST', True):
        logger.info("="*80)
        logger.info("Step 3: Backtesting")
        logger.info("="*80)

        # Initialize plan handler for stock scoring
        plan_handler = PlanHandler(
            backtest_config.get('TOP_K_NUM', 100),
            backtest_config.get('ABSOLUTE_SCORE', 500),
            main_ctx
        )

        # Load custom scoring plan from CSV
        plan: List[Dict[str, Any]] = []
        plan_file = "plan.csv"

        if os.path.exists(plan_file):
            plan_df = pd.read_csv(plan_file)
            plan_info = plan_df.values.tolist()

            for i in range(len(plan_info)):
                plan.append({
                    "f_name": plan_handler.single_metric_plan_no_parallel,
                    "params": {
                        "key": plan_info[i][0],         # Metric name (e.g., 'roe')
                        "key_dir": plan_info[i][1],     # Direction ('ascending'/'descending')
                        "weight": plan_info[i][2],      # Weight in scoring
                        "diff": plan_info[i][3],        # Use price difference
                        "base": plan_info[i][4],        # Base metric for normalization
                        "base_dir": plan_info[i][5]     # Base direction
                    }
                })

            plan_handler.plan_list = plan
        else:
            logger.warning(f"Plan file not found: {plan_file}")
            logger.info("Using default plan (empty)")

        # Run backtest simulation
        bt = Backtest(
            main_ctx,
            config,
            plan_handler,
            rebalance_period=backtest_config.get('REBALANCE_PERIOD', 3)
        )

        logger.info("âœ… Backtesting completed")

        # Cleanup
        del plan_handler
        del bt

    # 8. Pipeline completed
    logger.info("="*80)
    logger.info("Pipeline completed successfully!")
    logger.info("="*80)


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        err_logger = get_logger('main')
        err_logger.info("\n\nInterrupted by user")
        sys.exit(0)
    except Exception as e:
        err_logger = get_logger('main')
        err_logger.error(f"\n\nâŒ Fatal error: {e}", exc_info=True)
        sys.exit(1)
